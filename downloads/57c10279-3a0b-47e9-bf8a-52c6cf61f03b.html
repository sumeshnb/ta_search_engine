<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<meta content="IE=10" http-equiv="X-UA-Compatible"/>
<meta charset="utf-8"/>
<title> UTA CSE  PhD Thesis Defenses</title>
<meta content=" UTA CSE  PhD Thesis Defenses" name="description"/>
<meta content="Computer, Science, Engineering, Department, university, texas, arlington" name="keywords"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1" name="viewport"/><link href="../_css/screen.css" media="screen" rel="stylesheet"/> <link href="../_css/print.css" media="print" rel="stylesheet"/>
<script src="../_js/menu.js" type="text/javascript"></script>
<noscript>
<link href="../_css/noscript.css" rel="stylesheet"/>
</noscript>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js" type="text/javascript"></script>
<script src="http://malsup.github.io/jquery.cycle2.js" type="text/javascript"></script>
<script src="http://malsup.github.io/jquery.cycle2.tile.js" type="text/javascript"></script>
<script language="javascript" type="text/javascript">
function showHide(shID) {
   if (document.getElementById(shID)) {
      if (document.getElementById(shID+'-show').style.display !== 'none') {
         document.getElementById(shID+'-show').style.display = 'none';
         document.getElementById(shID).style.display = 'block';
      }
      else {
         document.getElementById(shID+'-show').style.display = 'inline';
         document.getElementById(shID).style.display = 'none';
      }
   }
}

function newDoc(page) 
{

    var url = location.href
    if(url.indexOf("devel") != -1)
    {
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            page = page + "-list"
            window.location.assign("http://cse-devel.uta.edu" + page + ".php");
        
        }
        else{
        window.location.assign("http://cse-devel.uta.edu" + page + ".php");
        }
    }else{
    
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            page = page + "-list"
            window.location.assign("http://cse.uta.edu" + page + ".php");
        
        }
        else{
        window.location.assign("http://cse.uta.edu" + page + ".php");
        }
    
    } 
}

function sortOption(page) 
{

    var url = location.href
    if(url.indexOf("devel") != -1)
    {
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            //page = page + "-list"
            window.location.assign("http://cse-devel.uta.edu" + page );
        
        }
        else{
        window.location.assign("http://cse-devel.uta.edu" + page );
        }
    }else{
    
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            //page = page + "-list"
            window.location.assign("http://cse.uta.edu" + page );
        
        }
        else{
        window.location.assign("http://cse.uta.edu" + page );
        }
    
    } 
}

</script>
<style type="text/css">
.more {
      display: none;
}
</style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-32021828-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
<p class="skip"><a accesskey="C" href="#content-area">Skip to content</a>. <a accesskey="N" href="#nav">Skip to main navigation</a>.</p>
<div id="wrapper">
<header id="page-header"><div id="desktop-header"><a href="http://www.uta.edu/uta/"><img alt="The University of Texas at Arlington" height="48" id="UTA-logo" src="../_images/elements/uta.png" width="327"/></a> <a href="http://www.uta.edu/engineering/"><img alt="College of Engineering" height="48" id="COE-logo" src="../_images/elements/college_of_engineering.png" width="311"/></a> <a href="../index.php"><img alt="Department of Computer Science and Engineering" id="logo" src="../_images/elements/department_of_computer_science_and_engineering.png"/></a></div>
<div id="mobile-header"><!-- It is important that some images have no size attributes for rendering on different screen sizes -->
<div id="mobile-uta-logo"><a href="/uta/index.php"><img alt="The University of Texas at Arlington" class="UTA-logo-m" src="../_images/elements/mobile-header.png"/></a><br/><a href="../index.php"><img alt="Department of Computer Science and Engineering" class="UTA-logo-m" src="../_images/elements/mobile-department-header.png"/></a></div>
<div id="mobile-menu"><a href="#_top" onclick="toggle('navcontainer');" onkeypress="toggle('navcontainer');"><img alt="Menu" src="../_images/elements/menu.png"/></a></div>
</div>
<img alt="Department of Bioengineering at The University of Texas at Arlington" class="print-only" src="../_images/elements/print-header.png"/></header>
<div id="top-line"></div>
<div id="content-wrapper">
<nav id="navcontainer"><ul id="nav">
<li><a href="../index.php">Home</a></li>
<li><a href="../news-highlights/index.php">News</a></li>
<li><a class="with-submenu" href="#_top" id="admission-link" onclick="togglemenu('submenu-admission','admission-link');" onkeypress="togglemenu('submenu-admission','admission-link');">Future Students</a>
<ul class="submenu" id="submenu-admission">
<li><a href="../future-students/under-app.php">Undergraduate Applicants</a></li>
<li><a href="../future-students/grad-app.php">Graduate Applicants</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="current-link" onclick="togglemenu('submenu-current','current-link');" onkeypress="togglemenu('submenu-current','current-link');">Current Students</a>
<ul class="submenu" id="submenu-current">
<li><a href="../current-students/undergraduate-studies.php">Undergraduate Study</a></li>
<li><a href="../current-students/graduate-studies.php">Graduate Study</a></li>
<li><a href="../current-students/courses-catalog-schedules.php">Courses, Catalog &amp; Schedules</a></li>
<li><a href="../current-students/student-clubs.php">Student Clubs/Organizations</a></li>
<li><a href="../current-students/student-services.php">Student Services</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="people-link" onclick="togglemenu('submenu-people','people-link');" onkeypress="togglemenu('submenu-people','people-link');">People</a>
<ul class="submenu" id="submenu-people">
<li><a href="../faculty-directory.php">Faculty Directory</a></li>
<li><a href="../staff-directory.php">Staff Directory</a></li>
<li><a href="../current-students/phd-students/directory-grid.php">PhD Student Directory</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="research-link" onclick="togglemenu('submenu-research','research-link');" onkeypress="togglemenu('submenu-research','research-link');">Research</a>
<ul class="submenu" id="submenu-research">
<li><a href="../research/centers-labs.php">Centers &amp; Labs</a></li>
<li><a href="../research/research-areas.php">Research Areas</a></li>
<li><a href="../research/technical-reports.php">Technical Reports</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="seminars-link" onclick="togglemenu('submenu-seminars','seminars-link');" onkeypress="togglemenu('submenu-seminars','seminars-link');">Seminars</a>
<ul class="submenu" id="submenu-seminars">
<li><a href="invited_talks.php">Invited Talks</a></li>
<li><a href="ms-defenses.php">MS Defenses</a></li>
<li><a href="phd-defenses.php">PhD Defenses</a></li>
</ul>
</li>
<li><a href="../staff-directory.php">Contacts</a></li>
</ul>   <div id="menu-bottom-extra">
<div id="talks-and-defenses">
<p><strong>Invited Talks</strong></p><img src="/talks/images/majewicz.png" style="margin-left:10px;"  /><p><b>Fri 05/01</b> 1:30pm - 3:00pm<br /> Ann Majewicz, PhD - <i>Designing Human-in-the-Loop Systems for Surgical  Training and Intervention - <b>ERB 103</b></i> <img id="refreshment" src="/talks/images/pizza.jpeg" /></p><hr /><p><a href="/talks/invited_talks.php">Details and More Talks ></a></p><hr /></div>
<p><strong>Computer Science<br/>and Engineering</strong></p>
<p><span class="underline" data-mce-mark="1">Administration</span><br/> Dr. Lynn Peterson<br/> Interim Chair, Associate Dean<br/> 817-272-3605 <br/> <a href="mailto:peterson@uta.edu">peterson@uta.edu</a><br/><br/>Dr. Ramez Elmasri<br/> Associate Chair<br/> 817-272-2337 <br/> <a href="mailto:chuong@uta.edu">elmasri@uta.edu</a></p>
<p><span class="underline">Deliveries</span><br/> Engineering Research Building<br/> Room 640<br/>500 UTA Blvd.<br/> Arlington, TX 76010<br/><br/><span class="underline">Mailing Address</span><br/> Box 19015<br/> Arlington, TX 76019<br/><br/> <span class="underline">Phone/Fax/Email</span><br/> Phone: 817-272-3785<br/> Fax: 817-272-3784<br/><br/> <span class="underline">Visitors</span><br/><a href="http://www.uta.edu/maps/index.php?id=23">Map</a><br/><a href="http://www.uta.edu/pats/parking/guest-parking.php">Parking</a> <br/><a href="http://www.uta.edu/admissions/visit/tours-virtual/">Virtual Tour</a></p>
</div></nav>
<aside><h1>Faculty Highlight</h1>
<div class="cycle-slideshow" data-cycle-fx="fade" data-cycle-next="#next" data-cycle-pause-on-hover="true" data-cycle-prev="#prev" data-cycle-random="true" data-cycle-slides="&gt; div" data-cycle-timeout="3000">
<div><img align="middle" alt="Dr. Taylor Johnson" src="../_images/faculty/johnson.jpg"/>
<h3><a href="../faculty-highlights/2014-8-3-taylor-johnson-highlight.php">Taylor Johnson, PhD</a></h3>
<p>Developing novel formal verification methods to ensure correct operation of cyber-physical systems.</p>
</div>
<div><img align="middle" alt="Dr. Junzhou Huang" src="../_images/faculty/jzhuang.jpg" style="max-height: 150px;"/>
<h3><a href="../faculty-highlights/2015-2-5-junzhou-huang-highlight.php">Junzhou Huang, PhD</a></h3>
<p>Developing efficient algorithms with nice theoretical guarantees to solve practical problems involved huge scale data.</p>
</div>
</div>
<hr/>
<h6>Undergraduate Applicants</h6>
<ul>
<li><a href="../contact-undergraduate-engineering.php">Let Us Know About You</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/">University Catalog</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/#courseinventory">Courses</a></li>
<li><a href="../future-students/under-app.php">Apply</a></li>
</ul>
<h6>Master's Applicants</h6>
<ul>
<li><a href="../contact-graduate-engineering.php">Request Information</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/">University Catalog</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/#courseinventory">Courses</a></li>
<li><a href="../future-students/grad-app.php">Apply</a></li>
</ul>
<h6>Ph.D. Applicants</h6>
<ul>
<li><a href="../contact-graduate-engineering.php">Request Information</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/">University Catalog</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/#courseinventory">Courses</a></li>
<li><a href="../future-students/grad-app.php">Apply</a></li>
</ul>  
<div id="aside-clear"></div>
</aside>
<div class="with-aside" id="content-area"> 
<section>
<article>
<header>
<h1> PhD Dissertation Defenses</h1>
</header>
<div>
<p><h2>Past Defenses</strong></h2><div style="border-bottom: 1px dotted #CCC;"><p><strong>NERGY EFFICIENT FRAMEWORKS FOR PARTICIPATORY URBAN SENSING</strong><br /> Monday, April 20, 2015<br /> Adnan Rahath Khan<br /><p><a href="#" id="609-show" class="showLink" onclick="showHide(609);return false;">Read More</a></p></p><div id="609" class="more"><p><a href="#" id="609-hide" class="hideLink" onclick="showHide(609);return false;">Hide</a></p><p><strong>Abstract: </strong>Participatory sensing is a powerful paradigm in which users participate in the sensing campaign by collecting and crowdsourcing fine-grained information and opinions about events of interest (such as weather or environment monitoring, traffic conditions or accidents, crime scenes, emergency response, healthcare and wellness management), thus leading to actionable inferences and decisions. Based on the nature of user involvement, participatory sensing applications can be of two types - automated and user manipulated. The first type of applications automatically collects data samples from smartphone sensors and sends them to the server. On the other hand, the second type of applications depends on the users to manually collect data samples and upload them at their convenience. Because of the high density of smartphone users in urban population and ease of participation, the automated participatory sensing paradigm can be effectively applied to continuous monitoring of various phenomena in urban scenarios (e.g., fine-grained temperature monitoring, noise or air pollution), leading to what is called participatory urban sensing - the subject of study in this dissertation.</p>  <p>However, for creating a fine-grained and real-time map of the monitored area, the data samples need to be collected continuously (at a high frequency) which poses several research challenges. First, how to ensure coverage of the collected data that reflects how well the targeted area is monitored? For effective data coverage, traditional approaches usually depend on the server to track each participating smartphone, thus imposing significant communication overhead and adversely impacting the battery life. Second, how to localize the smartphones since continuous usage of the location sensor (e.g., GPS) can drain the battery in few hours? Third, how to provide energy efficiency in the data collection process by collecting minimum number of data samples in each data collection round? Finally, how to store and backup the huge amount of collected data resulting from continuous monitoring?</p>  <p>In this dissertation, we first propose a novel framework called PLUS to address three major issues in real-time participatory urban monitoring applications, namely, ensuring coverage of the collected data, localization of the participating smartphones, and overall energy efficiency of the data collection process. Specifically the PLUS framework can guarantee a specified requirement of partial data coverage of the monitored area in an energy efficient manner. Additionally we devised a Markov-Predictor based energy efficient outdoor localization scheme for the mobile devices to participate in the data collection process. Simulation studies and real life experiments exhibit that PLUS can significantly reduce energy consumption of the mobile devices for urban monitoring applications as compared to traditional approaches. We extend the idea of PLUS and further propose another framework called STREET that can ensure k-coverage of the collected data from an urban street network. By simulating an urban monitoring application on a street network, we demonstrate that STREET can achieve k-coverage of the collected data while consuming significantly less amount of energy especially in busy urban area. Next, we propose PeerVault - a reliable online storage and backup service for the collected data based on a peer to peer (P2P) architecture. PeerVault is built on a graph theoretic approach to exploit long term online availability and unused resources of computing devices, and a distributed monitoring algorithm to form an online backup service. Experimental results on real traces confirm that PeerVault can be served as a cheap alternative for online data backup service with high availability and long term reliability.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Design and Analysis of Place Based Opportunistic Networks</strong><br /> Friday, April 17, 2015<br /> Yanliang Liu<br /><p><a href="#" id="612-show" class="showLink" onclick="showHide(612);return false;">Read More</a></p></p><div id="612" class="more"><p><a href="#" id="612-hide" class="hideLink" onclick="showHide(612);return false;">Hide</a></p><p><strong>Abstract: </strong>Nowadays capacity rich smart mobile devices are increasingly becoming the center of our everyday digital life, exemplified by popular smart phones and tablets. Driven by the potential of peer interactions among these devices on the move, opportunistic networks have been under intensive research and development in both the academia and industry. Different from conventional networks exemplified by the Internet, opportunistic networks are characterized by intermittent connectivity induced from opportunistic contacts among mobile devices.</p>  <p>Generally speaking, opportunistic networks exploit the potential capability of existing mobile devices carried by people to provide pervasive computing service, such as data forwarding, without pre-planted infrastructures. The movement of mobile devices introduced by human activity plays a crucial role in the construction of opportunistic networks, as it influences the contacts among mobile devices. It has been well-known that human movement presents place-centered, repetitive features, including intermittent movement among places and long stationary period at a place. These relatively stable human activity within a place could provide longer contact time and higher data exchange volume between mobile devices.</p>  <p>Based on the above observation, in this dissertation we propose a new type of opportunistic network termed Place Based Opportunistic Networks. We detail the concept of place based opportunistic networks and  investigate various topics on this new type of networks, ranging from basic theoretical understanding to potential applications operating thereon.</p> <p>Specifically, we mainly focus on the following topics. Firstly, since the place-centered message exchange feature of the new networks, the localization functionality is required to ensure where the message exchange takes place. To complement existing various localization schemes, we propose an energy efficient indoor localization scheme that could achieve desirable localization accuracy based on context information. Secondly, to better understand the characteristics of the new networks from the macro perspective, we study the two important topics: the network capacity and routing design. We propose a two-layer mathematical model to analyze the potential network capacity by solving a maximum flow problem. And we also propose two routing schemes specifically for the new networks, based on the place popularity information and the place data congestion information separately. In the end, we are interested in applying the new place based opportunistic networks into the reality. Briefly, we propose and implement a mobile applications recommendation system on the top of the potential data pool constructed by the new networks.</p></p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>AUTOMATIC TEST CASE GENERATION WITH DYNAMIC SYMBOLIC EXECUTION FOR PROGRAMS THAT ARE CODED AGAINST INTERFACES AND ANNOTATIONS OR USE NATIVE CODE</strong><br /> Thursday, April 16, 2015<br /> Mainul Islam<br /><p><a href="#" id="611-show" class="showLink" onclick="showHide(611);return false;">Read More</a></p></p><div id="611" class="more"><p><a href="#" id="611-hide" class="hideLink" onclick="showHide(611);return false;">Hide</a></p><p><strong>Abstract: </strong>It is important to generate useful test cases to ensure the quality of software programs. Modern software programs are built upon code written in different programming languages. Automatic test case generation for such programs is very powerful but suffers from a key limitation. That is, most current test case generation techniques fail to cover testee code when covering that code requires additional pieces of code that are not yet part of the program under test. To address some of these cases, the Pex state-of-the-art test case generator can generate basic mock code. However, current test case generators cannot handle cases in which the code under test uses multiple interfaces, annotations, or reflection.</p> <p>An advanced high-level programming language like Java often uses platform-specific (native) code written in other low-level languages for faster execution of the program. Testing of such multi-level software programs is a challenging task because of the architectural difference of the programming languages at each level. Current test case generators suffer to produce good test cases for such programs as they do not usually analyze the underlying native code in the context of the high-level program.</p>   <p>This dissertation consists of two parts. In the first part, we describe a novel technique for generating test cases and mock classes for programs in which the code under test uses multiple interfaces, annotations, or reflection. The technique consists of collecting constraints on interfaces, annotations, and reflection, combining them with program constraints collected during dynamic symbolic execution, encoding them in a constraint system, solving them with an off-the-shelf constraint solver, and mapping constraint solutions to test cases and custom mock classes. We demonstrate the value of this technique on open source applications. Our approach covered such third-party code with generated mock classes, while competing approaches failed to cover the code and sometimes produced unintended side-effects such as filling the screen with dialog boxes and writing into the file system.</p> <p>In the second part, we describe a novel technique that combines dynamic symbolic execution for both high-level Java programs and for low-level native code to generate test cases. In this technique, the constraints encountered while executing the lower-level native code for specific concrete values are mapped into higher-level constraints to build an aggregated constraint system. The whole constraint system is then solved by the constraint-solver to extract useful test inputs. We demonstrate our technique on Java programs and show that the generated test cases reduce the number of false positives and achieve higher code coverage than the test cases generated without native code analysis.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Personalized profile based Learning system for Power Management in Android</strong><br /> Wednesday, April 15, 2015<br /> Ashwin Arikere<br /><p><a href="#" id="616-show" class="showLink" onclick="showHide(616);return false;">Read More</a></p></p><div id="616" class="more"><p><a href="#" id="616-hide" class="hideLink" onclick="showHide(616);return false;">Hide</a></p><p><strong>Abstract: </strong>Mobile computing devices are becoming more ubiquitous everyday due to the phenomenal growth in technology powering them. With the amount of computing power available in these devices, users are capable of achieving a multitude of tasks that were only possible with a PC just a few years ago. However, these devices still face issues regarding power management. Battery technology has not kept pace with the development in other areas. With a limited supply of energy, the mobile device of today requires a fine balance of power management to provide adequate energy to support the heavy duty computing of the user while simultaneously enabling the device to stay alive for a long duration.</p>  <p>With such limitations, the onus is more on the user to limit his usage of the device and its features to conserve power, thus having a crippling effect on the user's operation of devices. This research effort analyzes how devices are used and explores the effect of demographics on power consumption. We also propose a solution which will adapt to the individual user and provide a customized power saving mechanism tailored to the user's usage of his/her device. The adaptive system will learn what type of apps are used by the user and can intelligently make decisions to conserve power based on prior learnings.  It is estimated that such a mechanism will have an improvement on battery life by 15%.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>METHODS FOR HUMAN MOTION ANALYSIS FOR AMERICAN SIGN LANGUAGE RECOGNITION AND ASSISTIVE ENVIRONMENTS</strong><br /> Thursday, April 02, 2015<br /> Zhong Zhang<br /><p><a href="#" id="607-show" class="showLink" onclick="showHide(607);return false;">Read More</a></p></p><div id="607" class="more"><p><a href="#" id="607-hide" class="hideLink" onclick="showHide(607);return false;">Hide</a></p><p><strong>Abstract: </strong>The broad application domain of the work presented in this thesis is human motion analysis with a focus on hand detection for American Sign Language recognition and fall detection for assistive environments.</p> <p>One of the motivations of the proposed thesis is a semi-automatic vision based American Sign Language recognition system. This system allows a user to submit as query a video of the sign of interest, or simply perform the sign in front of a camera. The system then asks the user to annotate the hands’ locations in the sign. Next, the hand trajectory of the query sign is compared with the models in a large sign database to find the best matches. At last, the user reviews the top results to verify which of them best matches the query sign. Towards making the system more automatic, a novel hand detection method is introduced which is a combination of four representative hand detection methods published in these years.</p> <p>On the topic of fall detection for assistive environments, the work in this thesis aims at improving the safety of patients and elderly persons living unaccompanied at home. More specifically, this thesis proposes a fully automatic vision based fall detection method which can serve as a component of a home monitoring system for elderly people. The major contributions of the fall detection work can be summarized as: (i) This thesis collects three kinds of fall datasets using Microsoft Kinect depth cameras: non-occlusion dataset, partial occlusion dataset and complete occlusion dataset. The non-occlusion dataset refers to the performer being always visible to the camera when he/she falls down. A partial occlusion fall refers to a fall where part of the body is occluded by a certain object when the person performs the fall action. When the end of a fall is totally occluded by a certain object, like a bed, the fall is called a complete occlusion case. All of these datasets are freely available online, together with annotations marking the beginning and end of each fall event. As far as we know, this is the first public fall datasets captured by depth camera. These datasets will enable researchers to explore their own fall detection methods. (ii) This thesis proposes a statistical fall detection method based on a single Kinect depth camera, that makes a decision based on information about how the human moved during the last few frames. Our method proposes novel features to be used for fall detection, and combines those features using a Bayesian framework. The proposed method is quantitatively compared with three most related publications which also use a single depth camera on the collected datasets. Experimental results demonstrate that the proposed method obtains much better detection accuracy than other competitors on non-occlusion and partial occlusion datasets. As for the complete occlusion dataset, although the proposed method does not get the best detection accuracy, the evaluation between the proposed method and the competitors can be taken as a benchmark for the assessment of more advanced fall detection method.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>EFFICIENT IMPLEMENTATIONS OF SPARSE CODING BASED CLASSIFICATION: TOWARDS EFFICIENCY AND ACCURACY</strong><br /> Wednesday, March 18, 2015<br /> Soheil Shafiee<br /><p><a href="#" id="606-show" class="showLink" onclick="showHide(606);return false;">Read More</a></p></p><div id="606" class="more"><p><a href="#" id="606-hide" class="hideLink" onclick="showHide(606);return false;">Hide</a></p><p><strong>Abstract: </strong>With the fast growing deployment of machine intelligence in several real-life applications, there are always increasing needs for faster and more precise machine learning algorithms, especially classification and object recognition algorithms. One of the most recent methods proposed for this purpose is Sparse Representation-based Classification which works based on the emerging theory of Compressive Sensing. SRC shows excellent classification results in comparison to many well-known classification approaches. However, despite its high recognition power, SRC suffers from high computational and memory costs since it directly uses all available original samples as representatives to build its training model. Given high recognition rates of SRC, it becomes important to reduce the time and memory requirements of this method while preserving its accuracy. These improvements help SRC to be a more practical solution especially to be used on portable devices. This research investigates different representative reduction approaches in the SRC context on multiple heterogeneous datasets and proposes a training model to be used in SRC framework by using fewer but more informative representatives for the training space. We also investigate how incorporating multiple modalities of the data helps to improve SRC outcomes by extending efficient SRC implementations to multi-modality schemes and introducing three different approaches for this purpose. Experimental results show the proposed methods not only perform faster, but they also improve the classification accuracy on different datasets.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Simultaneous Optimization of Performance, Energy, and Temperature while Allocating Tasks to Multi-Cores</strong><br /> Monday, February 16, 2015<br /> Hafiz Fahad Sheikh<br /><p><a href="#" id="605-show" class="showLink" onclick="showHide(605);return false;">Read More</a></p></p><div id="605" class="more"><p><a href="#" id="605-hide" class="hideLink" onclick="showHide(605);return false;">Hide</a></p><p><strong>Abstract: </strong>Multi-core processors have emerged as a solution to the problem of ever-increasing demands for computing power. However, higher power dissipation levels resulting into thermal problems and increasing cooling costs are major factors limiting their scalability into larger systems. Therefore, dynamic thermal management (DTM) and dynamic power management (DPM) of multi/many-core systems have emerged as important areas of research.</p>  <p>The existing resource management approaches are either energy-aware or thermal-aware. In this dissertation, we focus on a new problem of simultaneous performance (P), energy (E), and temperature (T) optimized scheduling (PETOS) for allocating tasks to multi-core systems. To allocate a set of parallel tasks to a set of cores, we propose a static multi-objective evolutionary algorithm (MOEA)-based task scheduling approach for determining Pareto optimal solutions with PET-optimized schedules defining the task-to-core mappings and the corresponding voltage/frequency settings for the cores. Our algorithm includes problem-specific techniques for solution encoding, determining the initial population of the solution space, and the genetic operators that collectively work on generating efficient solutions in fast turnaround time. We also propose a methodology to select one solution from the Pareto front given the user preference describing the related P, E, and T goals. We show that the proposed algorithm is advantageous in reducing both energy and temperature together rather than in isolation. We also propose a dynamic multi-objective optimization approach that can solve PETOS problem while taking into consideration the task and system model uncertainties.</p>  <p>Another contribution of this dissertation is the design of efficient heuristic algorithms that can generate a set of solutions to the PETOS problem. Central to each heuristic are strategies for task assignment and frequency selection decisions. The proposed set of heuristics includes several iterative, greedy, random, and utility function and model based methods to explore the scheduling decision space for the PETOS problem. We describe and classify these algorithms using a methodical classification scheme. The methods developed in this dissertation obtain multiple schedules with diverse range of values for makespan, total energy consumed, and peak temperature, and thus present efficient ways of identifying trade-offs among the desired objectives for a given application and architecture pair.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Discovery Of Anomalous Patterns Within Multidimensional, Asynchronous Time-Series With An Emphasis On The "Internet Of Things"</strong><br /> Tuesday, November 04, 2014<br /> Steve Emmons<br /><p><a href="#" id="600-show" class="showLink" onclick="showHide(600);return false;">Read More</a></p></p><div id="600" class="more"><p><a href="#" id="600-hide" class="hideLink" onclick="showHide(600);return false;">Hide</a></p><p><strong>Abstract: </strong>In this dissertation we examine "Internet-scale" systems that present us with multidimensional time-series data characterized by many sources sending symbols at irregular intervals over a common channel. We explore a unique method for the discovery of hidden populations of similar sources and their previously-unknown behavioral patterns, and using these discoveries, reveal anomalous sources and/or time-frames based on their statistical properties. To do so, we employ several well-studied mechanisms, such as k-means and Principle Component Analysis (PCA), and bring to bear analysis tools from other disciplines, such as the use of n-grams and "motifs," that have not previously been considered in these contexts. While applicable to the study of any system whose attributes can map to the generalized model we present, the approach is of particular interest when dealing with large numbers of remote devices that make up the "Internet of Things" (IoT). The method is applied to several discrete layers of cellular wireless communications infrastructure for a diverse set of commercial Machine-to-Machine (M2M) applications where the behavior of the devices was examined as they interacted with carrier network elements. While it is common to study these systems in the context of their application-level duties, the devices' behavior at lower levels of the solution stack has received less attention, and is often poorly understood by either the application engineers or network operators.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>INTEGRATIVE APPROACHES FOR BIOLOGICAL NETWORK INFERENCES</strong><br /> Friday, July 18, 2014<br /> DONG-CHUL KIM<br /><p><a href="#" id="597-show" class="showLink" onclick="showHide(597);return false;">Read More</a></p></p><div id="597" class="more"><p><a href="#" id="597-hide" class="hideLink" onclick="showHide(597);return false;">Hide</a></p><p><strong>Abstract: </strong>Inferring biological networks from high-throughput bioinformatics data is one of the most interesting areas in the systems biology research in order to elucidate cellular and physiological mechanisms. In this thesis, network inference methods are proposed to solve biological problems.   We first investigated how the exposure to low dose ionizing radiation (IR) affects the human body by observing the signaling pathway associated with Ataxia Telangiectasia mutated using Reverse Phase Protein Array and isogenic human Ataxia Telangiectasia cells under different amounts and durations of IR exposure. DNA damage-caused pathways are derived from learning Bayesian networks in integration with prior knowledge such as Protein-Protein Interactions and signaling pathways from well-known databases. The experimental results show which proteins are involved in signaling pathways under IR, how the inferred pathways are different under low and high doses of IR, and how the selected proteins regulate each other in the inferred pathways.  In network inference research, there are two issues to solve. First, depending on the structural or computational model of inference method, the performance tends to be inconsistent due to innately different advantages and limitations of the methods. Second, sparse linear regression that is penalized by the regularization parameter and bootstrapping-based sparse linear regression methods were suggested as state of the art in recent related works for network inference. However, they are not effective for a small sample size data and also a true regulator could be missed if the target gene is strongly affected by an indirect regulator with high correlation or another true regulator. To solve the limitations of bootstrapping, a lasso-based random feature selection algorithm is proposed to achieve better performance. In order to elucidate the overall relationships between gene expressions and genetic perturbations, we propose a network inference method to infer gene regulatory network where Single Nucleotide Polymorphism (SNP) is involved as a regulator of genes. In the most of the network inferences named as SNP-Gene Regulatory Network (SGRN) inference, pairs of SNP-gene are given by separately performing expression Quantitative Trait Loci (eQTL) mappings. A SGRN inference method without pre-defined eQTL information is proposed assuming a gene is regulated by a single SNP at most.  We also studied how a medicine can be customized to individual patients considering biological features of the patients, i.e., Personalized Medicine. Our goal is to predict drug sensitivity levels of cancer patients in order to provide an optimal drug to the patients avoiding a waste of time with ineffective treatments. For the classification of patients to the optimal drug, we employed Bayesian Network Classifier (BNC) that consists of two components, parameters and network structure. Since the networks of BNC represent the dependency of proteins, these multiple networks of BNCs for multiple drugs also provide important information of relationships between proteins in order to identify the biomarkers of a target cancer from the integration of multiple networks.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Inferring Answer Quality, Answerer Expertise, and Ranking in Question/Answer Social Networks</strong><br /> Friday, April 18, 2014<br /> Yuanzhe Cai<br /><p><a href="#" id="593-show" class="showLink" onclick="showHide(593);return false;">Read More</a></p></p><div id="593" class="more"><p><a href="#" id="593-hide" class="hideLink" onclick="showHide(593);return false;">Hide</a></p><p><strong>Abstract: </strong>Search has become ubiquitous mainly because of its usage simplicity. Search has made great strides in making information gathering relatively easy and without a learning curve. Question answering services/communities (termed CQA services or Q/A networks; e.g., Yahoo! Answers, Stack Overflow) have come about in the last decade as yet another way to search. Here the intent is to obtain good/high quality answers (from users with different levels of expertise) for a question when posed, or to retrieve answers from an archived Q/A repository. To make use of these services (and archives) effectively as an alternative to search, it is imperative that we develop a framework including techniques and algorithms for identifying quality of answers as well as the expertise of users answering questions. Finding answer quality is critical for archived data sets for accessing their value as stored repositories to answer questions. Meanwhile, determining the expertise of users is extremely important (and more challenging) for routing queries in real-time which is very important to these Q/A services – both paid and free. This problem entails an understanding of the characteristics of interactions in this domain as well as the structure of graphs derived from these interactions. These graphs (termed Ask-Answer graphs in this thesis) have subtle differences from web reference graphs, paper citation graphs, and others. Hence it is imperative to design effective and efficient ranking approaches for these Q/A network data sets to help users retrieve/search for meaningful information.   The objective of this dissertation is to push the state-of-the-art in the analysis of Q/A social network data sets in terms of theory, semantics, techniques/algorithms, and experimental analysis of real-world social interactions. We leverage “participant characteristics” as the social community is dynamic with participants changing over a period of time and answering questions at their will. The participant behavior seems to be important for inferring some of the characteristics of their interaction.   First, our research work has determined that temporal features make a significant difference in predicting the quality of answers because the answerer’s (or participant’s) current behavior plays an important role in identifying the quality of an answer. We present learning to rank approaches for predicting answer quality as compared to traditional classification approaches and establish their superiority over currently-used classification approaches. Second, we discuss the difference between ask-answer graphs and web reference graphs and propose the ExpertRank framework and several approaches using domain information to predict the expertise level of users by considering both answer quality and graph structure. Third, current approaches infer expertise using traditional link-based methods such as PageRank or HITS. However, these approaches only identify global experts, which are termed generalists, in CQA services. The generalist may not be the best person to answer an arbitrary question. If a question contains several important concepts, it is meaningful for a person who is an expert in these concepts to answer that question. This thesis proposes techniques to identify experts at the concept level as a basic building block. This is critical as it can be used as a basis for inferring expertise at different levels using the derived concept rank. For example, a question can be viewed as a collection of a few important concepts. For answering a question, we use the ConceptRank framework to identify specialists for answering that question. This can be generalized using concept taxonomy for classifying topics, areas, and other larger concepts using the primary concept of coverage.   Ranking is central to the problems addressed in this thesis. Hence, we analyze the motivation behind traditional link-based approaches, such as HITS. We argue that these link-based approaches correspond to statistical information representing the opinion of web writers for these web resources. In contrast, we address the ranking problem in web and social networks by using the ILP (in-link probability) and OLP (out-link probability) of a graph to help understand HITS approach in contexts other than web graphs. We have further established that the two probabilities identified correspond to the hub and authority vectors of the HITS approach. We have used the standard Non-negative Matrix Factorization (NMF) to calculate these two probabilities for each node. Our experimental results and theoretical analysis validate the relationship between ILOD approach and HITS algorithm.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Analysis and Modeling Techniques for Geo-Spatial Datasets</strong><br /> Thursday, April 17, 2014<br /> Kulsawasd Jitkajornwanich<br /><p><a href="#" id="594-show" class="showLink" onclick="showHide(594);return false;">Read More</a></p></p><div id="594" class="more"><p><a href="#" id="594-hide" class="hideLink" onclick="showHide(594);return false;">Hide</a></p><p><strong>Abstract: </strong>In recent years, spatio-temporal data has received a lot of attention and increasingly plays an important role in our everyday lives as we can witness from the fast-growing mobile technologies and its location-based application development. By spatio-temporal data, we mean data that is associated with specific spatial locations that change over time. For example, a cellphone or car with GPS will generate the object location at regular time intervals. Another example would be the track of a storm center as it moves. Spatio-temporal data could be thought of as a huge data warehouse, which contains hidden and meaningful information. However, to analyze the available spatio-temporal data directly from its original formats and locations is not easy because the data is often in a format that is difficult to analyze and is usually ‘big’. Our research goals focus on spatio-temporal datasets and how to summarize, model, and conceptualize them for analysis and mining. Four main parts of this dissertation include: 1) spatio-temporal knowledge representation, 2) identifying meaningful concepts from raw data, 3) converting raw data to conceptual data, and 4) analysis and mining of conceptual data.  In the first part of the dissertation, we look at the spatio-temporal datasets in general by considering spatio-temporal data semantics using techniques similar to those utilized in the “Semantic Web”. We work towards creating a spatio-temporal ontology framework, which can be used to represent and reason about spatio-temporal data. In the remaining parts, we focus on the spatio-temporal datasets in a specific domain, which is rainfall precipitation data in the hydrology domain. However, the techniques and methodology that we use can be adapted to different types of hydrological data such as soil moisture, water level, etc., as well as other types of big spatio-temporal data. Therefore, we propose a generalized framework for analyzing and mining big data in any given domain. The framework allows big data in a particular domain to be conceptually analyzed and mined by using ontologies and EER.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient Frameworks for Lifetime Maximization in Tree Based Sensor Network</strong><br /> Monday, April 14, 2014<br /> Kajal Arefin Imon<br /><p><a href="#" id="595-show" class="showLink" onclick="showHide(595);return false;">Read More</a></p></p><div id="595" class="more"><p><a href="#" id="595-hide" class="hideLink" onclick="showHide(595);return false;">Hide</a></p><p><strong>Abstract: </strong>In most wireless sensor network (WSN) applications, data are typically gathered by the sensor nodes and reported to a data collection point, called the sink. In order to support such data collection, a tree structure rooted at the sink is usually defined. Based on different aspects, including the actual WSN topology and the available energy budget, the energy consumption of nodes belonging to different paths in the data collection tree may vary significantly. This affects the overall network lifetime, defined in terms of when the first node in the network runs out of energy.  In this thesis, we address the problem of lifetime maximization of WSNs in the context of data collection trees through load balancing and data compression techniques. From load balancing perspective, we propose a novel and efficient algorithm, called Randomized Switching for Maximizing Lifetime (RaSMaLai) that intelligently changes the path (toward the sink) of sensors to distribute traffic load. We analytically show that, under appropriate settings of the operating parameters, RaSMaLai converges with a low time complexity. We further design a distributed version of our algorithm, called D-RaSMaLai. While D-RaSMaLai works on the same principals of RaSMaLai, the design of the distributed version is novel and energy efficient. Simulation results show that both the proposed algorithms outperform several existing approaches in terms of network lifetime. We also approach the lifetime maximization problem leveraging compression of sensor data streams. Compression of correlated data is one of the widely used techniques where the amount of transmitted data is minimized along their routes towards the sink. Existing works in this direction do not consider the temporal effect of correlation among data streams generated by periodic sensing. Moreover, the compression can introduce some imperfection that may affect the reliability of the collected data. In this thesis, we address the problem of energy efficient data gathering in WSNs while considering variability of correlation among data streams of neighboring sensors. We perform experiments on real data sets and show that our framework is very energy efficient, and contributes to lifetime maximization.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Online Efficient and Effective Search in Large and Noisy Sequence Databases</strong><br /> Friday, April 04, 2014<br /> Alexios Kotsifakos<br /><p><a href="#" id="591-show" class="showLink" onclick="showHide(591);return false;">Read More</a></p></p><div id="591" class="more"><p><a href="#" id="591-hide" class="hideLink" onclick="showHide(591);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis investigates the problem of similarity search in large and noisy sequence databases. A key application domain of interest in this work is the very challenging Query-By-Humming (QBH) problem, according to which, given a hummed part of a song, we would like to identify the closest matches in a large music repository. The problem of selecting the most appropriate, based on each specific query, distance measure out of a pool of measures for classification in time series data is also investigated. In addition, searching time series databases via examples, which may be either time series or models, is also part of this work. Moreover, motivated by the nature of music notes in the noisy QBH application, we explore the problem of interval-based sequence matching in a variety of application domains. More specifically, this thesis makes contributions both by defining novel similarity measures that are used to identify the best database matches, and by proposing methods to improve efficiency. Referring to the similarity measures, the thesis contributes a method, named SMBGT, for finding the closest subsequence matches from a large database to a given query. This measure is applied to the noisy QBH application domain, where the music pieces are represented as 2-dimensional time series. Since efficiency should be a key characteristic when searching large time series databases, ISMBGT is also proposed, which performs indexing on top of the SMBGT method in a filter-and-refine manner. Applying ISMBGT on synthetic and real query sets for QBH shows the usefulness of our indexing scheme. A second contribution of this thesis is the ``Hum-a-song&#039&#039 QBH system, which allows the user to select among a variety of distance measures and music representations in order to retrieve the closest songs to a hummed query. Apart from that, SMBGT is also exploited to perform genre classification of music pieces, which, among others, can be beneficial in the area of assistive environments. For example, music systems with therapeutic and educational functionalities need to be adapted based on the music preferences of the end-users, such as children, patients, or disabled. Being able to identify the genres of songs that fit to each category of end-users is imperative in such settings as it can assist effectively in medical treatments and learning tasks. Moreover, we address the problem of selecting the most appropriate distance measure out of a pool of measures, which depends on each specific query, for classification of time series. The proposed approach is, to the best of our knowledge, the first one to deal with this problem. The reason for the importance of such a problem is given through an example. Assuming that there are two available distance measures performing whole sequence matching, there may be cases where the first one may correctly classify a given query, while it may fail to classify another query, which is correctly classified by the second distance measure. Thus, it would be highly desirable to be able to choose the ``best&#039&#039 measure for each query. The classification accuracy of the proposed method compared to three state-of-the-art distance measures is significantly competitive on actual data. Furthermore, due to the fact that performing classification through the use of distance measures can be computationally expensive due to their quadratic complexity, we have explored the use of models, and specifically Hidden Markov Models (HMMs). HMMs are widely known and have been applied to a variety of domains, such as such as biology, speech recognition, and music retrieval, since they are able to model the underlying structure of sequences determining the relationships between their observations. Thus, in order to perform classification, we present a way of representing each class of a time series database with an HMM, and then perform searching based on the constructed models. The proposed indexing framework MTSI works in a filter-and-refine manner, and is shown to be both effective and efficient compared to a variety of distance-based measures on a large number of time series databases. Additionally, we have exploited HMMs so as to extract knowledge on what has happened in the past or to recognize what is happening in the present. Specifically, instead of searching a database of time series by providing a time series corresponding, e.g., to a specific activity, for finding the time series that are similar to that query, we can provide a model representing this activity. Comparing HMMs and several distance measures for such searches shows that our approach is particularly meaningful with excellent accuracy results. Finally, the problem of interval-based whole sequence matching has received limited attention, although it appears in many application domains, such as music informatics, sign language, medicine, geo-informatics, cognitive science, linguistics. Another contribution will be a novel distance mea</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Social analytics using tensor and sparse techniques</strong><br /> Friday, April 04, 2014<br /> Miao Zhang<br /><p><a href="#" id="592-show" class="showLink" onclick="showHide(592);return false;">Read More</a></p></p><div id="592" class="more"><p><a href="#" id="592-hide" class="hideLink" onclick="showHide(592);return false;">Hide</a></p><p><strong>Abstract: </strong>The development of internet and mobile technologies is driving an earthshaking social media revolution. They bring the internet world a huge amount of social media content, such as images, videos, comments, etc.  Those massive media content and complicate social structures require the analytic expertise to transform those flood of information into actionable strategies, because mining those data can help organizations take control of those data, therefore organizations can improve customer satisfaction, identify patterns and trends, and make smarter marketing strategies. Mining those data can also help the consumers to grasp the most important and convenient information from the overwhelming data sea.   By and large, there are three big constituents in social media content - users, resources/events and user&#039s tags on those resources. In this thesis, we study three key technology areas to explore the social media data. The first is viral marketing (word of mouth) technology: we try to identify the most influential individuals on the social networks. We propose highly efficient and scalable methods to calculate the influence spread and then different greedy strategies will be applied to find the most influential users.  Second, we tackle the 3D social tagging recommendation problem. Different from the traditional 2D recommender system, users are allowed to use short phrases, which refer to tags, to describe their social resources.  Therefore, there are three dimensions involved in tagging recommendation - the three constituents (users, items, tags) mentioned above.  Tag recommendation system helps the tagging process by advising a set of tags to the user that he may use for a specific item. The tagging information helps web sites to organize their resources, and also assist the users to communicate with each other.  We propose to use lower-order tensor decomposition techniques to tackle the extremely sparse social network data.  Last but not least, in the social tagging area, there are many types of social media resources, and image is a big component part.  We propose an efficient and robust model by applying tensor and L1 norm sparse coding techniques in the image representation and categorization areas.  We did extensive experiments on several real world data sets to evaluate our proposed models to the above three social network tasks, and experimental results demonstrate that our methods outperform state-of-the-art approaches consistently.   </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Dynamic Symbolic Data Structure Repair and Evaluation of Compilers, Program Analysis and Testing Tools</strong><br /> Friday, March 21, 2014<br /> Ishtiaque Hussain<br /><p><a href="#" id="590-show" class="showLink" onclick="showHide(590);return false;">Read More</a></p></p><div id="590" class="more"><p><a href="#" id="590-hide" class="hideLink" onclick="showHide(590);return false;">Hide</a></p><p><strong>Abstract: </strong>eneric automatic repair of complex data structures is a new and exciting area of research. Existing approaches can integrate with good software engineering practices such as program assertions. But in practice there is a wide variety of assertions and not all of them satisfy the style rules imposed by existing repair techniques. That is, a badly written assertion may render generic repair inefficient or ineffective. Moreover, the performance of existing approaches may depend on the location of an error in a corrupted data structure. This dissertation shows that generic automatic data structure repair can be implemented with full dynamic symbolic execution. Such an implementation can solve some of the problems of the existing generic repair approaches.   The dissertation also evaluates the usefulness of a novel random program generator, RUGRAT-Random Utility Generator for Program Analysis and Testing, for the evaluation and benchmarking of different Java source-to-bytecode compilers and pRogram Analysis and Testing (RAT) tools. I generated several programs in different size categories, ranging up to 5MLOC and used them to compare the various RAT tools and for finding bugs in them.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>New Matrix Completion Models for Social Information Retrieval Applications</strong><br /> Friday, November 22, 2013<br /> Jin Huang<br /><p><a href="#" id="581-show" class="showLink" onclick="showHide(581);return false;">Read More</a></p></p><div id="581" class="more"><p><a href="#" id="581-hide" class="hideLink" onclick="showHide(581);return false;">Hide</a></p><p><strong>Abstract: </strong>Many popular social web sites have emerged during the past decade and completely changed many users&#039 everyday live. Recently, social information retrieval models, where conventional information retrieval meets the social context of search and recommendation, have become the central topic in machine learning, data mining, information retrieval and many other areas.  A particular application of social information retrieval is the recommendation. Such recommendation ranges from classic recommendation movie rating recommendation in user-item matrices, trust and reputation modeling between members in any social network. If we model such recommendation in the form of matrices, then such recommendation can be formulated as recovering missing values in the matrices. This is a classic research topics and there are numerous literature papers regarding this.   In this dissertation, we propose a few different models in terms of social recommendation. Specifically, we develop different models to predict the trust between users in the discrete domain, trust and rating prediction via aggregating heterogeneous social networks, predicting the future events of users. We will introduce these models in different chapters, provide the mathematical deviation for the objective function optimization and demonstrate the effectiveness of these methods with other benchmark methods in each category. These methods provide new perspectives for discovering un-tagged relationships and predicting future events for social networks.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Image Annotation and Feature Engineering via Structural Sparsity and Low Rank Approximation</strong><br /> Friday, November 22, 2013<br /> Deguang Kong<br /><p><a href="#" id="589-show" class="showLink" onclick="showHide(589);return false;">Read More</a></p></p><div id="589" class="more"><p><a href="#" id="589-hide" class="hideLink" onclick="showHide(589);return false;">Hide</a></p><p><strong>Abstract: </strong>Nowadays, in order to sense environment and understand human behaviors, data analysis plays a more and more important role to handle heterogeneous data ranging from different domains, e.g., image categorization/annotation, customer segmentation,  traffic prediction, ad optimization, recommendation systems, privacy analysis, etc .  The large amount of multivariate data raises the fundamental problem of data mining: how to discover meaningful compact patterns hidden in the high-dimensional noisy observations?  One approach is to do dimension reduction, which finds the low-dimensional subspace and thus encodes data in a low-dimensional structure.  The other approach is to do feature selection or feature engineering, which manipulates the features to capture the most discriminant patterns for classification/clustering tasks.    In this thesis, to further improve the low-dimensional embedding results, an iteratively locally linear embedding algorithm is proposed, which captures the global structure of non-linear manifold through iteratively updating the embedding.   To handle noisy data (e.g., data with missing values, corrupted values) classification problem, a robust data recovery model via schatten-p norm is proposed to preprocessing the noisy data.   To handle noisy data clustering problem, a robust nonnegative matrix factorization model via L21 norm is proposed for robust clustering.  To utilize the feature structure with constraints, an efficient feature learning algorithm via group lasso is proposed to handle features on arbitrary structure. Extensive experiments indicate the good performance of proposed algorithms.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Novel Methods in Entity-Centric Information Exploration</strong><br /> Thursday, November 21, 2013<br /> Ning Yan<br /><p><a href="#" id="587-show" class="showLink" onclick="showHide(587);return false;">Read More</a></p></p><div id="587" class="more"><p><a href="#" id="587-hide" class="hideLink" onclick="showHide(587);return false;">Hide</a></p><p><strong>Abstract: </strong>We witness an unprecedented proliferation of entity graphs that capture entities (e.g., persons, products, organizations) and their relationships. Such entity data gradually evolves into the most comprehensive knowledge graph ever built by human beings. The flourish of entity data also boosts a growing demand for entity-centric information exploration tasks. Different from traditional information retrieval tasks that are based on statistics of text terms, entity-centric information exploration tasks are usually based on metrics over entities, their relationships, or graph structures. In this work, we identified two essential navigational tasks in entity-centric information exploration: 1) how to do faceted navigation over a set of homogeneous entities, utilizing entity relationships and concept hierarchies; 2) how to assist users in attaining a quick and rough preview for huge entity graphs. As an approach for the first problem, we proposed a novel method that dynamically discovers a query-dependent faceted interface for a set of Wikipedia articles resulting from a keyword query. We further extended this method into a general framework for faceted interface discovery for Web documents. Our model leverages the collaborative vocabularies in Wikipedia, such as its category hierarchy and intensive internal hyperlinks, for building faceted interfaces. We proposed metrics for ranking both individual facet hierarchies and faceted interfaces (each with k-facet hierarchies). We then developed faceted interface discovery algorithms that optimize these ranking metrics. As an approach for the second problem, we proposed a novel method that generates optimal preview tables for entity graphs. We studied scoring functions for measuring the goodness of previews. Based on the scoring measures, we formulated several optimization problems that look for previews with the highest scores, under various constraints on preview size and distance between preview tables. We proved that the optimization problem under distance constraint is NP-hard. We then designed a dynamic-programming algorithm and an Apriori-style algorithm for finding optimal previews. For both problems, we conducted experiments as well as user studies for evaluation purposes. The results demonstrated the efficiency and effectiveness of our proposed methods.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Hardware and Software Systems for Control of Assistive Robotic Devices using Point-of-Gaze Estimation</strong><br /> Wednesday, November 20, 2013<br /> Christopher McMurrough<br /><p><a href="#" id="586-show" class="showLink" onclick="showHide(586);return false;">Read More</a></p></p><div id="586" class="more"><p><a href="#" id="586-hide" class="hideLink" onclick="showHide(586);return false;">Hide</a></p><p><strong>Abstract: </strong>Eye gaze based interaction has many useful applications in human-machine interfaces, assistive technologies, and multimodal systems. Traditional input methods, such as the keyboard and mouse, are not practical in many situations and can be ineffective for some users with physical impairments. Knowledge of the user point of gaze (PoG) can be a powerful data modality in intelligent systems by facilitating intuitive control, perception of user intent, and enhanced interactive experiences.  This research aims to advance the use of non-traditional, multimodal interfaces in assistive robotic devices for the benefit of users with severe physical disabilities.  The data modalities which are of particular interest in this work are perception of the environment using 3D scanning and computer vision, estimation of the user point of gaze using video occulography, and perception of user intent during interaction with objects and locations of interest. A novel, mobile headset design is presented that provides both monocular and binocular pupil tracking together with a 3D reconstruction of the user&#039s field of view. Computational methods for obtaining a true 3D gaze vector and final PoG are also discussed, along with a demonstration of practical use. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Sparse and Large-Scale Learning Models and Algorithms for Mining Heterogeneous Big Data</strong><br /> Friday, November 15, 2013<br /> Xiao Cai<br /><p><a href="#" id="578-show" class="showLink" onclick="showHide(578);return false;">Read More</a></p></p><div id="578" class="more"><p><a href="#" id="578-hide" class="hideLink" onclick="showHide(578);return false;">Hide</a></p><p><strong>Abstract: </strong>With the development of PC, internet as well as mobile devices, we are facing a data exploding era. On one hand, more and more features can be collected to describe the data, making the size of the data descriptor larger and larger. On the other hand, the number of data itself explodes and can be collected from multiple resources. When the data becomes large scale, the traditional data analysis method may fail, suffering the curse of dimensionality and etc. In order to explore and analyze the large-scale data more accurately and more efficiently, based on the characteristic of the data, we propose several learning algorithms to mine the Heterogeneous data. To be specific, if the feature dimension is large, we propose several sparse learning based feature selection methods to select the key words from the text or to find the bio-marker from the gene expression data; if the number of data is huge and it is represented or collected by multiple resources, we propose graph based multi-modality model to do semi-supervised learning and clustering. We empirically evaluate each of our proposed models on several benchmark data sets and our methods can consistently achieve superior results with the comparison of state-of-art methods.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Design and Analysis of Location Centered Large-Scale Opportunistic Networks</strong><br /> Friday, November 15, 2013<br /> Shanshan Lu<br /><p><a href="#" id="579-show" class="showLink" onclick="showHide(579);return false;">Read More</a></p></p><div id="579" class="more"><p><a href="#" id="579-hide" class="hideLink" onclick="showHide(579);return false;">Hide</a></p><p><strong>Abstract: </strong>With the high penetration of the mobile devices and their equipments of wireless interfaces in people&#039s lives, the development of the scenario of Opportunistic Networks (OppNets) has become available. OppNets enable transmissions by exploiting temporary wireless links and connections opportunistically arising from mobility nature of nodes. The applications of OppNets have emerged in a wide range of areas.  OppNets have attracted a lot of attention from the wireless and mobile network research community, while little has been paid to the OppNets in large-scale scenario. This dissertation proposes a two-layer location centered large-scale opportunistic network architecture. The base layer handles connection which provides underlying technique to enable the device discovery and connection establishment. The upper layer deals with routing which delivers messages to targeted nodes/locations. The location centered design incorporates physical locations into routing. It has two purposes, one is to enable location targeted routing and another is to use users&#039 location information to aid the routing.  This dissertation proposes three routing schemes for location/node targeted routing in large-scale OppNets. Humans&#039 movements in large-scale scenario are explored to facilitate routing. Besides, these schemes are featured with advantages that are particularly desired for routing in large-scale OppNets, such as privacy preserving, scalable with network size, total distributed control etc. For the connection layer, a device discovery approach and connection establishment scheme using ad-hoc Wi-Fi is proposed in this dissertation. It discovers devices within a larger range compared to commonly used Bluetooth and is further enhanced with energy efficiency.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Adaptive Dialogue Systems for Assistive Living Environments</strong><br /> Friday, November 15, 2013<br /> Alexandros Papangelis<br /><p><a href="#" id="585-show" class="showLink" onclick="showHide(585);return false;">Read More</a></p></p><div id="585" class="more"><p><a href="#" id="585-hide" class="hideLink" onclick="showHide(585);return false;">Hide</a></p><p><strong>Abstract: </strong>Adaptive Dialogue Systems (ADS) are intelligent systems, able to interact with users via multiple modalities, such as text, speech, gestures, facial expressions and others. Such systems are able to make conversation with their users, usually on a specific, narrow topic. Assistive Living Environments are environments where the users are by definition not competent with technology, due to various factors, such as mental or physical disabilities, injuries, age and others. While technology that helps improve these people&#039s quality of life exists, many times they cannot access it due to inflexible interfaces. ADS, therefore, have the potential to bridge users and technology by acting as a mediator between them. There are several unique challenges posed by this problem, in addition to the challenges faced by a generic ADS. Our contributions to the state of the art focus on Online Dialogue Policy learning which, coupled with other methods we proposed, can lead to an ADS able to exhibit complex behaviour and appear more intelligent. As a consequence, users trust the system more and it becomes more functional as it is able to elicit behavioural information and use it, for example, to make basic diagnoses. We extensively evaluated the proposed algorithms and present our experimental setup and promising results.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Viewpoint Invariant Gesture Recognition and 3D Hand Pose Estimation using RGB-D</strong><br /> Friday, November 15, 2013<br /> Pavlos Doliotis<br /><p><a href="#" id="588-show" class="showLink" onclick="showHide(588);return false;">Read More</a></p></p><div id="588" class="more"><p><a href="#" id="588-hide" class="hideLink" onclick="showHide(588);return false;">Hide</a></p><p><strong>Abstract: </strong>The broad application domain of the work presented in this thesis is pattern classification with a focus on viewpoint invariant gesture recognition and 3D hand pose estimation. One of the main contributions of the proposed thesis is that it investigates and proposes new similarity measures for similarity search in multimedia databases given the task of 3D hand pose estimation using RGB-D. At the same time, towards making 3D hand pose estimation methods more automatic, a novel hand segmentation method is introduced which relies on depth data. Experimental results demonstrate that the use of depth data increases the discrimination power of the proposed method. On the topic of gesture recognition, a novel method is proposed that combines a well know similarity measure, namely the Dynamic Time Warping (DTW), with a new hand tracking method which is based on depth frames captured by Microsoft&#039s Kinect sensor. When DTW is combined with the near perfect hand tracker gesture recognition accuracy remains high even in very challenging datasets, as demonstrated by experimental results. Another main contribution of the current thesis is an extension of the proposed gesture recognition system in order to handle cases where the user is not standing fronto-parallel with respect to the camera. Our method can recognize gestures captured under various camera viewpoints. At the same time our depth hand tracker is evaluated against one popular open source user skeleton tracker by examining its performance on random signs from a dataset of American Sign Language (ASL) signs. Our structured motion dataset of (ASL) signs has been captured in both RGB and depth format using a Microsoft Kinect sensor and it will enable researchers to explore body part (i.e. hands) detection and tracking methods, as well as gesture recognition algorithms. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Multimodal interaction in ambient intelligence environments using speech, localization and robotics</strong><br /> Thursday, November 14, 2013<br /> Georgios Galatas<br /><p><a href="#" id="584-show" class="showLink" onclick="showHide(584);return false;">Read More</a></p></p><div id="584" class="more"><p><a href="#" id="584-hide" class="hideLink" onclick="showHide(584);return false;">Hide</a></p><p><strong>Abstract: </strong>An Ambient Intelligence Environment is meant to sense and respond to the presence of people, using its embedded technology. In order to effectively sense the activities and intentions of its inhabitants, such an environment needs to utilize information captured from multiple sensors and modalities. By doing so, the interaction becomes more natural as well as accurate and robust. We have focused on 3 aspects of such an environment, using speech, localization and robotics. Speech is one of the most natural forms of communication for humans. Therefore, it can be used as one of the main information sources for deriving the intentions and needs of a person. In our work, we have extended the traditional speech recognition paradigm by introducing 3 dimensional visual articulation information for recognizing spoken words. The development of our system included the capture of a novel dataset, implementation and extended testing under a variety of audio-visual noise types, demonstrating the usefulness of 3D visual information for this task. Additionally, person localization and identification is of paramount importance in a smart environment, since by knowing each person&#039s location, her/is actions can be derived and abnormal patterns can be recognized. Our implementation conducts person identification by means of RFID. Furthermore, three types of input are combined for multi-person localization, namely, skeletal tracking, audio localization and RFID signal strength. The system was deployed and tested in our simulated assistive apartment exhibiting high accuracy. Finally, every domestic environment changes dynamically over time, creating the need for altering the position, orientation and type of sensors used within it. In our approach, we developed a framework of sensor bearing robots with the ability to relocate automatically to compensate for such a dynamic environment. Their positioning is done in such a way so as to maximize coverage. Navigation is carried out using visual information and autonomous placement uses a decentralized algorithm.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Exploratory Mining of Collaborative Social Content</strong><br /> Tuesday, November 12, 2013<br /> Mahashweta Das<br /><p><a href="#" id="582-show" class="showLink" onclick="showHide(582);return false;">Read More</a></p></p><div id="582" class="more"><p><a href="#" id="582-hide" class="hideLink" onclick="showHide(582);return false;">Hide</a></p><p><strong>Abstract: </strong>The widespread use and growing popularity of online collaborative content sites (e.g., Yelp, Amazon, IMDB) has created rich resources for consumers to consult in order to make purchasing decisions on various items such as restaurants, movies, ecommerce products, movies, etc. It has also created new opportunities for producers of such items to improve business by designing better products, composing succinct advertisement snippets, building more effective personalized recommendation systems, etc. This motivates us to develop a framework for exploratory mining of user feedback on items in collaborative social content sites. Typically, the amount of user feedback (e.g., ratings, reviews) associated with an item (or, a set of items) can easily reach hundreds or thousands resulting in an overwhelming amount of information (information explosion), which users may find difficult to cope with (information overload). For example, popular restaurants listed in the review site Yelp routinely receive several thousand ratings and reviews, thereby causing decision making cumbersome. Moreover, most online activities involve interactions between multiple items and different users and interpreting such complex user-item interactions becomes intractable too. Our research concerns developing novel data mining and exploration algorithms to formally analyze how user and item attributes influence user-item interactions. In this dissertation, we choose to focus on short user feedback (i.e., ratings and tags) and reveal how it, in conjunction with structural attributes associated with items and users, open up exciting opportunities for performing aggregated analytics. The aggregate analysis goal is two-fold: (i) exploratory mining to benefit content consumers make more informed judgment (e.g., if a user will enjoy eating at a particular restaurant), as well as (ii) exploratory mining to benefit content producers conduct better business (e.g., a redesigned menu to attract more people of a certain demographic group, etc.). We identify a family of mining tasks and propose a suite of algorithms - exact, approximation with theoretical properties, and efficient heuristics - for solving the problems. Performance evaluation over synthetic data and real data crawled from the web validates the utility of our framework and effectiveness of our algorithms.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Practical End-to-End Performance Evaluation of Backend Software Applications</strong><br /> Monday, November 11, 2013<br /> Tuli Nivas<br /><p><a href="#" id="580-show" class="showLink" onclick="showHide(580);return false;">Read More</a></p></p><div id="580" class="more"><p><a href="#" id="580-hide" class="hideLink" onclick="showHide(580);return false;">Hide</a></p><p><strong>Abstract: </strong>This dissertation makes contributions to four areas of performance testing - the test process itself, monitoring, automation and end-to-end performance evaluation of backend applications.  The first contribution deals with the testing process. Performance testing is a key element of industrial software development, but we still encountered the following two problems. (1) While testing textbooks prescribe writing tests against performance goals, we find that it is impractical to gather from business analysts performance goals that are detailed enough for finding subtle performance bugs. (2) Once performance tests are conducted, we were asked questions such as - how do you make sure that the results collected during testing are a good indication of how the code will function in production? To enable practitioners to address these problems, we introduce two additional performance testing process components, which we call release certification and test data correlation. Our key idea to address problem (1) is to run two different versions of the same subject application side-by-side in the same test environment and (2) is to correlate the performance measurements of the test and production environments. Running individual soak, load and stress tests to assess different aspects of an application is a tedious process. So we introduce a new test - impulse test that not only combines the characteristics of multiple existing tests but also enables testing for engineering aspects such throttles, alerts and timeouts.  The second contribution is in the area of instrumentation. Monitoring the performance of distributed applications is an important task in practical software engineering. Current monitoring tools are often limited in the range of computing platforms they support, which limits their utility in several business monitoring scenarios. Current monitoring tools can also impose a significant performance overhead. We describe our in house built EI Enterprise Instrumentation monitoring tool that addresses these issues. We compare EI with a state-of-the-art monitoring tool on a real online shopping application and describe our experience with EI as well as feedback we received from EI users. The third component of the study deals with test automation and scripting. Scarcity of commercially available testing tools that could support all native or application specific message formats as well as those that cater to non GUI or non-web based backend applications led to creating our own customized traffic generator and scripts. This study provides (1) the general design principles for a test script that can be used to generate traffic for any request format as well as (2) specific factors to keep in mind when creating a script that will work in a test environment that uses a mock. It will also address the (3) design and properties of a test harness. It provides a simple framework that can be easily used to complete an end-to-end testing process: pre test, traffic generation and post test activities.  Last but not the least the dissertation proposes a solution to assess performance for backend applications. State space models specifically Markov models are used extensively to predict anomalies, capacity and throughput in computer systems. In most cases existing solutions deal with network, application throughput and environment related bottlenecks. In this study we will use Markov models to proactively predict the performance of a software system that is dependent on multiple varying input parameters. The goal is to find pressure points/bottlenecks for the application when it interacts with other systems and/or middleware components and database. Pressure points are any application resource that can become exhausted thereby restricting or degrading service level performance such as CPU, memory, disk, network and so on.   All solutions proposed in this study have been implemented on real world travel applications, in most cases an airline shopping application. The data gathered and all measurements/plots in this study are from travel related pieces of code that are live and used by customers today.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Performance Analysis of Multiple Classes of Traffic in Wi-Fi Networks: Theoretical and Experimental Study</strong><br /> Friday, November 08, 2013<br /> Sajib Datta<br /><p><a href="#" id="583-show" class="showLink" onclick="showHide(583);return false;">Read More</a></p></p><div id="583" class="more"><p><a href="#" id="583-hide" class="hideLink" onclick="showHide(583);return false;">Hide</a></p><p><strong>Abstract: </strong>This dissertation develops analytical models for performance evaluation of Wi-Fi networks (IEEE 802.11e/g wireless LANs) and designs admission control strategy that guarantees the quality of service (QoS) requirements of  the accepted traffic. Guaranteeing QoS is a major challenge, particularly for real-time multimedia traffic such as voice and video. The QoS measures of interest are throughput, delay, collision probability, and packet loss probability. Our analytical models accurately capture the complexity of multiple classes of traffic in Wi-Fi networks; we also attain several valuable insights about the performance of IEEE 802.11 WLANs.  Our study is based on the infrastructure wireless local area networks (WLANs) setting, in which mobile stations (STAs) access a high speed LAN (e.g., Eethernet) via an access point (AP). After deriving analytical models for interactive voice calls and video-conferencing traffic, we extend this model to include transport control protocol (TCP) for file downloads. Our model includes downlink video streaming traffic. We estimate the maximum number of voice calls that can be supported with and without TCP, with and without video; and additionally the maximum video streaming rate when combined with voice over IP (VoIP) calls and TCP controlled file transfers. We also analyze the effect of arbitrary queue size on the voice capacity, video-conferencing capacity and throughput, TCP and video streaming throughput; and determine bandwidth utilization for variable packet arrival rates. Our proposed model is generic in the sense that it provides a high level system view approach, and some of the existing approaches for Wi-Fi performance analysis become special cases of our model for specific queue sizes.  The underlying method of our analytic model is to identify an embedded discrete-time Markov chain at random times. The Markov chain consists of the number of nonempty queues or mobile STAs, the number of packets is waiting therein and the variable to keep track of which traffic classes are allowed to attempt in a channel slot. Here, the channel slot is defined as the interval between two successive channel activities, which are any sort of action carried out by a mobile station at any time. The stationary probabilities of all the Markov states are derived by solving a set of linear equations, thus obtaining the desired performance measures of IEEE 802.11 WLANs.  Finally, we use the Qualnet simulator  to  simulate the Enhanced Distributed Channel Access (EDCA) model according to the IEEE 802.11 specifications.  We also conduct an experimental study of the EDCA mechanism of the IEEE 802.11 standard with a real-environment testbed consisting of one access point and 11 laptops to further validate the analytical model and simulation results.	 </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Run-time compilation and dynamic memory use analysis for GPUs</strong><br /> Monday, July 22, 2013<br /> Derek White<br /><p><a href="#" id="577-show" class="showLink" onclick="showHide(577);return false;">Read More</a></p></p><div id="577" class="more"><p><a href="#" id="577-hide" class="hideLink" onclick="showHide(577);return false;">Hide</a></p><p><strong>Abstract: </strong>As the availability of powerful GPUs becomes more common and users begin to realize their potential as an efficient and cost effective means of performing computationally intensive tasks, the desire to write software to take advantage of this new specialized platform is greater than the capability of most developers. Multi-level memory architectures, data transfer considerations, and a large array of processor cores that favor data-parallel tasks can present a daunting challenge for programmers that lack experience with GPUs. Other significant restrictions imposed by the OpenCL specification include the inability to allocate memory dynamically, kernel code (the core computational task of a GPU program) must be written in a subset of C99 rather than a more natural object-oriented language, and lack of recursion. Once novice GPU programmers begin to move beyond simple examples, they quickly learn that they are still a long way from the proficiency required to write optimized code. Ju st as assemblers, compilers, and high level languages allowed for abstractions that opened the door for many programmers of early computers, so too must software tools make programming GPUs as approachable as possible. A modern, expressive programming language combined with creative uses of static and dynamic program analysis can alleviate many of these issues and open up the potential of general purpose GPU computing to more developers.   Addressing the problem of dynamic memory allocation, we present a memory usage analysis that gives programmers the illusion of dynamic memory allocation while operating within the constraints of OpenCL devices. Static analysis is performed on GPU kernels written in the Scala programming language using the Firepile GPU programming library. An upper bound on memory usage of the kernel is computed. This bound is used to pre-allocate memory needed to successfully execute the kernel on the GPU. Our experiments show that the analysis finds a conservative upper bound with respect to actual memory allocated by execution of the program on the JVM.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Quantitative Analysis of Surface Enhanced Raman Spectra (SERS)</strong><br /> Friday, July 19, 2013<br /> Shuo Li<br /><p><a href="#" id="574-show" class="showLink" onclick="showHide(574);return false;">Read More</a></p></p><div id="574" class="more"><p><a href="#" id="574-hide" class="hideLink" onclick="showHide(574);return false;">Hide</a></p><p><strong>Abstract: </strong>Quantitative analysis of Raman spectra using Surface Enhanced Raman scattering (SERS) nanoparticles has shown the potential and promising trend of development in vivo molecular imaging. One of the key job is from the intensities of Raman signals to predict the quantities of analytes. Direct classical least squares (DCLS) and Multivariate calibration (MC) are commonly used methods. DCLS relies on source Raman signals as the references. But the inherent Instability of Raman signals make the DCLS model not robust enough. MC model relies on a batch of training mixture Raman signals together with the ground truth mixing concentrations to build the multivariate multiple linear regression models, so as to reduce the bias from the instability of source Raman signals. But it also brings in the more-variables-than-observations problem. Latent variable regression (LVR) model avoids that problem by extracting low dimensional latent variables (LVs) (or extracted features) to do regression with concentrations. Among several LVR methods, partial least squares regression (PLSR) algorithms are more robust, since their LVs both represent original Raman signals and predict concentrations. In this thesis, quantitative analysis models and methods are compared to show why PLSR algorithms are more robust for the purpose of quantitative analysis of Raman spectra.     Only PLSR cannot handle the unstable background of Raman signals. Baseline correction methods are commonly used as the preprocessing to find a slowly changed baseline under the signal as the estimated background. Raman peaks are extracted then by subtracting the baseline from the Raman signal. But baseline correction methods are usually time consuming iterative processes, and normally they cannot deal with the multi-scale property of Raman peaks. We designed a simple algorithm, called continuous wavelet transform (CWT) based partial least squares regression (CWT-PLSR) that uses the average CWT coefficients of mixture Raman signals to do PLSR with mixing concentrations. It extracts the multi-scale information of Raman peaks and so is more robust than traditional baseline correction methods.     The features extracted by PLSR give more weights to those Raman peaks that both representing Raman signals and predicting concentrations well. But the portion of each purpose is fixed in the objective function of PLSR. To improve the flexibility of PLSR, we designed a new continuum regression method that use a tuning parameter to control the portion of each purpose in the objective function and it gives more reasonable weights to Raman peaks. It beats other two CR methods by embracing PCR, RRR and PLS as three special cases, and is simply achieved by NIPALS algorithm.     Tuning parameters of PLSR and CR methods are normally decided by time consuming cross-validation methods. And some parameters have infinite numbers of possible values in continuous ranges. There is no way to test every value by cross-validation methods. Nonparametric Bayesian models of these methods are needed to decide the parameters automatically from the training data. As a foundation work, we design a probabilistic PLS regression model to give a probabilistic view of the PLSR methods. Future Bayesian models can be achieved by adding reasonable priors of the parameters.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>ADVANCED COMBINATORIAL TESTING ALGORITHM AND APPLICATIONS</strong><br /> Wednesday, July 10, 2013<br /> Linbin Yu<br /><p><a href="#" id="573-show" class="showLink" onclick="showHide(573);return false;">Read More</a></p></p><div id="573" class="more"><p><a href="#" id="573-hide" class="hideLink" onclick="showHide(573);return false;">Hide</a></p><p><strong>Abstract: </strong>Combinatorial testing (CT) has been shown to be a very effective testing strategy. Given a system with n parameters, t-way combinatorial testing, where t is typically much smaller than n, requires that all t-way combinations, i.e., all combinations involving any t parameter values, be covered by at least one test. This dissertation focuses on two important problems in combinatorial testing, including constrained test generation and combinatorial sequence testing.  For the first problem, we focus on constraint handling during combinatorial test generation. Constraints over input parameters are restrictions that must be satisfied in order for a test to be valid. Constraints can be handled either using constraint solving or using forbidden tuples. An efficient algorithm is proposed for constrainedtest generation using constraint solving. The proposed algorithmextends an existing combinatorial test generation algorithmthat does not handle constraints, and includes several optimizations to improve the performance of constraint handling. Experimental results on both synthesized and real-life systems demonstrate the effectiveness of the propose algorithm and optimizations.  For the second problem, the domain of t-way testing is expanded from test data generation to test sequence generation. Many programs exhibit sequence-related behaviors. We first formally define the system model and coverage for t-way combinatorial sequence testing, and then propose four algorithms for test sequence generation. These algorithms have their own advantages and disadvantages, and can be used for different purposes and in different situations. We have developed a prototype tool that applies t-way sequence testing on Antidote, which is  an healthcare data exchange protocol stack. Experimental results suggest that t-way sequence testing can be an effective approach for testing communication protocol implementations.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>APPLYING COMBINATORIAL TESTING TO SYSTEMS WITH A COMPLEX INPUT SPACE</strong><br /> Friday, April 26, 2013<br /> Mehra Nouroz Borazjany<br /><p><a href="#" id="570-show" class="showLink" onclick="showHide(570);return false;">Read More</a></p></p><div id="570" class="more"><p><a href="#" id="570-hide" class="hideLink" onclick="showHide(570);return false;">Hide</a></p><p><strong>Abstract: </strong>Combinatorial testing, which has been shown very effective in fault detection, is a testing strategy that applies the theory of combinatorial design to test software programs. Given a program under test with k parameters, t-way combinatorial testing requires all combinations of values of t (out of k) parameters be covered at least once, where t is usually a small integer. Combinatorial testing can significantly reduce the cost of testing while increasing its effectiveness. Input space modeling is an important step in combinatorial testing. The input space of a subject program must be modeled before combinatorial testing can be applied to it. The effectiveness of combinatorial testing to a large extent depends on the quality of the input space model. If the input space is modeled properly, all faults caused by interactions involving no more than t parameters will be detected.  In this dissertation, we develop an input space modeling methodology for combinatorial testing. The main idea is to consider the process of input space modeling as two steps, including input structure modeling and input parameter modeling. The first step tries to capture the structural relationship among different components in the input space. The second step tries to identify parameters, values, relations and constraints for individual components. We present case studies of applying the proposed methodology to five real-life programs.  These studies are designed to validate the proposed methodology in a practical setting. They are also designed to evaluate the effectiveness of combinatorial testing applied to real-life programs. We compare the proposed methodology to two other random approaches: (1) pure-random, which spends minimum effort on modeling; and (2) modeled-random, which generates random tests from the same model created by the proposed methodology. The results show that proposed approach is more effective than the modeled-random approach, which is significantly more effective than the pure-random approach.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Interpolation Based Approach for Pattern Recognition and Generation</strong><br /> Thursday, April 18, 2013<br /> Vishnukumar Galigekere<br /><p><a href="#" id="572-show" class="showLink" onclick="showHide(572);return false;">Read More</a></p></p><div id="572" class="more"><p><a href="#" id="572-hide" class="hideLink" onclick="showHide(572);return false;">Hide</a></p><p><strong>Abstract: </strong>Abstract: A large number of problems in computer vision and computer graphics can essentially be  reduced to a pattern recognition problem. In this thesis, we explore a novel interpolation based  framework to address some of the various recognition problems in these areas. Our interpolation based  framework is a supervised learning algorithm that allows for both generation (synthesis) of new patterns  as well as perception (analysis) of existing patterns. The method is simple to implement and yet, at the  same time, expects a very straightforward and intuitive set of parameters to model the complex nature  of such recognition problems.   Specifically, given a set of training data along with their parameters, we can learn a model that is a  compact representation of the set of all patterns defined in a parametric space. Having learnt such a  model we are able to generate any new patterns defined within that parametric space. Moreover, as an  inverse operation, we are also able to estimate the parameters of any existing pattern. Based on this  &#039synthesis-analysis&#039 approach we propose a method to recognize patterns and evaluate it in rather  diverse areas such as recognition of objects/faces in varying illumination conditions and, human motion  across different skeleton sizes. Using the same approach we demonstrate the methods application in  the area of image based modeling and rendering, where, we are able to render unknown objects into a  scene provided we have at least one known object in it. Another application is in the area of animation  where, given a set of human motion data differing in skeleton size but for a specific action, we are able  to re-target that specific action to an identical skeleton but of varying bone lengths.   Also, in this thesis, we describe our software tool that allows for automatic generation of ground-truth  data for various computer vision problems such as camera calibration, feature matching, 3D  reconstruction, object tracking and object recognition. We also explore a novel image feature descriptor built using a bank of Gabor filters and evaluate its effectiveness in an object recognition framework  using synthetic and real data.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>EFFECTIVE COLLABORATION IN OPPORTUNITIC NETWORKS</strong><br /> Monday, April 01, 2013<br /> Umair Sadiq<br /><p><a href="#" id="568-show" class="showLink" onclick="showHide(568);return false;">Read More</a></p></p><div id="568" class="more"><p><a href="#" id="568-hide" class="hideLink" onclick="showHide(568);return false;">Hide</a></p><p><strong>Abstract: </strong>Opportunistic networks formed by users&#039 mobile devices have the potential to exploit a rich set of distributed service components that can be composed to provide each user with a multitude of application level services. In opportunistic networks, tasks such as content sharing and service execution among remote devices are facilitated by intermediate devices capable of short-range wireless connectivity, also called relays that receive, move around, and forward the data.  To enable effective collaboration in such an environment, we make three novel contributions: (i) an adaptive forwarding scheme, ProxiMol, to select suitable relays for data transfer; (ii) a distributed mechanism to select services that can be composed; and (iii) an incentive-compatible credit scheme, CRISP, to promote participation of relays to forward data. These contributions resolve key challenges in opportunistic networks as in existing works, data transfer is not adaptive to changing user behavior, service composition is not performed on remote devices in the absence of a connected path, and rewards are not incentive-compatible (i.e. rewards do not promote honest behavior of the users).  ProxiMol is a novel forwarding scheme leveraging two simple facets of users in opportunistic networks - some users have better likelihood of message delivery due to higher mobility, while others do due to their location?s proximity to destination. Key contributions to the design of ProxiMol include: (i) a model to infer user?s location over time from diffusion (a measure of mobility); (ii) an analytical result to estimate distance between users; and (iii) an empirical method to estimate diffusion of a user. These are used to compute the likelihood of delivery taking into account both the mobility of a user and her proximity to destination. ProxiMol improves delivery ratios (10-20%) and reduces delays by up to 50%, when compared against previously proposed algorithms, in user environments that range from relatively homogeneous to highly heterogeneous settings.  The proposed service composition algorithm derives efficiency and effectiveness by taking into account the estimated load at service providers and expected time to opportunistically route information between devices. Based on this information the algorithm decides the best composition to obtain a required service. It is shown that using only local knowledge collected in a distributed manner, performance close to a real-time centralized system can be achieved. Scope, performance guarantee, and applicability of the service composition algorithm in a range of mobility characteristics are established through extensive simulations on real/synthetic mobility traces.  CRISP (Collusion-Resistant Incentive-ComPatible routing and forwarding) is the first credit scheme in which both routing and forwarding are designed to be incentive compatible in opportunistic networks, i.e., honest behavior of users maximizes their profit. Data transfer and loss in opportunistic networks are modeled as a linear generalized flow network where flow maximization leads to optimal relay behavior. This optimal behavior is made incentive compatible by requiring a relay to make a specific payment upon receiving the data and earn reward for forwarding the data. A cryptographic technique is used to make the scheme collusion resistant. A distributed payment framework (DPF) has been developed for implementation of CRISP in opportunistic networks. DPF is the first framework that does not require a trusted third party to maintain credits. DPF maintains consistency by using vector clocks and is secured by using digital signatures and Merkel trees. Simulations on real and synthetic mobility traces validate our analysis, showing a significant gain in throughput when compared with the existing credit schemes that are not incentive compatible.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Multi Person Tracking and Querying with Heterogeneous Sensors</strong><br /> Monday, November 26, 2012<br /> Shahina Ferdous<br /><p><a href="#" id="562-show" class="showLink" onclick="showHide(562);return false;">Read More</a></p></p><div id="562" class="more"><p><a href="#" id="562-hide" class="hideLink" onclick="showHide(562);return false;">Hide</a></p><p><strong>Abstract: </strong>Tracking the location of a user is considered to be the most fundamental step for creating a context aware application such as activity monitoring in an assistive environment. The problem becomes very challenging if the number of people involved in the scenario is larger than one.  The reason is that any multi-person environment such as a hospital demands a simultaneous identification and localization mechanism, thus making the system extremely complex.  In this dissertation, we present a novel, less-intrusive system that uses RFID and sensors deployed at various locations of an assistive apartment to continuously track and identify every person in a multi-person assistive environment. In addition, the system stores the large scale spatio-temporal sensor data into a common repository and provides a flexible query interface to track the history of the patient. The visualization tool embedded to the system helps the therapists to remotely monitor a person present in a scene in near real time.  Such a visualization gives a very good indication about a person/patient?s activity and behavior in the assistive environment as well.  The system also incorporates the metadata mapping of the large amount of stored data so that a doctor/therapist can query about a patient?s records without even knowing all the schemas stored in the repository.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A GAME THEORETIC FRAMEWORK FOR COMMUNICATION DECISIONS IN MULTI AGENT SYSTEMS</strong><br /> Friday, November 16, 2012<br /> Tummalapalli Sudhamsh Reddy<br /><p><a href="#" id="556-show" class="showLink" onclick="showHide(556);return false;">Read More</a></p></p><div id="556" class="more"><p><a href="#" id="556-hide" class="hideLink" onclick="showHide(556);return false;">Hide</a></p><p><strong>Abstract: </strong>Communication is the process of transferring information between multiple entities. We study the communication process between different entities that are modeled as multiple agents. To study the interactions between these entities we use Game theory, which is a mathematical tool that is used to model the interactions and decision process of these multiple players/agents. In the presence of multiple players their interactions are generally modeled as a stochastic game.  Here we assume that the communication medium, protocols and language are already present in this multiagent system. We address the question of information selection in the communication process.  In this thesis, we develop a formal framework for communication between different agents using game theory. Our major contributions are:  A classifications of the multi agent systems and what information to communicate in these various cases.  Algorithms for Inverse Reinforcement Learning in multiagent systems, which allow an agent to get a better understanding about the other agents.  A mathematical framework using which the agents can make two important decisions, when to communicate, and, more importantly what to communicate in different classes of multiagent systems.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Energy-Efficient Protocols and Systems for Wireless Sensor Networks and Smart Environments</strong><br /> Monday, November 12, 2012<br /> Giacomo Ghidini<br /><p><a href="#" id="558-show" class="showLink" onclick="showHide(558);return false;">Read More</a></p></p><div id="558" class="more"><p><a href="#" id="558-hide" class="hideLink" onclick="showHide(558);return false;">Hide</a></p><p><strong>Abstract: </strong>In a wireless sensor network, small computing devices, called sensors, sense the surrounding environment and relay the sensed data to a base station over a multi-hop wireless network, eventually processing them en-route. Wireless sensor networks and other devices, such as smartphones, smart meters, and smart appliances, cooperate in smart environments to obtain information about the environment, and then use this information to improve the experience of the users. Since most of these systems rely on battery power, there is a need for energy-efficient solutions for their operation. The objective of this dissertation is to design algorithms and protocols to improve the energy efficiency of such systems, and validate them using mathematical analysis, software simulations, and testbed experiments. In the first part of the dissertation, we look at two fundamental problems in wireless sensor networks: localization and duty cycling. In the area of localization, we describe a novel protocol for duty cycling wireless actor and sensor networks, and present a mathematical analysis based on the coupon collector&#039s problem and the theory of coverage processes, as well as simulation results. Our analysis and results show that the proposed protocol achieves the user-requested localization accuracy while maximizing the sleep time of sensor nodes. As far as duty cycling is concerned, we present novel Markov chain-based randomized schemes, and discuss the probabilistic analysis, as well as the experiments we conducted on Sun SPOT sensors. These results show that our proposed schemes reduce the sleep latency, while not affecting other performance metrics such as the energy efficiency, or vice versa. In the second part of the dissertation, we shift our focus to smart environments, and present our research work on data fusion and visualization aimed to provide lay users with actionable information. We introduce a framework, called FuseViz, to leverage already existing data sources such as smartphones, online databases and services, and wireless sensor networks, while addressing the challenges posed by large, live, heterogeneous, and autonomous data streams. We demonstrate the concepts behind our framework with a case study in building energy efficiency, and introduce E2Home, a Web-based application for this problem developed on top of the framework. Preliminary experiment results for the proposed E2Home system not only show that the actionable information can be easily computed, but also demonstrate energy savings of about 10%. Finally, we conclude our dissertation with an overview of a system-level energy model, built using data from the above-mentioned sources, that can be tailored for each home, its location, and residents, and can help further minimize energy consumption.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Similarity Measures and Indexing Methods for Time Series and Multiclass Recognition</strong><br /> Friday, November 09, 2012<br /> Alexandra Stefan<br /><p><a href="#" id="555-show" class="showLink" onclick="showHide(555);return false;">Read More</a></p></p><div id="555" class="more"><p><a href="#" id="555-hide" class="hideLink" onclick="showHide(555);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis investigates the problem of similarity search in multimedia databases. A key application domain of the proposed work is pattern classification, with emphasis on classification of gestures, handshapes, faces, and time series. A secondary application of the proposed work is efficient similarity search in large biological databases of protein and DNA sequences.  More specifically, the thesis makes contributions both by defining novel similarity measures, that are used to identify the best database matches, and by proposing methods to improve efficiency. On the topic of similarity measures, the thesis contributes a method for measuring similarity in a database of videos from American Sign Language (ASL). This method produces promising results towards enabling useful educational applications for the ASL community. A second contribution of the thesis is on the theoretical problem of how to define a useful metric distance measure for time series data. The thesis proposes a novel metric, called MSM (abbreviation for Move-Split-Merge), which has both attractive theoretical properties and competitive classification accuracy on actual data.  With respect to the problem of improving the efficiency of similarity search, the thesis contributes a novel method for recognition of a large number of classes. While many researchers have worked on the topic of how to train good classifiers for this task, the thesis proposes a new perspective by explicitly addressing efficiency. In particular, the thesis shows that, under some conditions, multiclass recognition becomes theoretically equivalent to similarity search, and in that case we can use off-the-shelf similarity indexing methods to significantly speed up multiclass recognition. The thesis also proposes a dimensionality reduction method specifically designed for speeding up similarity search in large string databases. While dimensionality reduction methods are commonly used in vector spaces, our method allows similar techniques to be used for spaces of strings under the edit distance measure.  Thorough experimental evaluation on a variety of datasets demonstrates state-of-the-art performance for the methods that constitute the contributions of the thesis.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A New Integrative Data Mining Framework for Analyzing the Cancer Genome Atlas Data</strong><br /> Tuesday, July 10, 2012<br /> Dijun Luo<br /><p><a href="#" id="552-show" class="showLink" onclick="showHide(552);return false;">Read More</a></p></p><div id="552" class="more"><p><a href="#" id="552-hide" class="hideLink" onclick="showHide(552);return false;">Hide</a></p><p><strong>Abstract: </strong>Besides accuracy and efficiency, understandability is another key issue of predictive modeling in real-world applications, especially in biomedical and healthcare data analysis. We develop a new integrative framework to enhance the interpretability of data by sparsity-based learning. We proposed several novel sparsity-based learning models, emphasizing different understandable properties of data, such as explicit sparsity, low redundancy, and low rank, and apply to The Cancer Genome Atlas (TCGA) data analysis. Results indicate that the proposed methods provide more insights from TCGA data while maintaining stable and competitive performances in predictive modeling. To further enhance the interpretability of biological processes and disease mechanisms, we also develop a novel visualization tool by considering heterogeneous relationships among genomics elements. By applying the novel learning models and the visualization tools, pathways of several important cancer diseases are revisited and a series of novel potential bio-markers are discovered which improves our ability to diagnosis, treat and prevent cancer.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>From Phenotype to Genotype: A Structured Sparse Learning framework for Imaging Genetics Studies</strong><br /> Monday, June 11, 2012<br /> Hua Wang<br /><p><a href="#" id="548-show" class="showLink" onclick="showHide(548);return false;">Read More</a></p></p><div id="548" class="more"><p><a href="#" id="548-hide" class="hideLink" onclick="showHide(548);return false;">Hide</a></p><p><strong>Abstract: </strong>Sparsity is one of the intrinsic properties of real-world data, thus sparse representation based learning models have been widely used to simplify data modeling and discover predictive patterns. By enforcing properly designed structured sparsity, one can unify specific data structures with the learning model. We proposed several novel structured sparsity learning models for multi-modal data fusion, heterogeneous tasks integration, and group structured feature selection. We applied our new structured sparse learning methods to the emerging imaging genetics studies by integrating phenotypes and genotypes to discover new biomarkers which are able to characterize neurodegenerative process in the progression of Alzheimer?s disease and other brain disorders. Different to traditional association studies, our new structured sparse learning models can elegantly take advantage of the useful information contained in biomarkers, cognitive measures, and disease status, where, crucially, the interrelated structures within and between both genetic/imaging data and clinical outcomes are gracefully exploited by our newly designed convex sparse regularization models.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Extensible Self-Adapting Dissemination Framework for High-Speed Continuous XML Streaming Data</strong><br /> Friday, April 20, 2012<br /> Anthony Okorodudu<br /><p><a href="#" id="541-show" class="showLink" onclick="showHide(541);return false;">Read More</a></p></p><div id="541" class="more"><p><a href="#" id="541-hide" class="hideLink" onclick="showHide(541);return false;">Hide</a></p><p><strong>Abstract: </strong>The efficient dissemination of data has become increasingly important with recent advances in technology that provide us immediate access to information at our fingertips.  The insatiable demand for data has increased significantly with the popularity and widespread use of smart phones and tablets throughout the world.  This thesis is focused efficient push-based means of disseminating data to a large number of interested clients using a network of co-operating machines, called brokers.  There have been extensive studies in optimizing both XML filtering and XPath-based subscription management in publish/subscribe systems with the aim of disseminating data in a timely fashion.  Much less attention has been devoted to the optimization of the underlying broker network overlay in a dynamic environment where the subscription and publication demographics evolve over time.  Although XML filtering and subscription management optimizations are important in reducing publication latency, their effects can be marginalized by a subpar broker network configuration.  The effects of network configuration are exacerbated under various constraints, such as network bandwidth.  Optimizing the network through manual means is impractical due to the sheer size of possible alternate network configurations.   In this thesis, we present an extensible large-scale self-adapting publish/subscribe system for disseminating streaming XML data.  We introduce DOXTOR (Dissemination of XML Through Optimized Routing), a distributed self-adapting publish/subscribe system.  Our experimental results show that our distributed self-adapting algorithm improves the fitness of the broker network overlay over time with respects to a given cost function.  We also address the issue of data loss during network reconfiguration, which has been largely overlooked in this area.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Improving Quality of Service in XML Data Stream Processing  Using Load Shedding</strong><br /> Friday, April 20, 2012<br /> RANJAN DASH<br /><p><a href="#" id="542-show" class="showLink" onclick="showHide(542);return false;">Read More</a></p></p><div id="542" class="more"><p><a href="#" id="542-hide" class="hideLink" onclick="showHide(542);return false;">Hide</a></p><p><strong>Abstract: </strong>In recent years, we have witnessed the emergence of Data Stream Management systems (DSMS) that deal with large volumes of streaming data. Examples include financial data analysis on feeds of stock tickers, sensor-data monitoring (environmental monitoring, Industrial operation monitoring), network traffic monitoring and click stream analysis to push customized advertisements or intrusion detection. Unlike DBMS, DSMS requires low-latency processing on live data from push-based sources.   A common but challenging issue in DSMS is to deal with unpredictable data arrival rate. Data arrival may be fast and bursty at times that surpass available system capability to handle. When input rates exceed system capacity, the Quality of Service (QoS) at system outputs falls below the acceptable levels. The problem of system overloading is more acute in XML data streams than their counterpart in relational streams, as they have to spend extra resources on input creation and result construction.   The main focus of this thesis is to find out suitable ways to process this high volume of data streams gracefully with limited or fixed resources while dealing with the spikes in data arrival rates in XML stream context. One established method is to shed load by selectively dropping tuples under these conditions. This helps to improve the observed latency of the results but degrades the answer quality. Victim selection is more complex in XML streams than relational streams. In this thesis, we first define the QoS in the context of XML stream processing, cover various mechanisms to improve the QoS, specially the method of load shedding. We will provide a general solution framework for implementing Load Shedding using Synopsis, while minimizing the loss in QoS. We prove the effectiveness of this framework using it in various types of query (set-valued, aggregation and join). We also introduce synopsis based multi-dimensional stream analysis using stream cubes to yield exact result.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Effective and Secure Use of Human Relation Networks</strong><br /> Friday, April 06, 2012<br /> Na Li<br /><p><a href="#" id="535-show" class="showLink" onclick="showHide(535);return false;">Read More</a></p></p><div id="535" class="more"><p><a href="#" id="535-hide" class="hideLink" onclick="showHide(535);return false;">Hide</a></p><p><strong>Abstract: </strong>With the advent of Web 2.0 and advanced techniques of wireless devices (e.g., smart phones or iPhones), Online Social Networks (OSNs) and Mobile Social Networks (MSNs) are becoming integral part of our lives as two main digital social communities. People&#039s communication on OSNs and MSNs creates a valuable information pool containing a huge amount of data collected from human activities. These data make human relation networks more visible, even at widely apart geographical distances, as compared to their existence in our physical world. For instance, the friend list on a user profile page on Facebook.com clearly tells us the user&#039s friendship with others. Moreover, the short-range wireless communication techniques (e.g., Bluetooth) also enable us to ?sense? human relations in MSNs composed of wireless devices carried by human. On the one hand, human relation networks can facilitate socially intelligent computing, for example, the ?friend? recommendation service provided by most of the OSNs. On the other hand, improperly using it may lead to significant concerns and challenges for OSNs as well as MSNs.  For example, when OSN owners share their user data to the third-parties, they should ensure that privacy preservation of users? sensitive relations. Similarly, there exist problems while using human relations to facilitate data forwarding in MSNs. This thesis focuses on effective and secure use of human relations in OSNs and MSNs, particularly addressing: (1) from the perspective of OSN owners, how to preserve users? relation privacy in publishing OSN data to the third parties; (2) from the viewpoint of a third-party analyst, how to leverage the topological properties of human relation networks on OSNs to efficiently search a subgraph connecting a group of target users on OSNs; (3) designing a reputation-based framework to ensure the use of reliable relations to forward data in MSNs.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Routing and Channel Assignment Schemes for Interference Avoidance in Wireless Mesh Networks</strong><br /> Monday, April 02, 2012<br /> Fawaz Bokhari<br /><p><a href="#" id="538-show" class="showLink" onclick="showHide(538);return false;">Read More</a></p></p><div id="538" class="more"><p><a href="#" id="538-hide" class="hideLink" onclick="showHide(538);return false;">Hide</a></p><p><strong>Abstract: </strong>The concept of wireless mesh networks (WMN) has emerged as a promising technology for the provision of affordable and low cost broadband internet access among communities. With the availability of off-the-shelf, low cost, commodity networking hardware, it is possible to incorporate multiple radio interfaces operating in different radio channels on a single mesh router; thus forming a multi radio multi channel wireless mesh network (MRMC-WMN). This enables a potential large improvement in the capacity of mesh networks compared to single radio mesh networks. This thesis focuses specifically on designing routing and channel assignment schemes for interference avoidance in WMNs. Routing algorithms deal with discovering efficient data paths to forward network traffic and channel assignment provides link configuration to channels in such a way that the network throughput is improved by reducing link interference thereby eventually increasing the network capacity. Both, routing and channel assignment are challenging problems mainly for two reasons: first, the existence of network interference, and second, the issue of load balancing in WMNs. Therefore, we demonstrate in this thesis how to improve throughput, provide efficient load balancing, achieve interference avoidance and increase network capacity in WMNs. The significant contributions of our research is the development and design of two routing algorithms that help in improving network throughput by selecting less interference paths both for single and multiple radio WMNs and the design of an intelligent channel assignment scheme which increases the overall network capacity by assigning partially overlapped channels having less interference among neighboring ones for MRMC-WMNs. Our findings are supported by comprehensive evaluations of our proposed routing and channel assignment schemes.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A COMPUTATIONAL FRAMEWORK FOR HUMAN-CENTERED MULTIMODAL DATA ANALYSIS</strong><br /> Tuesday, November 22, 2011<br /> Vangelis Metsis<br /><p><a href="#" id="532-show" class="showLink" onclick="showHide(532);return false;">Read More</a></p></p><div id="532" class="more"><p><a href="#" id="532-hide" class="hideLink" onclick="showHide(532);return false;">Hide</a></p><p><strong>Abstract: </strong>Human-Centered computing defines a field of study in which computational processes affect the human being, either through ubiquitous and pervasive use of devices or any effect that improves the human condition. Human-Centered Computing applications face serious challenges in the handling of data collection, modeling, and analysis. Traditionally, the analysis of different aspects of human well-being derives from a variety of non-interrelated methods which has made it difficult to correlate and compare the different experimental findings for an accurate assessment of the contributing factors. This thesis describes new algorithms that enable more accurate and efficient multimodal data analysis of Human-Centered computing applications in order to improve decision-making in healthcare.  In particular, this work provides a theoretical framework for multimodal and inter-related data analysis and demonstrates the theory in different cases where the purpose is to (a) monitor the health condition of the human subject, and (b) to improve the quality of life through the understanding of a subject&#039s behaviors. This work presents a computational framework which can efficiently analyze and interpret data of different modalities coming from the same human subjects. Emphasis is put on the evaluation of feature selection and classification techniques and their use for heterogeneous data fusion in order to improve the accuracy of the obtained results. Our experimental results show that the same basic methods can be used to analyze data regarding both the genotypic and the phenotypic formation of a subject, and to correlate the different findings into more meaningful and reliable information.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SCALABLE PARALLEL PROCESSING OF MULTI-OBJECTIVE OPTIMIZED DNA SEQUENCE ASSEMBLY</strong><br /> Thursday, November 10, 2011<br /> MUNIB AHMED<br /><p><a href="#" id="531-show" class="showLink" onclick="showHide(531);return false;">Read More</a></p></p><div id="531" class="more"><p><a href="#" id="531-hide" class="hideLink" onclick="showHide(531);return false;">Hide</a></p><p><strong>Abstract: </strong>Bioinformatics is an emerging branch of science where issues pertaining to molecular biology are evaluated and resolved by leveraging the techniques and algorithms devised in the field of computer science. Most of these issues are due to the enormous amount of data and the computational complexity involved in generating expeditious and qualitatively viable solutions. This poses a challenge to the algorithm developers who must strive to achieve multiple conflicting objectives of processing very large dataset with the highest accuracy possible while keeping the execution time to a minimum. Genome assembly is one such problem in bioinformatics where a DNA sequence is reconstructed using millions of small fragments of DNA that are produced in the laboratory as a result of sequencing process. When examined purely as data, these fragments are small in size (< 1000 characters long) but large in numbers, have repetitive regions which exacerbates the complexity of the reconstruction algorithms, and contain erroneous data due to imperfect laboratory procedures. This dissertation takes a holistic approach to resolve these issues by first presenting a comprehensive study of contemporary work, highlighting its strengths and weaknesses while proposing improvements wherever needed, followed by the design and implementation of a new parallel framework. With the extra processing power available in a parallel computing environment, this framework enhances accuracy of the solution by correcting errors in the low quality data regions and improves the speedup by dynamically balancing the load among multiple processors and by utilizing innovative data structures along with a hashing technique that require lesser memory compared to other contemporary programs. One of the chief objectives of this work is to carve out an important and sizeable piece of the DNA sequence assembly process and provide a modular implementation based on a new parallel algorithm in order to facilitate the scalability analysis and parametric study of various characteristics and interdependencies of multiple conflicting objectives such as speedup, accuracy, and data size. A comparison between experimental and model-predicted statistics of the system explains how real life scenarios deviate from theoretical models in this domain.  This research work and the underlying approach can be easily extended to other related areas of bioinformatics, including multiple sequence alignment and phylogenetics, using parallel computing.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A THEORETICAL FRAMEWORK FOR DESIGN SPACE EXPLORATION OF MULTICORE AND MANYCORE PROCESSORS</strong><br /> Thursday, October 27, 2011<br /> Hun Jung<br /><p><a href="#" id="530-show" class="showLink" onclick="showHide(530);return false;">Read More</a></p></p><div id="530" class="more"><p><a href="#" id="530-hide" class="hideLink" onclick="showHide(530);return false;">Hide</a></p><p><strong>Abstract: </strong>	As design space and workload space in multicore/manycore era are continuously expanding, it is a challenge to identify optimal design points quickly during the early stage of multicore/manycore processor design or programming phase. To meet this challenge, a thread-level modeling methodology is developed in this dissertation. The idea is to model multicore/manycore processors at the thread-level, overlooking instruction-level and microarchitectural details. Since the thread-level modeling is much coarser than the instruction-level modeling, the analysis at this level turns out to be significantly faster than that at the instruction level. This feature makes the methodology particularly amenable for fast performance evaluation in a large design space.   	Based on this methodology, we developed a thread-level simulation tool for quick evaluation of a specific design point and also a theoretical framework that can capture the general performance properties for a class of multicore/manycore processors of interest over a large design space and workload space, free of scalability issues. In the theoretical framework, queuing network models that model multicore/manycore processors at the thread level are developed and scalability issues in the queuing networks are solved based on an iterative algorithm over a large design space and workload space. This framework scales to virtually unlimited numbers of cores and threads. 	For the simulation tool, Case studies based on a large number of code samples available in INTEL IXP1200/2400 workbenches show that the maximum throughput estimated using our tool are consistently within 6% of cycle-accurate simulation results. Moreover, each simulation run takes only a few seconds to finish on a Pentium 4 computer, which strongly demonstrates the power of this tool for fast CP performance testing. For the theoretical frame work, the testing results demonstrates that the throughput performance for manycore processors with 1000 cores can be evaluated within a few seconds on an Intel Pentium 4 computer and the results are within 5% of the simulation data obtained based on the thread-level simulator tool.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Generalized Reinforcement Learning with an Application in Generic Workload Management System</strong><br /> Friday, July 29, 2011<br /> Po-Hsiang Chiu<br /><p><a href="#" id="529-show" class="showLink" onclick="showHide(529);return false;">Read More</a></p></p><div id="529" class="more"><p><a href="#" id="529-hide" class="hideLink" onclick="showHide(529);return false;">Hide</a></p><p><strong>Abstract: </strong>Learning by trial and error and being able to form levels of abstraction from the past experience has been an important factor for sentient beings to develop intelligent behaviors and cope with ever-changing environment. Complex control domains, in a similar way, often require the interacting agents to learn adaptive control strategies for time-varying or potentially evolving systems. This dissertation will begin by investigating an example of complex domains, Grid network, through a collaborative effort in the design and implementation of a generic workload management system, PanDA-PF WMS used in the ATLAS experiment. With the incentive of boosting the performance of PanDA-PF WMS and increasing its applicability in a general resource-sharing environment, we will subsequently motivate an automated and adaptive learning approach that optimizes computational resource usage.     From the experience of developing Grid applications such as PanDA-PF, we found that a flexible infrastructure still has its limit in performance both from the perspective of high-performance computing (HPC) and high-throughput computing (HTC). The key is that an optimal resource allocation strategy is highly contingent upon many factors hidden in the intricate dynamics behind the scene including the task distribution, real-time resource profile in addition to compatibility between the user tasks and the allocated machines, etc. Reinforcement learning framework establishes a unique way of solving a wide range of control and planning tasks through the state space representation of the system over which the control policy unfolds as a sequence of control decisions toward a maximum payoff. Intuitively, reinforcement learning seems to be an ideal candidate among other machine learning methods for developing an optimal resource allocation strategy that harvests free computation resource by learning the their intricate dynamics.     However, our hope in applying standard reinforcement learning in the context of resource allocation is diminished due to an inherent limitation in its representation. In particular, the control policy is often formulated from the perspective of decision theoretic planning (DTP) such that actions, as control decisions, are assumed to be atomic with fixed action semantics. Consequently, the derived policy in general lacks the ability in adapting to possible variations in the action outcomes or the action set itself due to versatility of the system. This would be a major barrier in learning an ideal resource allocation strategy where each compute resource is often characterized by time-varying properties that determine its performance. In addition, the available resource may be highly volatile depending on the resource-sharing infrastructure. In a dynamic computational cluster, for instance, the underlying resource is acquired on-demand in terms of distributed virtual machines that may not be persistently available to end users. As a consequence, the optimal strategy for task assignment learned earlier may not be strictly applicable in the future.     Inspired by the challenge in complex domains like optimal resource sharing, this dissertation will progressively develop an extended reinforcement learning framework, CDLA, that enables adaptive policy learning over the abstraction of the progressively evolved samples of experience. In particular, we provide an alternative view of reinforcement learning by establishing the notion of the reinforcement field through a collection of policy-embedded particles gathered during the policy learning process. The reinforcement field serves as a policy generalization mechanism over correlated decisions through the use of kernel functions as a state correlation hypothesis in combination with Gaussian process regression as a value function approximator. Subsequently, through “kernelizing” the spectral clustering mechanism, the policy-learning experience retained in the memory of the agent can be further subdivided into a set of concept-driven abstract actions, each of which implicitly encodes a set of context-dependent local policies. We will show from a simulated task-assignment domain that the end result of our generalized reinforcement learning framework will enable both the learning of an action-oriented conceptual model and simultaneously deriving an optimal policy out of the high-level conceptual units. Moreover, to demonstrate the general applicability of our learning approach, we apply the work in a generalized navigation domain – the gridworld without the grid in which the agent is free to move in all directions with stochastic behaviors in actions and subsequently show the learning result in terms of both an improved learning curve and reinforcement “field plots.”</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Detecting Program Vulnerabilities Using Trace-based Security Testing</strong><br /> Friday, July 15, 2011<br /> Dazhi Zhang<br /><p><a href="#" id="523-show" class="showLink" onclick="showHide(523);return false;">Read More</a></p></p><div id="523" class="more"><p><a href="#" id="523-hide" class="hideLink" onclick="showHide(523);return false;">Hide</a></p><p><strong>Abstract: </strong>Software vulnerabilities are program flaws that can be exploited by attackers to compromise the security of a software system. Although many approaches have been proposed to detect or prevent software attacks, software security incidents continue to occur every year. Security testing aims at detecting program vulnerabilities through a set of test cases and has shown to be effective to detect program vulnerabilities. The primary challenge is how to efficiently produce test cases that are highly effective in detecting vulnerabilities. This dissertation proposes trace-based security testing approaches towards addressing some fundamental challenges in security testing.   The first study is to use trace-based symbolic execution and satisfiability analysis to detect C program vulnerabilities. A security testing model is proposed to unify program states and security requirements into logical expressions. Specifically, program constraints (PC), i.e., all possible values of program variables at a given point in an execution, are derived from symbolic execution on the trace. Security constraints (SC), i.e., secure values of program variables at security critical points of the program, are derived from security knowledge. Both PC and SC are represented in first order logic. Therefore, the satisfiability of predicate PC ?¬SC indicates a program vulnerability. A tool named SecTAC has been developed and applied to test several open source C programs. Many known and unknown vulnerabilities have been detected.   The second study is a novel fuzzing approach that aims to test deep program semantics through the analysis of program execution trace. Intuitively, program execution trace reflects the semantics of program input data from the program’s point of view. This study proposes a test case similarity metric to model the semantic similarity between well-formed input data and its mutations. Such similarity is used to direct a two-stage fuzzing process to produce more test cases that are more likely to explore deep program semantics. A prototype tool named SimFuzz is developed to test real programs, and the experimental result shows that deep program semantics can be extensively tested compared to traditional fuzzing approaches.   The third study is to utilize end user data for security testing as well as provide timely protection to end users. The idea is to monitor how program paths are explored by benign user data or malicious exploits. Once a new path is being explored, it is sent to testing site for security testing using trace-based security testing. Several techniques are proposed to make the system feasible in practice. First, tree-based bit tracing is proposed to reduce user site overhead and preserve user privacy. Second, conditional runtime monitor is proposed to ensure user security while reduce latency. Third, test decomposition is proposed to reduce space overhead. A prototype system named SecTOD has been developed and applied to test the Apache server program. The result shows that it is effective in terms of vulnerability detection and efficient in terms of computation and space overhead.   Overall, this dissertation proposes trace-based security testing and studies techniques to (1) reuse existing test cases for security testing (2) extensively test deep program semantics (3) utilize end user data for security testing as well as protect end user security. These studies show that trace-based security testing approach is a promising technique for security testing in terms of the effectiveness and efficiency.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Performance Analysis and Resource Allocation for Multithreaded Multicore Processors</strong><br /> Friday, July 15, 2011<br /> MIAO JU<br /><p><a href="#" id="526-show" class="showLink" onclick="showHide(526);return false;">Read More</a></p></p><div id="526" class="more"><p><a href="#" id="526-hide" class="hideLink" onclick="showHide(526);return false;">Hide</a></p><p><strong>Abstract: </strong>With ever expanding design space and workload space in multicore era, a key challenge to program a multithreaded multicore processor is how to evaluate the performance of various possible program-task-to-core mapping choices and provide optimal resource allocation during the initial programming phase, when the executable program is yet to be developed. In this dissertation, we put forward a thread-level modeling methodology to meet this challenge. The idea is to model thread-level activities only and overlook the instruction-level and microarchitectural details. A model developed at this level assumes the availability of only a piece of pseudo code that contains information about the thread-level activities, rather than an executable program that provides instruction-by-instruction information. Moreover, since the thread-level modeling is much coarser than the instruction-level modeling, the analysis at this level turns out to be significantly faster than that at the instruction level. These features make the methodology particularly amenable for fast performance evaluation of a large number of program-task-to-core mapping choices during the initial programming phase. Based on this methodology, in this dissertation we further developed: 1) an analytic modeling technique based on queuing theory which allows large design space exploration; and 2) a framework that allows program tasks to be mapped to different core resources to achieve maximal throughput performance for many-core processors. Case studies against Cycle-Accurate-Simulator demonstrate that the throughput estimated using our modeling technique is consistently within 8% of cycle-accurate simulation results.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Adaptive High Level Context Reasoning in Pervasive Environments</strong><br /> Wednesday, July 13, 2011<br /> Bridget Beamon<br /><p><a href="#" id="525-show" class="showLink" onclick="showHide(525);return false;">Read More</a></p></p><div id="525" class="more"><p><a href="#" id="525-hide" class="hideLink" onclick="showHide(525);return false;">Hide</a></p><p><strong>Abstract: </strong>It is hard to believe that the internet is now in its adolescent stage. Our information age is replete with communication capable, intelligent, sensor equipped devices. Social networks, web services, and global information repositories make a wealth of information available instantly. There exist endless possibilities for creating useable knowledge. Much of what is considered useable knowledge is not directly observable from low level sensory devices.  Abstract situations, relationships and activities must be inferred using a variety of techniques that fuse information from multivariate data sources. We refer to this useable knowledge as high level context. Social, physiological, environmental, computational, activity, location and situation are but a few categories of high level context used today. In a general sense, context is any domain specific knowledge relevant to decision making. Low level contexts can be inferred after minimal manipulation and preprocessing of sensor data.  High level context is intrinsically more complex. High level context involves many levels of data fusion for inferring high level concepts. The increased dimensionality of representing and reasoning on relationships among contextual components, factoring uncertainty and ignorance, makes it difficult to effectively reason. A research problem in the area of context-aware computing is adaptive and effective high-level context reasoning. Effectiveness refers to the suitability of reasoning methodology for efficiently reasoning and representing the heterogeneous characteristics of context. Adaptive reasoning aides in maintaining context content and  quality in the face of dynamic resource availability, degrading reasoning performance and evolving requirements.  Context architects are at times challenged; constrained by the limited reasoning provided in the available platforms. Incorporating a generalized hierarchical hybrid reasoning engine, offering variety and optimization for reasoning across heterogeneous complex contexts would provide an effective alternative. Such architecture integrates a variety of configurable reasoning techniques, supporting the modularity of complex high level context. Ultimately, it promotes context reasoning framework reuse, knowledge sharing, and improved context aware application performance. The dissertation proposes a middleware framework with flexible components enabling effective adaptive context reasoning.  The focus is on middleware solutions for deriving high level context, with support for maintaining quality in dynamic pervasive environments. Considerations include: shareable context data models, integrated reasoning, deriving integrated quality of context, and adaptive generalized context reasoning. The solutions provided can be used to extend existing architectures, resulting in greater reuse. Reuse leads to rapid and innovative context aware application development, a necessary evolution for achieving the vision of ubiquitous computing and beyond.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A HOLISTIC SIMILARITY-BASED APPROACH FOR PERSONALIZED RANKING IN WEB DATABASES</strong><br /> Wednesday, July 06, 2011<br /> ADITYA TELANG<br /><p><a href="#" id="524-show" class="showLink" onclick="showHide(524);return false;">Read More</a></p></p><div id="524" class="more"><p><a href="#" id="524-hide" class="hideLink" onclick="showHide(524);return false;">Hide</a></p><p><strong>Abstract: </strong>With the advent of the Web, the notion of ``information retrieval&#039&#039 has acquired a completely new connotation and currently encompasses several disciplines ranging from traditional forms of text and data retrieval in unstructured and structured repositories to retrieval of static and dynamic information from the contents of the surface and deep Web. From the point of view of the end user, a common thread that binds all these areas is to support appropriate alternatives for allowing users to specify their intent (i.e., the user input) and displaying the resulting output ranked in an order relevant to the users.   In the context of specifying an user&#039s intent, the paradigms of querying as well as searching have served well, as the staple mechanisms in the process of information retrieval over structured and unstructured repositories. Processing queries over known, structured repositories (e.g., traditional and Web databases) has been well-understood, and search has become ubiquitous when it comes to unstructured repositories (e.g., document collections and the surface Web). Furthermore, searching structured repositories has been explored to a limited extent. However, there is not much work in querying unstructured sources which, we believe is the next step in performing focused retrievals.   Correspondingly, one of the important contributions of this dissertation is a novel semantic-guided approach, termed Query-By-Keywords (or QBK), to generate queries from search-like inputs for unstructured repositories. Instead of burdening the user with schema details, this approach utilizes pre-discovered semantic information in the form of taxonomies, relationship of keywords based on context, and attribute &amp; operator compatibility to generate query skeletons that are subsequently transformed into queries. Additionally, progressive feedback from users is used to further improve the accuracy of these query skeletons. The overall focus thus, is to propose an alternative paradigm for the generation of queries on unstructured repositories using as little information from the user as possible.  Irrespective of the template for intent specification (i.e., either a search or a query request), the number of results typically returned in response to such intents, are often, extremely large. This is particularly true in the context of the deep Web where a large number of results are returned for queries on Web databases and choosing the most useful answer(s) becomes a tedious and time-consuming task. Most of the time the user is not interested in all answers; instead s/he would prefer those results, that are ranked based on her/his interests, characteristics, and past usage, to be displayed before the rest.  Furthermore, these preferences vary as users and queries change.  Accordingly, in this dissertation, we propose a novel Similarity-based framework for supporting user- and query-dependent ranking of query results in Web databases. This framework is based on the intuition that -- for the results of a given query, similar users display comparable ranking preferences, and a user displays analogous ranking preferences over results of similar queries. Consequently, this framework is supported by two novel and comprehensive models of: 1) Query Similarity, and 2) User Similarity, proposed as part of this work. In addition, this ranking framework relies on the availability of a small yet representative set of ranking functions collected across several user-query pairs, in order to rank the results of a given user query at runtime. Appropriately, we address the subsequent problem of establishing a relevant Workload of ranking functions that assists the similarity model in the best possible way to achieve the goal of user- and query-dependent ranking. Furthermore, we advance a novel Probabilistic Learning model that infers individual ranking functions (for this workload) based on the implicit browsing behavior displayed by users. We establish the effectiveness of this complete ranking framework by experimentally evaluating it on Google Base&#039s vehicle and real estate databases with the aid of Amazon&#039s Mechanical Turk users.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient Exploration Techniques on Large Databases</strong><br /> Monday, April 11, 2011<br /> Senjuti Basu Roy<br /><p><a href="#" id="518-show" class="showLink" onclick="showHide(518);return false;">Read More</a></p></p><div id="518" class="more"><p><a href="#" id="518-hide" class="hideLink" onclick="showHide(518);return false;">Hide</a></p><p><strong>Abstract: </strong>Search, retrieval, and exploration of information have become some of the most intense and principal research challenges in many enterprize and e-commerce applications off late. The mainstay of this thesis is to analyze and investigate different aspects of online data exploration, and propose techniques to accomplish them efficiently. In particular, the results in this thesis widen the scope of existing {em faceted search}, and {em recommendation systems} - two upcoming fields in data exploration which are still in their infancy.   {em Faceted search}, the de facto standard for e-commerce applications, is an interface framework with the primary design goal of allowing users to explore large information spaces in a flexible manner.  We study this alternative search and exploration paradigm in the context of structured and unstructured data. Motivated by the rapid need of knowledge discovery and management in large enterprize organizations, we propose {em DynaCet} - a minimum effort driven dynamic faceted search system on structured databases. In addition, we study the problem of dynamic faceted retrieval in the context of unstructured data using {em Wikipedia}, the largest and most popular online encyclopedia.  We propose {em Facetedpedia}, a faceted retrieval system which is capable of dynamically generating query-dependent facets for a set of Wikipedia articles.   The ever-expanding volume and increasing complexity of information on the web has made {em recommender systems} essential tools for users in a variety of information seeking or e-commerce activities by exposing them to the most interesting items, and by offering novelty, diversity, and relevance. Current research suggests that there exists an increasing growth in online social activities that leaves behind trails of information created by users. Interestingly, recommendation tasks stand to benefit immensely by tapping into these latent information sources, and by following those trails. A significant part of this thesis has investigated on how to improve the online recommendation tasks with novel functionalities by considering additional contexts that can be leveraged by tapping into social data.  To this end, this thesis investigates problems such as, how to compute recommendation for a group of users, or how to recommend composite items to a user. Underlying models leverage on social data (co-purchase or browsing histories, social book-marking of photos) to derive additional contexts to accomplish those recommendation tasks. In particular, it focuses on techniques that enable a recommendation system to interact with the user in suggesting composite items - such as, bundled products in online shopping, or itinerary planning for vacation travel. We investigate the technical and algorithmic challenges involved in enabling efficient recommendation computation, both from the user (the interaction should be easy, and should converge quickly), as well as the system (efficient computation of composite item) points of view.   This thesis also discusses performance and user study results, which were conducted using the crowd-sourcing platform Amazon Mechanical Turk. We conclude by briefly describing other promising problems with future opportunities in this field.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Accurate and Cost Efficient Object Localization Using Passive UHF RFID</strong><br /> Thursday, April 07, 2011<br /> Jae Sung Choi<br /><p><a href="#" id="517-show" class="showLink" onclick="showHide(517);return false;">Read More</a></p></p><div id="517" class="more"><p><a href="#" id="517-hide" class="hideLink" onclick="showHide(517);return false;">Hide</a></p><p><strong>Abstract: </strong>In a smart environment, accurate locations of objects are a fundamental and critical issue. To achieve this goal, we present several methods based on passive far-field UHF RFID technologies, which can satisfy accuracy, robustness and reliability, cost efficiency, simplicity, compatibility, and scalability. Our research overcomes several negative characteristics of the use of cost efficient passive UHF RFID. Our research has several important contributions.  First, we study the causes of the problems of using passive UHF RFID in localization, with detailed empirical results, and then, present the impacts of the causes on existing localization techniques such as KNN.  Second, we present a new model of backscattered signal strength for passive far-field UHF RFID system under tag-to-tag interference. We propose a method to estimate power variations due to tag interference, based on a tag-to-tag distance and angle using a second order under-damped system. We present a novel localization algorithm to estimate target object location using our Tag-to-tag Interference Model (LMTI). According to the empirical results, LMTI improves accuracy be over 200% compared with RSSI based KNN algorithm when objects are empty boxes, and 127% improvement when objects are the print cartridges contained in aluminum foil bags.  Third, we present another approach to achieve accurate localization. Localization using Detection of Tag Interference (LDTI) algorithm, which detects the tag interference on a map of reference tags to estimate target location. To avoid selection of spatially non-adjacent reference tags, we also present the most interfered reference group finding algorithm, which considers spatial relations between reference tags. LDTI based smart shelf performs on average 0.0948m estimation error for 9 empty cardboard boxes, and average 0.1831m estimation error for 9 print cartridge containers, which is a 71% accuracy improvement compared to the KNN algorithm.  Finally, we present a novel Vision and passive UHF RFID integrated Localization (VRL) system on smart shelf application to improve performance under harsh conditions. VRL performs on average 0.079m estimation error for 10 print cartridge containers, which is a 61% accuracy improvement compared to the LDTI algorithm under low False Negative Reference (FNR) interrogation conditions. Moreover, it shows 555% computation overhead reduction compared a homogeneous vision system. In high FNR conditions, VRL system achieves over 620% increased accuracy compared to LDTI, and 437% reduced computation time compared to a pure vision based localization system.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Improving Privacy and Performance in Anonymous Communications</strong><br /> Thursday, December 30, 2010<br /> Nayantara Mallesh<br /><p><a href="#" id="516-show" class="showLink" onclick="showHide(516);return false;">Read More</a></p></p><div id="516" class="more"><p><a href="#" id="516-hide" class="hideLink" onclick="showHide(516);return false;">Hide</a></p><p><strong>Abstract: </strong>Anonymous communications systems provide an important privacy service by keeping passive eavesdroppers from linking communicating parties.  However, an attacker can use long-term statistical analysis of traffic sent to and from such a system to link senders with their receivers. While it is important to protect anonymous systems against such attacks, it is also important to ensure they provide good performance. In this thesis, we aim to make contributions to both these areas. <br /><br />  In the statistical disclosure attack (SDA), an eavesdropper isolates his attack against a single user, whom we call Alice, with the aim of exposing her set of contacts.  To study the SDA we introduce an analytical method to bound the time for the eavesdropper to identify a contact of Alice, with high probability. We analyze the attack in different scenarios beginning with a basic scenario in which Alice has a single contact. Defenses against this attack include sending cover traffic, which consists of sending dummy messages along with real messages. We extend our analysis to study the effect of two different types of cover traffic on the time for the attack to succeed. We further extend our analysis to investigate the effectiveness of the attack for a partial eavesdropper who can observe only a part of the network. We validate our analysis through simulations and show that the simulation results closely follow the results of analysis. Although our bounds are loose, they provide a way to compare between different amounts and types of cover traffic in various scenarios. <br /><br />  In this part of the thesis, we investigate how cover traffic can be used as an effective counter strategy against this attack. We propose that the mix generate cover traffic that mimics the sending patterns of users in the system. This receiver-bound cover (RBC) helps to make up for users that aren’t there, confusing the eavesdropper. We show through simulation how this makes it difficult for the eavesdropper to discern cover from real traffic and perform attacks based on statistical analysis. Our results show that receiver-bound cover substantially increases the time required for this attack to succeed. When our approach is used in combination with user-generated cover traffic, the attack takes a very long time to succeed. <br /><br />  The original statistical disclosure attack has focused on finding the receivers to whom Alice sends. In this part, we investigate the effectiveness of statistical disclosure in finding all of Alice’s contacts, including those from whom she receives messages. To this end, we propose a new attack called the Reverse Statistical Disclosure Attack (RSDA). RSDA uses observations of all users sending patterns to estimate both the targeted user’s sending pattern and her receiving pattern. The estimated patterns are combined to find a set of the targeted user’s most likely contacts. We study the performance of RSDA in simulation using different mix network configurations and also study the effectiveness of cover traffic as a countermeasure. Our results show that that RSDA outperforms the traditional SDA in finding the user’s contacts, particularly as the amounts of user traffic and cover traffic rise. <br /><br />  In the final part of this thesis, we study how a sparse network topology affects the security of anonymous systems. We show that an expander topology such as a sparse, D-regular graph exhibits security properties comparable to a fully connected graph; within a reasonable number of hops and even for small values of degree D. Further, we show that if the expander graph is constructed with a bias towards lower round-trip time links, there is a considerable gain in performance without compromise in security. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>REINFORCEMENT LEARNING BASED STRATEGIES FOR ADAPTIVE WIRELESS SENSOR NETWORK MANAGEMENT</strong><br /> Monday, November 22, 2010<br /> Kunal Shah<br /><p><a href="#" id="513-show" class="showLink" onclick="showHide(513);return false;">Read More</a></p></p><div id="513" class="more"><p><a href="#" id="513-hide" class="hideLink" onclick="showHide(513);return false;">Hide</a></p><p><strong>Abstract: </strong>In wireless sensor networks (WSN), resource-constrained nodes are expected to  operate in highly dynamic and often unattended environments. WSN applications  need to cope with such dynamicity and uncertainty intrinsic in sensor networks,  while simultaneously trying to achieve efficient resource utilization. A  middleware framework with support for autonomous, adaptive and distributed  sensor management, can simplify development of such WSN applications. We present  a reinforcement learning based WSN middleware framework to enable autonomous and  adaptive applications with support for efficient resource management. The  uniqueness of our framework lies in using a bottom-up approach where each sensor  node is responsible for its resource allocation/task selection while ensuring  optimization of system-wide parameters like total energy usage, network lifetime  etc. The framework allows creation of a distributed and scalable system while  meeting application&#039s goal.   In this dissertation, a Q-learning based scheme called DIRL (Distributed  Independent Reinforcement Learning) is presented first. DIRL learns the utility  of performing various tasks over time with mostly local information at nodes.  DIRL uses these utility values along with application constraints for task  management subject to optimal energy usage. DIRL scheme is extended to create a  two-tier reinforcement learning based framework consisting of micro-learning and  macro-learning. Micro-learning enables individual sensor nodes to self-schedule  their tasks using local information allowing for a real-time adaptation as in  DIRL. Macro-learning governs the micro-learners by setting their utility  functions in order to steer the system towards application&#039s optimization goal  (e.g. maximize network lifetime etc). The effectiveness of our framework is  exemplified by designing a tracking/surveillance application on top of it.   Efficient data collection in sparse WSNs by special nodes called Mobile Data  Collectors (MDCs) that visit sensor nodes is investigated. As contact times are  not known a priori and in order to minimize energy consumption, the discovery of  an incoming MDC by the static sensor node is a critical task. An adaptive  discovery framework is proposed that exploits DIRL and can be effectively  applied to various applications while minimizing energy consumption. The  principal idea is to learn MDC&#039s arrival pattern and tune sensor node&#039s duty  cycle accordingly. Through extensive simulation analysis, the energy efficiency  and effectiveness of the proposed framework is demonstrated.    Finally, design and evaluation of a complete and generalized middleware  framework called DReL is presented with focus on distributed sensor management  on top of our multi-layer reinforcement learning scheme. DReL incorporates  mechanisms and communication paradigm for task, data and reward distributions.  DReL provides an easy-to-use interface to application developers for creating  customized applications with specific QoS and optimization requirements.  Adequacy and efficiency of DReL is shown by developing few sample applications  on top of it and evaluating those applications&#039 performance.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Episodic Task Planning and Learning in Pervasive Environments</strong><br /> Thursday, November 18, 2010<br /> Yong Lin<br /><p><a href="#" id="506-show" class="showLink" onclick="showHide(506);return false;">Read More</a></p></p><div id="506" class="more"><p><a href="#" id="506-hide" class="hideLink" onclick="showHide(506);return false;">Hide</a></p><p><strong>Abstract: </strong>During planning and control of autonomous robots in a pervasive environment designed to serve people, we will inevitably face the situations of needing to perform multiple complex tasks. Management and optimization of the execution of complex tasks involve the design of efficient approach and framework based on algorithm, artificial intelligence, machine learning, cognitive science, etc. In this dissertation, we have developed a new method for complex task planning of robots, so that they can improve the service for the elderly and the disabled. The word ``episode&#039&#039 comes from the Greek word “ ”, which means “event”, or “occurrence”. Humans learn and plan from past episodes by connecting them to the current environment and the task at hand. In cognitive science, episodic memory refers to a human memory subsystem that is concerned with storing and remembering specific sequences and occurrences of events pertaining to a person&#039s ongoing perceptions, experiences, decisions and actions [Tulving 1983]. It helps a human plan the next task. In recent years, researchers have begun to realize the importance of episodic memory to artificial intelligence and cognitive robots, and the episodic like approaches to general event processing.  In this dissertation, we propose a computational framework that utilizes the idea of episodic memory to cope with robot planning on complex tasks. Our approach is based on the traditional mathematical model of Markov decision processes, combining the episodic memory approach. In this way, it provides a human-like thinking for autonomous robots, so that they can accomplish complex tasks in pervasive assistive environments, and thus achieve the goal of assisting the everyday living of people. In regard to the traditional hierarchical algorithms for Markov decision processes, although they have been proved to be useful for the problem domains with multiple subtasks due to their strength in task decomposition, they are weak in task abstraction, something that is more important for task analysis and modeling. Using episodic task planning and learning, we propose a task-oriented design approach, which addresses the functionality of task abstraction. Our approach builds an episodic task model from different problem domains, which the robot uses to plan at every step, with more concise structure and much improved performance than the traditional hierarchical model. According to our analysis and experimental evaluation, our approach has shown to have better performance than the existing hierarchical algorithms, such as MAXQ [Dietterich 1998] and HEXQ [Hengst 2002].  We further introduce a hierarchical multimodal framework for robot planning in multiple-sensor pervasive environments, using multimodal POMDPs. Considering realistic assistive applications may be time-critical and highly related with the risk of planning, we develop a risk-aware approach, allowing robots to possess risk attitudes [Howard 1972] in their planning. Thus, we have answered the question of how to plan and make sequential decisions efficiently and effectively under complex tasks in pervasive assistive environments, which is very important for the design of applications to assist the living of the elderly and the disabled. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Distributed Services in Pervasive System</strong><br /> Monday, November 15, 2010<br /> SAGAR TAMHANE<br /><p><a href="#" id="505-show" class="showLink" onclick="showHide(505);return false;">Read More</a></p></p><div id="505" class="more"><p><a href="#" id="505-hide" class="hideLink" onclick="showHide(505);return false;">Hide</a></p><p><strong>Abstract: </strong>Pervasive systems have found applications in various areas such as entertainment industry, healthcare, military and others. Devices in a pervasive system are generally resource constrained, heterogeneous, personal, and mobile. Each device may offer a set of software services to other devices in the network. A pervasive device might need to perform data processing tasks such as video and audio filtering in such applications as monitoring of elderly in homes for the aged. Devices may need to perform such tasks even if the required software is not installed locally on the device. Since the devices are resource constrained, a device might not have sufficient residual energy to complete the task. Hence collaboration between devices is exploited to perform computational tasks. In the past, service discovery, service composition and cyberforaging techniques have been used to facilitate resource sharing amongst devices in pervasive systems. Some researchers have performed offloading of computationally intense tasks from resource constrained mobile devices to high end servers. These servers are used to perform decision making and task execution. Such high end and centralized servers might not be available within a pervasive environment. Study of decentralized, autonomous and distributed execution of the tasks in pervasive systems is available in a very limited amount in the literature. 	This dissertation provides algorithms for cooperative service executions in pervasive environments that might not have high end or centralized servers. Pervasive systems are classified into smart spaces, Mobile Ad hoc Networks (MANETs), Vehicular Ad hoc Networks (VANETs) and Opportunistic Networks (OPNETs). The major contributions of this dissertation are: •	Cluster Based Scheduling (CBS) algorithm that performs service scheduling in smart spaces. •	Decentralized Grading Autonomous Selection (DGAS) algorithm that performs fault tolerant service execution in Mobile Ad hoc Networks. •	Mutual Exclusion for Opportunistic Networks (MEOP) algorithm that facilitates exclusive access to shared resources in opportunistic networks. •	An algorithm for performing service composition in opportunistic networks.  The main advantages of CBS over existing schemes are: reduced communication and storage overhead, and support for usage of multiple task scheduling algorithms. CBS factors such challenges as device heterogeneity, service availability and device mobility into the scheduling algorithm. CBS schedules tasks that have dependencies amongst each other. DGAS assigns independent tasks onto mobile service providers. Service execution on devices in pervasive systems might suffer from node and link failures. DGAS provides fault tolerance by replicating service execution onto multiple devices. When multiple devices need exclusive access to a shared resource, the middleware installed on the devices should facilitate the access. MEOP is a token requesting algorithm that provides exclusive access to resources in opportunistic networks. MEOP has lower communication overhead as compared to token ring algorithms. MEOP can be used along with any routing protocol. This dissertation also presents service composition algorithm for opportunistic network. Analysis of success probability of task execution and the length of compositions is presented and verified. A prototype of the service composition algorithms is implemented on PDAs and netbooks. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Context Reasoning under Uncertainty based on Evidential Fusion Networks in Home-based Care</strong><br /> Friday, July 16, 2010<br /> Hyun Lee<br /><p><a href="#" id="502-show" class="showLink" onclick="showHide(502);return false;">Read More</a></p></p><div id="502" class="more"><p><a href="#" id="502-hide" class="hideLink" onclick="showHide(502);return false;">Hide</a></p><p><strong>Abstract: </strong>Pervasive computing technologies use embedded intelligent systems to enable various real-time applications. Some of these applications are: continuous healthcare monitoring, autonomous diagnosis and treatment, and remote disease management without spatial-temporal limitations. Additional healthcare applications include home-based care, disaster relief management, medical facility management, and sports health management.  Issues related to the pervasive healthcare are generally classified into five categories: Hardware, Software, Regulations, Standardization and Organization. Our focus in this dissertation is on software issues. We propose new methods to generate a reliable context in a pervasive information system that has high rates of new measurements over time using data aggregation and data fusion. Different aggregation and fusion techniques can be applied depending on the types of sensed data and autonomous processing within the fusion step.  The goal of this research is to produce a high confidence level in the generated context for remote monitoring of patients. Reliable contextual information of remotely monitored patients can prevent hazardous situations by recognizing emergency situations in home-based care. However, it is difficult to achieve a high confidence level of contextual information for several reasons. First, the pieces of information obtained from multi-sensors have different degrees of uncertainty. Second, generated contexts can be conflicting even though they are acquired by simultaneous operations. And last, context reasoning over time is difficult because of unpredictable temporal changes in sensory information. In particular, some types of contextual information are more important than others in home-based care. The weight of this information may change, due to the aggregation of evidence and the variation of the values of evidence over time. This causes difficulty in defining the absolute weight of evidence in order to obtain the correct decision making.   In this dissertation, we propose an evidential fusion process as a context reasoning method based on the defined context classification and state-space based context modeling. First, the context reasoning method processes sensor data with an evidential form based on Dezert-Smarandache Theory (DSmT). The DSmT approach reduces ambiguous or conflicting contextual information in multi-sensor networks. Second, we deal with dynamic metrics such as preference, temporal consistency, and relation-dependency of the context using Autonomous Learning Process (ALP) and Temporal Belief Filtering (TBF) to improve the confidence level of the information, so as to make a correct decision about the situation of the patient. And last, we deal with both relative and individual importance of evidence to obtain the optimal weight of evidence. We then apply dynamic weights of evidence into Dynamic Evidential Network (DEN) to improve the belief level of the context and to understand the emergency progress of the patient in home-based care.  Finally, we compare the Evidential Fusion Process on DSmT with traditional fusion processes such as Bayesian Networks (BNs), Dempster-Shafer Theory (DST), and Dynamic Bayesian Networks (DBNs). This comparison makes us understand the uncertainty analysis in decision-making by distinguishing sensor reading errors (i.e., False alarm) from new sensor activations, and shows the improvement of our proposed method compared to the others.  The main contributions of the proposed context reasoning method under uncertainty based on evidential fusion networks are: 1) Reducing the uncertainty level and improving the confidence level by adapting the DSmT, PCR5 combination rule, and GPT, 2) Distinguishing the sensor reading error from new sensor activations by considering the ALP and the TBF algorithm, and 3) Representing the optimal weights of evidence by applying the normalized weighting technique. These advantages help to make correct decisions about the situation of the patient in home-based care. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Data Analytics Over Hidden Databases</strong><br /> Thursday, July 15, 2010<br /> Arjun Dasgupta<br /><p><a href="#" id="503-show" class="showLink" onclick="showHide(503);return false;">Read More</a></p></p><div id="503" class="more"><p><a href="#" id="503-hide" class="hideLink" onclick="showHide(503);return false;">Hide</a></p><p><strong>Abstract: </strong>Web based access to databases have become a popular method of data delivery. A multitude of websites provides access to their proprietary data through web forms. In order to view this data, customers use the web form interface and pose queries on the underlying database. These queries are executed and a resulting set of tuples (usually the top-k ones) is served to the customer. Top-k along with strict limits on querying are constraints used by the database providers to conserve the power of the underlying data distribution.  Delivering limited access only to tuples that satisfy a query enables providers to expose only a small snippet of the entire inventory at a time. This method of data delivery prevents analysts from deriving information on the holistic nature of data. Analytical queries on the data statistics are hence blocked through these access restrictions.  The objective of this work is to provide detailed approaches that obtain results towards inferring statistical information on such hidden databases, using their publicly available front-end forms. To this end, we first explore the problem of random sampling of tuples from hidden databases. Samples representing the underlying data open up a proprietary database to a plethora of opportunities by giving external parties a glimpse into the holistic aspects of the data. Analysts can use samples to pose aggregate queries and gain information on the nature and quality of data. In addition to sampling, we also present efficient techniques that directly produce unbiased estimate of various interesting aggregates. These techniques can be also applied to address the more general problem of size estimation of such databases.  In light of techniques towards inferring aggregates, we introduce and motivate the problem of privacy preservation in hidden databases from the data provider’s perspective, where the objective is to preserve the underlying aggregates while serving legitimate customers with answers to their form-based queries.   </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Advanced Software Testing Techniques Based on Combinatorial Design</strong><br /> Wednesday, July 14, 2010<br /> Wenhua Wang<br /><p><a href="#" id="501-show" class="showLink" onclick="showHide(501);return false;">Read More</a></p></p><div id="501" class="more"><p><a href="#" id="501-hide" class="hideLink" onclick="showHide(501);return false;">Hide</a></p><p><strong>Abstract: </strong>Combinatorial testing refers to a testing strategy that applies the principles of combinatorial design to the domain of software test generation. Given a system with k parameters, combinatorial testing requires all the combinations involving t out of k parameters be covered at least once, where t is typically much smaller than k. The key insight behind combinatorial testing is that while the behavior of a system may be affected by many parameters, most faults are caused by interactions involving only a small number of parameters. Empirical studies have shown that combinatorial testing can dramatically reduce the number of tests while remaining effective for fault detection.               Existing work on combinatorial testing has mainly focused on functional requirements, and has only considered non-interactive systems, i.e., systems that take all inputs up front without interacting with the user in the middle of a computation. In this dissertation, we develop three new combinatorial testing techniques, two of which deal with interactive web applications, and the third one deals with non-functional security requirements: (1) Combinatorial construction of web navigation graphs: A navigation graph captures the navigation structures of a web application. The main challenge is handling dynamic web pages that are only generated at runtime and in response to user requests. We develop a combinatorial approach that generates user requests to discover these dynamic web pages. We report a software tool called Tansuo, and demonstrate the effectiveness of our approach using several real-life open-source web applications. (2) Combinatorial test sequence generation for web applications: One important aspect of web applications is that they often consist of dynamic web pages that interact with each other by accessing shared objects. It is nearly always impossible to test all possible interactions that may occur in a web application of practical scale. We develop a combinatorial approach to systematically exercising these interactions. Our experimental results show that our approach can effectively detect subtle interaction faults that may exist in a web application. (3) Detection of buffer overflow vulnerabilities: Buffer overflow vulnerabilities are program defects that can cause a buffer to overflow at runtime. Many security attacks exploit buffer overflow vulnerabilities to compromise critical data structures. We develop a combinatorial approach to detecting buffer overflow vulnerabilities. Our approach exploits the fact that combinatorial testing often achieves a high level of code coverage. Our experimental results show that our approach can effectively detect buffer overflow vulnerabilities.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>COMBATING DOS ATTACKS IN WIRELESS NETWORKS USING LIGHTWEIGHT KEY MANAGEMENT SCHEMES</strong><br /> Tuesday, June 15, 2010<br /> Qi Dong<br /><p><a href="#" id="495-show" class="showLink" onclick="showHide(495);return false;">Read More</a></p></p><div id="495" class="more"><p><a href="#" id="495-hide" class="hideLink" onclick="showHide(495);return false;">Hide</a></p><p><strong>Abstract: </strong>The unique features of wireless networks lead to many attractive applications in both military and civilian operations. Equipments in wireless networks usually use batteries as power supply, which requires all the operations to be as efficient as possible. Since the resources such as the battery energy and wireless communication channels are critical in wireless networks, the attacker my try to disable the system operation by launching denial of service (DoS) attacks. Specifically, the attacker prevents the system from working by using up or blocking the limited resources.  This dissertation includes three studies on security mechanisms to combat DoS attacks using lightweight key management schemes. The first study introduces a novel pairwise key establishment technique. It can achieve both high resilience to node compromises and high efficiency by using a small number of additional sensor nodes.  The second study presents two filtering techniques, a group-based filter and a key chain-based filter, to handle DoS attacks when digital signatures are used for broadcast authentication in sensor networks. Both methods are efficient for resource-constrained sensor networks and can significantly reduce the number of unnecessary signature verifications that a sensor node has to perform.  The third study introduces two techniques for jamming-resistant broadcast systems, partial channel sharing technique and unpredictable channel assignment. Both schemes can significantly reduce the extra communication cost. The analytic and simulation results show that the proposed approaches greatly push limit of jamming-resistant broadcast towards optimal.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Event Recognition from Ambient Assistive Living Technologies</strong><br /> Friday, May 14, 2010<br /> Eric Becker<br /><p><a href="#" id="493-show" class="showLink" onclick="showHide(493);return false;">Read More</a></p></p><div id="493" class="more"><p><a href="#" id="493-hide" class="hideLink" onclick="showHide(493);return false;">Hide</a></p><p><strong>Abstract: </strong>As the population ages and technology advances, a need exists for creating ambient intelligent systems to be placed within the home environment. Attitudes towards technology have been changing, and home monitoring is now considered a less expensive and desirable alternative. Ideally, such systems should be small, wireless, and take the minimum of effort and cost to install and place within the home.  In order to detect human activity in an assistive environment, key questions about the construction and operation of the technology and methods needed to detect that activity. To that end, a computational framework has been created inside an apartment testbed combining a variety of algorithms, tools, and methods that support an assistive living apartment using Wireless Sensor Networks and other devices and sensors.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>INFORMATION-DRIVEN DATA GATHERING IN WIRELESS SENSOR NETWORKS</strong><br /> Monday, May 10, 2010<br /> Jing Wang<br /><p><a href="#" id="492-show" class="showLink" onclick="showHide(492);return false;">Read More</a></p></p><div id="492" class="more"><p><a href="#" id="492-hide" class="hideLink" onclick="showHide(492);return false;">Hide</a></p><p><strong>Abstract: </strong>Since the advances of wireless technology enable the mass production of low-cost, small-sized sensor nodes, the sensor nodes can be densely deployed in an area for the purpose of high tolerance to node failure or to achieve better coverage statistically. The presence of dense wireless sensor networks leads to severe competition for the limited resources, cooperation opportunity to fulfill the application level tasks and redundancy in the network in terms of temporal and spatial correlation of the sensory data.     Temporal and Spatial correlation in dense wireless sensor networks motivate the research on exploiting the correlated sensory data to extend lifetime of wireless sensor networks. Aiming at temporal correlation in the sensory data, compression based approaches was proposed to compress the sensory data at the nodes&#039 side and to restore the sensory data through decompression at the sink. Whereas, node selection schemes focus on the spatial correlation among the sensor nodes. At any moment, only a subset of the sensor nodes are involved in the sensing, processing and transmitting of the sensory data to achieve the energy efficiency goal.  Being able to address the spatial correlation in conjunction with the temporal correlation, compressive sensing and distributed source coding were introduced to extend the lifetime of the sensor networks through reducing the amount of traffic to be transferred from the sensor nodes to the sink. The spatially correlation sensor nodes work cooperatively in compressive sensing and distributed source coding to send processed sensory data that are necessary for the reconstruction of the raw sensory data at the sink&#039s side.     The existing approaches basically target at sensory data that are already high correlated with each other. The idea of changing the sampling strategy of the sensor nodes to reduce the correlation among the sensory data has been paid little attention. In this dissertation, sampling strategies and the relevant medium access control protocol are presented to demonstrate how the correlation can be reduced through adjusting the sampling moments of the sensor nodes. The gaol of the sampling strategies and the proposed medium access control protocol is to achieve a better tradeoff between the energy consumption of the WSNs and the performance at the application level.  The temporal-spatial correlation model based on the exponential correlation model is established firstly. After justifying the correlation model using real data from implementations of wireless sensor networks, the asynchronous sampling strategies are proposed to introduce the shifts of the sampling moments for sensor nodes, which reduce the correlation of the sensory data according to the temporal-spatial correlation model. Furthermore, the entropy of the joint Gaussian random variables is adopted to quantify the improvement on the information metric by asynchronous sampling strategies. Oppeinhem&#039s inequality is applied to prove the entropy is increased by the introduction of the non-zero temporal correlation parameter. A recursive algorithm is presented to solve the optimal asynchronous sampling problem with a set of sub-optimal sampling shifts. Bounds on the performance of three asynchronous sampling strategies are derived respectively.     Motivated by the benefit of asynchronous sampling, an information-driven medium access control protocol is proposed to avoid the severe collisions of event reports in the event detection applications. Other than choosing a subset of nodes to report the event, the proposed protocol assigns sampling shifts to nodes in order to change the bursty traffic into streamlining traffic. Consequently, the MAC performance is improved by essentially replacing the bursty traffic. An optimal probability model is adopted for selecting nodes&#039 transmission slots that minimize the collisions of transmissions and in turn reduce the correlation among event reports. Through theoretical analysis and simulations, it is shown that the protocol relates the MAC performance with the information quality of the event reports and that the Cramer-Rao lower bound, which quantifies the performance of the application tasks, is lowered by introducing the ID-MAC protocol to shift the nodes&#039 s sampling moments from each other.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>HIGH PERFORMANCE REAL-TIME EMBEDDED SYSTEMS: DESIGN AND IMPLEMENTATION</strong><br /> Friday, April 16, 2010<br /> Jareer Qader<br /><p><a href="#" id="489-show" class="showLink" onclick="showHide(489);return false;">Read More</a></p></p><div id="489" class="more"><p><a href="#" id="489-hide" class="hideLink" onclick="showHide(489);return false;">Hide</a></p><p><strong>Abstract: </strong>Multi-core processors are becoming main components for many embedded systems. Designing such embedded systems is a complex task, since developers must deal with a number of issues such as multi-threading and optimal use of parallel processing. Multi-core processors can provide benefits to embedded applications in terms of performance improvement and power utilization. Problems related to real-time embedded systems in terms of designing, modeling, simulating, and implementing both hardware and software parts of embedded systems will be researched and discussed in this work.  This research investigates and establishes the necessary steps to design, analyze, and implement a real-time embedded multi-core application.  The application used is a real-time road surface analyzer system. The system is used for measuring, collecting, processing, and displaying pavement surface characteristics. This application is based on the road profiler, one of the main instruments used by transportation engineers to test road surfaces and determine their condition. The research analyzes the requirements of the system, defining and designing the appropriate tools needed for implementing the real-time multi-core embedded system.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Framework for Robust Detection and Prevention of Wide-Spread Node Compromise in Wireless Sensor Networks</strong><br /> Friday, April 16, 2010<br /> Jun-Won Ho<br /><p><a href="#" id="490-show" class="showLink" onclick="showHide(490);return false;">Read More</a></p></p><div id="490" class="more"><p><a href="#" id="490-hide" class="hideLink" onclick="showHide(490);return false;">Hide</a></p><p><strong>Abstract: </strong>Wireless sensor networks are known to be vulnerable to a variety of attacks that could undermine normal sensor network operations. Many schemes have been developed to defend the wireless sensor networks against various attacks. Most of them focus on making the network and service protocols be attack-resilient rather than rooting out the source of attacks. Although the attack-resiliency approach mitigates the threats on sensor network protocols, it requires substantial time and effort for continuously enhancing the robustness of the protocols in line with the emergence of new types of attacks. Accordingly, if we are able to detect and remove the sources of attacks as soon as possible, we would save the large amount of time and effort incurred from employing the attack-resiliency approach. In wireless sensor networks, the principle sources of various attacks are compromised nodes. Specifically, since sensor nodes are deployed in an unattended manner, an adversary can physically capture and compromise sensor nodes, and mount a variety of attacks with the compromised nodes. He can also move the compromised nodes to multiple locations to evade the detection. Moreover, he can create wide-spread influence by generating many replica nodes of a few compromised nodes or propagating malicious worm into the network. Our works are designed for rooting out the sources of possible threats by quickly detecting and removing static/mobile compromised nodes and preventing wide-spread node compromise through replica node and worm propagation attacks.  To meet this challenge, we propose a framework for robust detection and revocation of wide-spread node compromise in wireless sensor networks. In the framework, we first propose reputation-based trust management scheme to facilitate static node compromise detection, and then propose a distributed detection scheme to achieve fast mobile node compromise detection, and finally propose replica node detection and worm propagation detection schemes to prevent wide-spread node compromise. Specifically, the framework is composed of five components. In the first component, we quickly detect the suspected regions in which compromised nodes are likely placed and perform software attestation against the nodes in the suspected regions, leading to the detection and revocation of the compromised nodes. However, if the attacker moves the compromised nodes to multiple locations in the network, such as by employing simple robotic platforms or moving the nodes by hand, he can evade the detection scheme in the first component.  In the second component, to resolve this limitation, we quickly detect these mobile malicious nodes that are silent for unusually many time periods---such nodes are likely to be moving---and block them from communicating in fully distributed manner.   To reduce the time and effort incurred from directly compromising many benign nodes, attacker may launch replica node attacks in which he generates many replica nodes of a few compromised nodes and widely spread them over the network. To thwart wide-spread node compromise by replica node attacks, we propose two complementary schemes for replica detection as the second and third components. In the third component, we detect static replica nodes by leveraging the intuition that static replica nodes are placed in more than one location. In the fourth component, we quickly detect mobile replicas by leveraging the intuition that mobile replicas are in two or more locations at once and thus appear to move much faster than benign nodes, leading to highly likely exceed the predefined maximum speed.  However, the attacker needs to prepare as many sensor nodes as the number of replicas that he wants to generate in replica node attacks. Thus, the attack costs will increase in proportion to the number of deployed replicas. To reduce these costs, the attacker may attempt to widely spread node compromise by capturing a few nodes and having the captured nodes propagate malicious worm through the network, leading to the fast compromise of many benign nodes. In the fifth component, to fight against this type of attack, we quickly detect worm propagation in fully distributed fashion by leveraging the intuition that worm code&#039s communication pattern is different to one of benign packet.  Through analysis and experimental study, we show that these components achieve robust and effective detection and revocation capability of static/mobile node compromise, replica node, worm propagation with reasonable overhead. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>ENHANCING PERSONALIZED SEARCH AND IMPROVING ACCURACY AND PERFORMANCE FOR KEYWORD-BASED XML QUERIES</strong><br /> Friday, March 26, 2010<br /> Kamal Taha<br /><p><a href="#" id="483-show" class="showLink" onclick="showHide(483);return false;">Read More</a></p></p><div id="483" class="more"><p><a href="#" id="483-hide" class="hideLink" onclick="showHide(483);return false;">Hide</a></p><p><strong>Abstract: </strong>This dissertation research focuses on three aspects related to querying of XML data. The three focus areas are: (1) Improving accuracy of XML keyword queries by modeling the contexts of XML elements; (2) Enhancing XML-based personalized search by using group profiling to determine individual preferences; and (3) Improving performance of distributed XML querying by caching of frequently-used query results. For each of these three focus areas, we developed formal concepts and algorithms that lead to the improved accuracy and performance. Our contributions are as follows: 1.	Improving the accuracy of XML keyword queries: We improve search accuracy by utilizing nodes’ contexts in an XML tree. Overlooking nodes’ contexts when building relationships between the nodes may lead to erroneous query results. The context of a data node is determined by its parent node. By treating each set of nodes consisting of a parent and its children data nodes as one unified entity and then determining the relationships between the different unified entities, an XML system can build much more accurate relationships between data nodes in less processing time, resulting in more accurate query results. 2.	Enhancing XML-based personalized search:  By pre-defining and categorizing social groups based on demographic, ethnic, cultural, religious, or other characteristics, a user profile could be inferred from the profiles of the social groups to which the user belongs. This would simplify personalized search and make its process more efficient. We implemented this approach in an XML-based recommender system. The system is able to output ranked lists of content items taking into account not only the initial preferences of the user, but also the preferences of the user’s various social groups. 3.	Improving performance of distributed XML querying: Distributed XML documents are too big and complicated to be rapidly queried every time a user submits a query due to the overhead involved in decomposing the queries, sending the decomposed queries to remote site(s), and executing structural join operations to compose the results. We investigated strategies and mechanisms to tackle these problems. We then implemented these mechanisms in a query processor, and compared their performance to standard XML query processors.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An integrated framework for QoS-aware data reporting in wireless sensor networks</strong><br /> Thursday, December 03, 2009<br /> Hyun Choe<br /><p><a href="#" id="480-show" class="showLink" onclick="showHide(480);return false;">Read More</a></p></p><div id="480" class="more"><p><a href="#" id="480-hide" class="hideLink" onclick="showHide(480);return false;">Hide</a></p><p><strong>Abstract: </strong>Wireless sensor networks are being deployed in a wide variety of applications such as environment monitoring, smart buildings, security, machine surveillance system, and so on.  The deployment of sensor networks for a specific sensing application enhances the ability to control and examine the physical environments while collecting meaningful information from the monitoring area.  In densely deployed networks, the sensor nodes located in an adjacent area detect the targeted phenomena in its sensing range and report the gathered (raw or processed) data to designated sinks via single-hop or multi-hop communication paths.  Although the correlation of data from proximity sensors cause overheads in terms of energy consumption for data delivery and processing, yet they improve data accuracy.  Therefore, the definition of quality of service (QoS) and the metrics to evaluate the performance of a wireless sensor network are different from traditional networks in that the QoS attributes highly depend on the specific sensing tasks and applications.  While energy efficiency is an important consideration for designing algorithms and protocols for wireless sensor networks, other QoS parameters such as the coverage rate, the end-to-end delay, fairness, throughput, and error rates for delivery or sensing may be equally important depending on the application objectives.  Thus, an important issue in a sensor network is to design task-specific QoS-aware data reporting algorithms and protocols that optimize resource consumption and extend the network lifetime.  In this dissertation, we propose an integrated framework for QoS-aware data reporting in wireless sensor networks.  More specifically, the proposed framework is designed for single-hop cluster-based wireless sensor networks and includes two strategies: an intra-cluster data reporting control strategy (IntraDRC) and an inter-cluster data reporting control strategy (InterDRC). The IntraDRC strategy is based on the selection of data reporting nodes that applies the block design concept from combinatorial theory and a novel two-phase node scheduling (TNS) scheme that defines class-based data reporting rounds and node assignment for each time slot.  The objective of IntraDRC is to provide optimized data reporting control in a distributed manner.  In this strategy, a certain number of data reporting nodes are selected in each cluster in order to satisfy the throughput fidelity specified by the applications while reducing redundant data reporting by selecting a subset of cluster members.  This intra-cluster reporting control eventually helps control the overall amount of traffic in the network.  The TNS scheme schedules data reporting while considering the priority of data, yet guaranteeing that sensor nodes compete with each other in the same class only.  The InterDRC strategy, on the other hand, is based on QoS-aware data reporting tree management scheme that balances the trade-off between the end-to-end delay and energy efficiency.  The idea of this strategy is to manage variants of the data reporting tree based on two information, such as the hop counts to a data sink and the traffic amount generated from local area.  For this purpose, each cluster head analyzes the traffic scenario of its cluster for load balancing and congestion control, thus improving the overall network performance.  In InterDRC, the proposed spanning tree construction algorithm first builds the fewest hop-based reporting tree, used for delay constrained data delivery.  This tree is updated with traffic load information in order to construct a traffic-adaptive reporting tree, used for energy efficient data delivery. By separating the controls of data reporting within a cluster and that from one cluster to another, the proposed integrated framework can define different levels of various QoS parameters in each intra-cluster data reporting as well as inter-cluster reporting.  To the best of our knowledge, we are the first to propose node arrangement using block designs in order to design task-specific data report scheduling in wireless sensor networks.  This node arrangement strategy facilities an efficient local data collection in a cluster.  Simulation results demonstrate that the proposed framework results in a significant conservation of energy by reducing the competition between data reporting nodes and establishing traffic-adaptive data reporting paths.  The results also show that the throughput performance of our integrated framework is especially good due to stable data reporting independent of the network density.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Privacy Enhanced Situation-Aware Middleware Framework for Ubiquitous Computing Environments</strong><br /> Thursday, December 03, 2009<br /> Gautham Pallapa<br /><p><a href="#" id="481-show" class="showLink" onclick="showHide(481);return false;">Read More</a></p></p><div id="481" class="more"><p><a href="#" id="481-hide" class="hideLink" onclick="showHide(481);return false;">Hide</a></p><p><strong>Abstract: </strong>The Ubiquitous Computing paradigm integrates myriads of small, inexpensive, heterogeneous networked devices, including sensors, distributed throughout the environment, with the intent of enabling context awareness in systems deployed to monitor the environment. This is accomplished by monitoring events, such as access, or utilization of resources, and obtaining knowledge about user activities, and interactions with other entities in the environment. Existing context-aware systems predominantly encapsulate the occurred activities either by using Event-Condition-Action rules, where an instance of the event performs as a trigger, or by prediction mechanisms, such as Dynamic Bayesian Networks, which compute decisions, based on the information obtained. However, these approaches are constrained by computational overheads, rule complexities, and potential loss of information, introduced by deconstructing activities. This emphasizes the need for a ?natural interaction? paradigm involving the input from the user and the environment in a cooperative manner, making it imperative to understand the potential relationship between activity and the embedded context information. In this dissertation, user activity is described by a finite number of states called situations, characterized by interaction with other entities or devices in the environment. The information perceived from these situations, enable systems deployed in the environment to monitor interactions, and develop dynamic rules customized to the user. Deploying such systems on a significant scale, however, introduces the additional challenge of protecting information among users, thereby accentuating the need for robust privacy management.<br/>This dissertation focuses on the challenges of situation perception, user privacy, and human-computer interaction through ubiquitous middleware. We investigate the limitations of deconstructing context to capture information required to describe situations. We discuss our approach to understand user interactions as situations, by introducing the concept of situation trees, built by parsing the sequence of contexts and device information obtained from the monitored environment. We then present our scheme to build the vocabulary and situation grammar from the situation trees, which facilitates behavior-specific dynamic rule generation, and demonstrate the potential of our scheme for efficient decision making. This dissertation also looks at the conundrum of privacy, and we have devised various approaches to quantify user privacy in ubiquitous computing environment. We compare these approaches in various scenarios, and present our experimental results and findings. We finally present the design of our ubiquitous middleware framework to support the perception, modeling, rule generation, and privacy management of user interactions, and examine the effectiveness of our framework in an assisted environment. The experimental results presented in this dissertation substantiate the effectiveness of our approach to situation perception and privacy quantization.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Attributes Selection and Package Design to Maximize Visibility of Objects</strong><br /> Friday, October 23, 2009<br /> Muhammed Miah<br /><p><a href="#" id="474-show" class="showLink" onclick="showHide(474);return false;">Read More</a></p></p><div id="474" class="more"><p><a href="#" id="474-hide" class="hideLink" onclick="showHide(474);return false;">Hide</a></p><p><strong>Abstract: </strong>In recent years, there has been significant interest in the development of ranking functions and efficient top-k retrieval algorithms to help users in ad-hoc search and retrieval in databases (e.g., buyers searching for products in a catalog). We introduce a complementary problem: how to guide a seller in selecting the best attributes of a new tuple (e.g., a new product) to highlight so that it stands out in the crowd of existing competitive products and is widely visible to the pool of potential buyers. For example, assume one wants to sell an iPod in e-commerce site, but the title allows only 12 characters of space, should he write &quot;Purple iPod&quot;, &quot;Apple iPod&quot; or &quot;iPod 30g&quot;? Which title is good? Do people care more about color, brand or size? We refer this problem as ?attributes selection? problem. Package design based on user input is a problem that has also attracted recent interest. Given a set of elements, and a set of user preferences (where each preference is a conjunction of positive or negative preferences for individual elements), we investigate the problem of designing the most ?popular package?, i.e., a subset of the elements that maximizes the number of satisfied users. Numerous instances of this problem occur in practice. For example, a vacation package consisting of a subset of all possible activities may need to be assembled,  that satisfies as many potential customers as possible, where each potential customer may have expressed his preferences (positive or negative) for certain activities. Likewise, selecting topics for a social network group based on users? preferences as well as the problem of designing new products, i.e., deciding which features to add to a new product (e.g., to an iPod) that satisfies as many potential customers as possible, also falls under this framework. We refer this later problem as ?package design? problem. We develop several formulations of both the problems. Even for the NP-complete problems, we give several exact (optimal) and approximation algorithms that work well in practice.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>AUTONOMIC TRUST MANAGEMENT IN DYNAMIC SYSTEMS</strong><br /> Monday, July 20, 2009<br /> Brent Lagesse<br /><p><a href="#" id="473-show" class="showLink" onclick="showHide(473);return false;">Read More</a></p></p><div id="473" class="more"><p><a href="#" id="473-hide" class="hideLink" onclick="showHide(473);return false;">Hide</a></p><p><strong>Abstract: </strong>Research in pervasive computing is aimed at creating environments where users can seamlessly benefit from the ubiquitous computing resources whilst not having to notice that those resources are operating in a complex environment.  Providing security in such systems is a difficult task since traditional security mechanisms often require significant user attention and do not scale well to large, mobile, and open environments.  To combat this problem, distributed trust is often used to provide security in pervasive systems.  While much research has been performed in the area, many vulnerabilities and insufficiencies still exist, especially in systems such as mobile ad-hoc systems that cannot support distributed trust mechanisms requiring pre-existing infrastructure and cooperation.  Some pervasive systems fall into a class of dynamic pervasive systems.  These systems operate in highly dynamic environments that introduce more challenges such as intermittent connectivity and lack of infrastructure.   This dissertation discusses several pertinent problems in the design and deployment of distributed trust mechanisms in dynamic pervasive systems.  Furthermore, four systems designed to enhance security in pervasive computing are discussed.  The first of these systems is the Distributed Trust Toolkit (DTT) which is a modular framework for the design and deployment of distributed trust mechanisms over a wide variety of systems, networks, and devices.  Two other systems, built for two specific classes of applications that occur frequently in pervasive computing, are an Adaptive Resource Exploration (AREX) and a Reliable Service Composition (ReSCo).  AREX uses a game theoretic approach to motivate strategic, malicious entities to attack less often.  ReSCo is designed for dynamic service composition systems and works by adapting to make selections of compositions paths and nodes.  The final system, SoTru, is a framework and system for augmenting trust mechanisms such as AREX and ReSCo with information from users&#039 social networks to reduce risk and enhance their performance.  As standalone systems, each of these addresses challenges in securing dynamic systems.  The DTT eases development of trust mechanisms and reduces energy and bandwidth costs, AREX and ReSCo provide scalable, low cost security mechanisms that provide protection despite hostile, open, and mobile environments.  When used together, with the addition of SoTru, the outcome of this dissertation enhances the effectiveness and seamlessness of security in dynamic systems.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>AUTONOMOUS ABSTRACTION OF POLICIES BASED ON POLICY HOMOMORPHISM</strong><br /> Thursday, July 16, 2009<br /> Srividhya Rajendran<br /><p><a href="#" id="472-show" class="showLink" onclick="showHide(472);return false;">Read More</a></p></p><div id="472" class="more"><p><a href="#" id="472-hide" class="hideLink" onclick="showHide(472);return false;">Hide</a></p><p><strong>Abstract: </strong>A life long learning agent performing in a complex and dynamic environment needs the ability to learn complex tasks over time. These agents over their life time have to learn new tasks, adapt the policies of already learned tasks and extract / reuse the knowledge gained to learn new, more complex tasks. To do this, they need methods that allow them to autonomously extract knowledge from the already learned policy instances of a task type and reuse the knowledge gained to learn related tasks in novel environments.  This dissertation presents a novel approach that enables the agent to autonomously abstract reusable skills and concepts using policy instances of a similar task type and use the resulting abstractions to learn related tasks in novel situations. To achieve this, this work formalize a novel idea of policy homomorphism that allows autonomous extraction of general policies for task types. Each extracted general policy is here an abstract policy that is homomorphic to the set of specific policy instances of the corresponding task type that it is derived from and is made up of abstract states that identify situations in which the given policy is applicable and abstract actions that identify actions that need to be performed in those situations. Once extracted, the generalized policies are reused in new contexts to address related tasks by adding them as a higher level action that the agent can choose to perform.  To facilitate the autonomous abstraction of a policy of a given task type from a set of policies, the agent has to have the ability to identify and categorize policies for various tasks into different task types. To achieve this the policy generalization approach presented here employs a utility-based criterion that enables the agent to autonomously categorize and generalize a set of situation specific policies of different task types into a set of general policies containing one general policy for each identified task type using the policy homomorphism framework. To demonstrate the working of this policy generalization method we show the abstraction of a general policy for a specific task type using two sets of policies of different task types in a grid world domain and further show how the abstracted general policies can be used to learn related tasks in novel grid world environments.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Energy Efficient Routing Structures and Wakeup Schemes for Wireless Sensor Networks</strong><br /> Monday, May 04, 2009<br /> Byoungyong Lee<br /><p><a href="#" id="466-show" class="showLink" onclick="showHide(466);return false;">Read More</a></p></p><div id="466" class="more"><p><a href="#" id="466-hide" class="hideLink" onclick="showHide(466);return false;">Hide</a></p><p><strong>Abstract: </strong>Wireless sensor networks, which consist of a large number of sensor nodes and a base station, are used for many applications aimed at collecting information. Each sensor node is equipped with a small amount of battery, limited memory, finite radio range and small CPU. It gathers required information and it sends the information to the base station. The large number of sensors can cover a large area by cooperating with each other to build a multi-hop wireless network. However, the small amount of battery is one of the critical concerns because sensor network life time depends on battery longevity. It is hard to replace or recharge the battery in each sensor node. Generally a sensor node consumes its energy during processing, receiving, transmitting and overhearing of messages. Among those, we focus on reducing the data communication and overhearing energy consumption. In order to accomplish these tasks, we propose novel energy efficient routing structures and wakeup schemes in this dissertation.   First we propose an energy balanced technique for in-network aggregation with multiple tree structure (MULT). We try to reduce concentrating network traffic on a few special nodes. For building the multiple tree structure, we first create node clusters, and then connect the nodes in each cluster. Finally with cluster head nodes, we construct a multiple tree structure.  In the second technique, we propose a sensor network subtree merge algorithm (SNSM), which uses the union of disjoint set forest algorithm to avoid unnecessary energy consumption in ancestor nodes for routing. We reduce the energy consumption for routing in sensor network for spatial range query through the SNSM algorithm. We apply SNSM algorithm to a minimum spanning tree structure. For our third contribution, we make a wakeup scheme to reduce the overhearing energy consumption using different wakeup time scheduling on children nodes. Our wakeup scheme includes two wakeup schedules. One is odd and even wakeup scheduling (OEWS) and another is individual wakeup scheduling (IWS). There is a trade off between reducing overhearing energy consumption and delay time. Therefore we propose double tree structure called DTS to reduce the delay time. Simulation results illustrate that our energy efficient routing structures and wakeup schemes extend the sensor network lifetime and make a small trade-off between energy consumption and delay time.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SEARCHING AND RANKING XML DATA IN A DISTRIBUTED ENVIRONMENT</strong><br /> Monday, November 24, 2008<br /> WEIMIN HE<br /><p><a href="#" id="460-show" class="showLink" onclick="showHide(460);return false;">Read More</a></p></p><div id="460" class="more"><p><a href="#" id="460-hide" class="hideLink" onclick="showHide(460);return false;">Hide</a></p><p><strong>Abstract: </strong>Due to the increasing number of independent data providers on the web, there is a growing number of web applications that require searching and querying data sources distributed at different locations over the internet. Since XML is rapidly gaining in popularity as a universal data format for data exchange and integration, locating and ranking distributed XML data on the web are gaining importance in the database community. Most of existing XML indexing techniques combine structure indexes and inverted lists extracted from XML documents to fully evaluate a full-text query against these indexes and return the actual XML fragments of the query answer. In general, these approaches are well-suited for a centralized date repository since they perform costly containment joins over long inverted lists in order to evaluate full-text XML queries, which does not scale very well to large distributed systems.  In this thesis work, we present a novel framework for indexing, locating and ranking schema-less XML documents based on concise summaries of their structural and textual content. Instead of indexing each single element or term in a document, we extract a structural summary and a small number of data synopses from the document, which are indexed in a way suitable for query evaluation. The search query language used in our framework is XPath extended with full-text search. We introduce a novel data synopsis structure to summarize the textual content of an XML document that correlates textual with positional information in a way that improves query precision. In addition, we present a two-phase containment filtering algorithm based on these synopses that speeds up the searching process. To return a ranked list of answers, we integrate an effective aggregated document ranking scheme into the query evaluation, inspired by TF*IDF ranking and term proximity, to score documents and return a ranked list of document locations to the client. Finally, we extend our framework to apply to structured peer-to-peer systems, routing a full-text XML query from peer to peer, collecting relevant documents along the way, and returning list of document locations to the user. We conduct many experiments over XML benchmark data to demonstrate the advantages of our indexing scheme, the query precision improvement of our data synopses, the efficiency of the optimization algorithm, the effectiveness of our ranking scheme and the scalability of our framework.  We expect that the framework developed in this thesis will serve as an infrastructure for collaborative work environments within public web communities that share data and resources. The best candidates to benefit from our framework are collaborative applications that host on-line repositories of data and operate on a very large scale. Furthermore, good candidates are those applications that seek high system and data availability and scalability to the network growth. Finally, our framework can also benefit to those applications that require complex/hierarchical data, such as scientific data, schema flexibility, and complex querying capabilities, including full-text search and approximate matching. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>HELMHOLTZ’S THEOREM BASED PARAMETRIC NON-RIGID IMAGE REGISTRATION</strong><br /> Friday, November 21, 2008<br /> Hsi-Yue (Sean) Hsiao<br /><p><a href="#" id="456-show" class="showLink" onclick="showHide(456);return false;">Read More</a></p></p><div id="456" class="more"><p><a href="#" id="456-hide" class="hideLink" onclick="showHide(456);return false;">Hide</a></p><p><strong>Abstract: </strong>Helmholtz’s theorem states that, with suitable boundary condition, a vector field is completely determined if both of its divergence and curl are specified everywhere. Based on this, we developed a new parametric non-rigid image registration algorithm. Instead of the displacements of regular control grid points, the curl and divergence at each grid point are employed as the parameters. This leads to a very simple mathematical model - two Poisson equations in 2-D or three Poisson equations in 3-D which will be used to solved for the displacement field. The similarity measure is sum of squared difference. And multi-resolution is applied on the gradient descent optimization.   In this dissertation, we reviewed two closest related works are reviewed. The first one is the fast parametric elastic image registration which the parameters are the Bspline coefficients of the displacement field at each control grid point. However, in that work, it is very likely to result in grid folding in the final deformation field if the distance between adjacent control grid points (knot spacing) is less than 8. This implies that the high frequency components in the deformation field can not be accurately estimated. Another relevant work is the parametric non-rigid image registration method based on Helmholtz decomposition. In that work, the deformation is considered as the result of two types of particles, namely the vortex particles and sink and source particles. Two parameters are associated with each particle: the vorticity (curl) or divergence strength and the influence domain. Their method leads to a very complicated mathematic model and cannot be generalized to 3-D case easily. On the contrary, in our case, the divergence and curl, used as the only control parameters, are associated with each grid point. This leads to a very simple mathematical model and can be applied to 3D case easily.               Though the present work does not guarantee the regularity (no mesh folding) of the resulting deformation field, it is rarely occurred in the practical case. However, to keep the completeness of the proposed method, we set the parameters as the divergence and curl of the B-spline coefficients of the displacement field which further strengthen the regularization to be strong enough to warrant a folding free deformation field.   In the experiments, we illustrate the proposed method has feasible capability to handle various real medical images for both 2-D and 3-D. Also an experiment was devised to show the proposed method can overcome the aperture problem. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Regridding in Nonrigid Image Registration</strong><br /> Friday, November 21, 2008<br /> Ting-Hung Lin<br /><p><a href="#" id="459-show" class="showLink" onclick="showHide(459);return false;">Read More</a></p></p><div id="459" class="more"><p><a href="#" id="459-hide" class="hideLink" onclick="showHide(459);return false;">Hide</a></p><p><strong>Abstract: </strong>Regridding is ?rst introduced to solve the non-rigid image registration problems on large deformation applications. We investigate the usage of regridding on the following leading non-rigid image registration algorithms including elastic, fluid, di?usion, curvature and demons algorithms to compare the performance and accuracy. We also introduce a grid repairing mechanism based on the adaptive grid generation method to prevent the transformation from folding and the proposed a grid repairing method to set bounds on the Jacobian determinant of the transformation. We showed that our regridding and grid repairing method can outperform the original registration algorithms especially in large deformation applications. How can it improve the ability of the original registration algorithms on large deformation applications and how the grid repairing method can be embedded in the algorithms are also introduced in this dissertation. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>COMPUTATIONAL ANALYSIS OF STRUCTURE AND FUNCTION OF GENOMIC SEQUENCES</strong><br /> Monday, November 17, 2008<br /> Abanish Singh<br /><p><a href="#" id="455-show" class="showLink" onclick="showHide(455);return false;">Read More</a></p></p><div id="455" class="more"><p><a href="#" id="455-hide" class="hideLink" onclick="showHide(455);return false;">Hide</a></p><p><strong>Abstract: </strong>The genetic code consists of long chains of deoxyribonucleic acid (DNA) present in every cell of a living organism. These chains contain both functional and non-functional DNA sequences, and their proportion in the mix varies widely along the tree of life. Generally, more complex organisms tend to feature large amounts of “junk” DNA, whose importance is still subject of a debate in the scientific circles. The functional sequences include coding sequences (genes) and various types of signals, mostly, but not exclusively, controlling the regulation of coding sequences, i.e. activating and deactivating the expression of genes, during the developmental stage, in response to external stimuli, or during housekeeping activities in a cell or organism. Such expression leads to the production of various ribonucleic acids (RNAs), out of which the most common is messenger RNA (mRNA) which serves as a template for chains of amino acids, or polypeptides. The polypeptides themselves fold and group into proteins, providing structural components and functionalities to the living cells and tissues.  Regulatory signals in DNA tend to act as parts of complex networks, whose structure and dynamics have been subject to biomolecular studies for many decades. Recently, especially after sequencing of several major eukaryotic genomes has been completed, these studies have become increasingly computational. The applied techniques focus on sequence features, such as periodicity, motif over--representation,  phylogenetic conservation, sequence or structural homology, or the experimental data about binding effects,  patterns of gene co--expression, and, more recently, epigenetic information.  In particular, over the last several years, the search for functional elements in human and other genomes by exploiting motif over-representation became increasingly popular. Although there has been some success in this field, the existing tools are still neither sensitive nor specific enough, usually suffering from the detection of a large number of false positive signals. Given the properties of genomic sequences, some of which we analyze in this document, this is not unexpected, but one can still find interesting signals worthy of further computational and laboratory investigation.  In this thesis we present several algorithms for DNA sequence analysis, and in particular the identification and characterization of short motifs. We start with presenting an efficient algorithm to find all significant variable motifs shared within target sequences, generally taken from the upstream regions of co-expressed genes. Various filtering techniques have been applied to this problem in the past, but in our view it is important that we generate complete data, upon which separate selection criteria can be applied, depending on the nature of the sites one wants to locate. Though we primarily intended to develop software to locate the significant motifs based on their over-representation in the given DNA sequences, we also attempted to answer why such software often fails in locating the real elements. We have thus performed a study of the repetitive structure and distribution of short motifs in human genomic sequences. In most mammalian species about half of the genome consists of known or readily recognizable repeated elements, and we demonstrate that in addition to these repeats human genomic sequences feature many short motifs which are significantly over-represented, and that their frequency varies only slightly between random repeat--masked sequences and regions located immediately upstream of the known genes.  Recent studies have established the existence of evolutionary (and thus presumably functional) constraint on only about 5% of the human genome. If a half of it consists of known repeated sequences, which leaves us with the question about the source of the remaining 45%, for which we postulate that it should have mostly originated from ancient transpositional or other duplication activity. The original copies could have become so broken over time that they cannot be recognized as such any more, giving rise to seemingly unique sequences which nevertheless share large numbers of greatly over--represented short motifs. We have developed an algorithm, and written software which efficiently associates these motifs and reconstructs the consensus sequences of possible ancient broken repeats. We have found a significant number of new large repeated sequences, in addition to the previously characterized transposable elements and other duplications in the human genome, and we have built their consensus sequences and attempted to characterize them. We believe that in view of a recently proposed model postulating that transposable elements have been a significant source of transcriptional regulatory signals, further study of broken genomic repeats would be very useful.  The software implementing our methods have been made available in the public domain, and we h</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SUBCellular Structure Modeling and Tracking for Cell Dynamics Study</strong><br /> Wednesday, August 06, 2008<br /> Quan Wen<br /><p><a href="#" id="453-show" class="showLink" onclick="showHide(453);return false;">Read More</a></p></p><div id="453" class="more"><p><a href="#" id="453-hide" class="hideLink" onclick="showHide(453);return false;">Hide</a></p><p><strong>Abstract: </strong>The introduction of sensitive and fast electronic imaging devices and the development of biological methods to tag proteins of interest by green fluorescent proteins (GFP) have made a full understanding of live cell dynamics achievable. With the latest hardware technology, such as high speed laser scanning confocal microscopy (LSCM), it has now become critical to develop automatic quantitative data analysis tools to keep pace with and to fully exploit the functionalities of state-of-the-art hardware. One task of such tools is the motility analysis of multiple subcellular structures. <br /><br /> In this dissertation, we propose a Markovian state-space model to describe the tracking process and use the sequential Monte Carlo (SMC) filtering approach to approximate this nonlinear, non-Gaussian, state dimension changing dynamic system. The proposed SMC tracking framework is applied to multiple subcellular structure tracking in both 2D+T and 3D+T image sequences. <br /><br /> To detect and represent the multiple subcellular structures automatically during the tracking process, a morphological gray-scale reconstructive opening algorithm is applied first. Then the regional maxima are used to label each individual subcellular structure. The individual state  description is generated from the oriented bounding box of the marker which is the dilated version of the regional maxima. The variable number of tracked subcellular structures are finally represented by a joint state consisting of the state of individual subcellular structures. <br /><br /> Once the joint state is formed, reversible jump Markov chain Monte Carlo (RJMCMC) moves, including object appear move, disappear move, update move, height swap move, and identity swap move, are constructed to sample the distribution of the dimension varying joint state. The evolution of each individual state in the joint state is sampled by an update move. In order to deal with the random appearance locations of subcellular structures, marker residual image guided appearance model is proposed to detect the newly appearing object, and appear move and disappear move are applied to generate samples resulting from the new object appearance. The interaction between objects is modeled by augmenting object representation in the 2D plane using an extra dimension and evaluating the object overlapping relationship in the 3D space by a novel height swap move. To prevent the RJMCMC sampling from trapping at the local maxima of the joint state distribution and to accelerate the mixing of the samples, the identity swap move is also constructed. <br /><br /> Based on the tracking framework in 2D+T, a 3D+T version of SMC tracking approach is applied to the 3D+T multiple subcellular structure tracking, in which the image operation and representation are extended from 2D to 3D and the RJMCMC sampling method consists of four moves, namely update move, identity swap move, appear move, and disappear move. <br /><br /> Finally, an observation model is proposed by matching the size and intensity profile of the subcellular structures. <br /><br /> Experiments were performed on both synthetic and real confocal microscopy image sequences. Phenomena of multiple objects executing translational or Brownian motion with object appearing, disappearing, splitting, and merging are correctly detected and tracked,  which shows the feasibility and effectiveness of the proposed work.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Enhanced Bioinformatics Data Modeling Concepts And Their Use In Querying And Integration</strong><br /> Monday, August 04, 2008<br /> Feng Ji<br /><p><a href="#" id="452-show" class="showLink" onclick="showHide(452);return false;">Read More</a></p></p><div id="452" class="more"><p><a href="#" id="452-hide" class="hideLink" onclick="showHide(452);return false;">Hide</a></p><p><strong>Abstract: </strong>In bioinformatics research, scientists usually face the problems of modeling complex data types and integrating diverse resources. Traditional data models such as EER lack the expressing power to capture many characteristics that are common in bioinformatics data. We first propose extensions to the ER model that allow accurate representation of many of these characteristics. We then utilize these concepts in an integrative system to provide an easy-to-use interface for biologists to construct queries. Our research utilizes the enhanced conceptual modeling concepts to create a prototype mediator for querying multiple data sources. The various relationships between different biological entities are all semantically represented as domain ontologies stored in the mediator for experts to analyze and correlate the integrated query results. The following research has been conducted: (1) We first propose new EER schema notation to represent the common occurring biological concepts: the ordering properties of the DNA sequences, the 3D structure of proteins and the functional processes of metabolic pathways. (2) Then, we utilize these new relationships in the development of the mediated domain ontology, which helps the interface design and query processor implementation of our mediator system. <br /><br /> Our mediated schema features are based on a hybrid of taxonomy ontologies (core concepts and external classification/annotation concepts) for interpretation of raw data sets (protein and gene sequences) in the context of molecular interactions, biochemical pathways and biological processes. We adopt the RDF data model to implement the mediation data. Our mediator mainly takes a browsing-based approach to integrate different data sources. Extra data can be dynamically retrieved through the web service. By browsing the ontology tree in the query interface, users can select concepts of interest and associated attributes to formulate queries based on their domain knowledge. The query result is a set of various database entry accessions with associated attribute values. Users can click each link of the accessions to see the detailed reports, or cross-compare attributes of these data instances. Query usability and performance experiments are tested for real data sets from UniProt, ENZYME, CATH, and GO.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Congestion Control for Networks in Challenged Environments</strong><br /> Thursday, July 24, 2008<br /> Guohua Zhang<br /><p><a href="#" id="450-show" class="showLink" onclick="showHide(450);return false;">Read More</a></p></p><div id="450" class="more"><p><a href="#" id="450-hide" class="hideLink" onclick="showHide(450);return false;">Hide</a></p><p><strong>Abstract: </strong>Congestion occurs when resource demands exceed the capacity of a network. The goal of congestion control is to use the network as efficiently as possible. While extensive efforts have been devoted to providing optimization based, distributed congestion control schemes for efficient bandwidth utilization and fair allocation in the Internet and wireless networks, little consideration was given to congestion control for networks in challenged environments, specifically for networks with time-varying link capacities and networks that intermittently communicate. In this dissertation, we explore optimal congestion control strategies for such networks based on optimization techniques and repeated game theory. <br /><br /> For networks with time varying link capacities, we explicitly model link capacities to be time varying and investigate the corresponding optimal congestion control strategies. In particular we propose a primal-dual congestion control algorithm which is proved to be trajectory stable in the absence of feedback delay. Different from system stability around a single equilibrium point, trajectory stability guarantees the system is stable around a time varying reference trajectory. Moreover, we obtain sufficient conditions for the scheme to be locally stable in the presence of delay. The key technique is to model time variations of capacities as perturbations to a constant link. Furthermore, to study the robustness of the algorithm against capacity variations, we investigate the sensitivity of the control scheme and through simulations study the tradeoff between stability and sensitivity. <br /><br /> For a set of challenged networks that continuous end-to-end connectivity may not exist, network nodes may only communicate during opportunistic contacts (they are often referred as delay tolerant networks or opportunistic networks).  While custody transfer can provide certain reliability in delay these networks, a custodian node cannot discard the message unless its life time expires or the custody is transferred to another node after a commitment. This creates a challenging decision making problem at a node in determining whether to accept a custody transfer: on one hand, it is beneficial to accept a large number of messages as it can potentially advance the messages toward their ultimate destinations and network utilization can be maximized; on the other hand, if the receiving node over-commits itself by accepting too many messages, it may find itself setting aside an excessive amount of storage and thereby preventing itself from receiving further potentially important, high yield (in terms of network utilization) messages. To solve this congestion control problem, we apply the concepts of revenue management, and employ dynamic programming to develop congestion control strategies. For a class of network utility functions, we show that our solution is optimal. More importantly, our solution is distributed in nature where only the local information such as available buffer of a node is required. This is particularly important given the nature of delay tolerant networks where global information is often not available and the network is inherently dynamic. Our simulation results show that the proposed congestion management scheme is effective in avoiding congestion and balancing network load among the nodes. <br /><br /> In the above scheme, we have assumed that the time horizon is finite in making the decision of resource allocation.  However, in practice, in certain situations, it might be difficult or impossible to predict when the dynamic behavior will stop. This will render the proposed dynamic programming based control strategy not directly applicable.  As an alternative solution, we also employ repeated game theory to model the decision making for custody transfer and propose a new congestion control strategy. The repeated game based approach is particularly suitable for the situations where a node cannot be certain that when a contact will occur and when the dynamic behavior is going to stop. Our simulation results show that the control strategy based on repeated game theory is effective in avoiding congestion and balancing network load.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Context-Aware Learning, Prediction And Mediation Framework For Resource Management In Smart Pervasive Environments</strong><br /> Thursday, July 24, 2008<br /> Nirmalya Roy<br /><p><a href="#" id="451-show" class="showLink" onclick="showHide(451);return false;">Read More</a></p></p><div id="451" class="more"><p><a href="#" id="451-hide" class="hideLink" onclick="showHide(451);return false;">Hide</a></p><p><strong>Abstract: </strong>Advances in smart devices, mobile wireless communications, sensor networks, pervasive computing, machine learning, middleware and agent technologies, and human computer interfaces have made the dream of smart environments a reality. An important characteristic of such an intelligent, ubiquitous computing and communication paradigm lies in the autonomous and pro-active interaction of smart devices used for determining inhabitants' important contexts such as current and near-future locations, activities or vital signs. `Context Awareness' is perhaps the most salient feature of such an intelligent computing environment. An inhabitant's mobility and activities play a significant role in defining his contexts in and around the home. Although there exists optimal algorithm for location and activity tracking of a single inhabitant, the correlation and dependence between multiple inhabitants' contexts within the same environment make the location and activity tracking more challenging. In this thesis, first we propose a cooperative reinforcement learning algorithm for location-aware resource management in multi-inhabitant smart homes. This approach adapts to the uncertainty of multiple inhabitants' locations and most likely routes, by varying the learning rate parameters. Using the proposed cooperative game-theory based framework, all the inhabitants currently present in the house attempt to minimize this overall uncertainty in the form of utility functions associated with them. Joint optimization of the utility function corresponds to the convergence to Nash equilibrium and helps in accurate prediction of inhabitants' future locations and activities. Hypothesizing that every inhabitant wants to satisfy his own preferences about activities, next we look into the problem from the perspective of non-cooperative game theory where the inhabitants are the players and their activities are the strategies of the game. We prove that the optimal location prediction across multiple inhabitants in smart homes is an NP-hard problem and to capture the correlation and interactions between different inhabitants' movements (and hence activities), we develop a novel framework based on a non-cooperative game theoretic, Nash H-learning approach that attempts to minimize the joint location uncertainty of inhabitants. Our framework achieves a Nash equilibrium such that no inhabitant is given preference over others. This results in more accurate prediction of contexts and more adaptive control of automated devices, thus leading to a mobility-aware resource (say, energy) management scheme in multi-inhabitant smart homes. Experimental results demonstrate that the proposed framework is capable of adaptively controlling a smart environment, significantly reduces energy consumption and enhances the comfort of the inhabitants. To promote independent living and wellness management services in this smart home environment we envision sensor rich computing and networking environments that can capture various types of contexts of patients (or inhabitants of the environment), such as their location, activities and vital signs. However, in reality, both sensed and interpreted contexts may often be ambiguous, leading to fatal decisions if not properly handled. Thus, a significant challenge facing the development of realistic and deployable context-aware services for home based healthcare applications is the ability to deal with ambiguous contexts to prevent hazardous situations. In this thesis, we propose a quality assured context mediation framework, based on efficient context-aware data fusion and information theoretic system parameter selection for optimal state estimation in resource constrained sensor network. The proposed framework provides a systematic approach based on dynamic Bayesian network to derive context fragments and deal with context ambiguity or error in a probabilistic manner. It has the ability to incorporate context representation according to the applications' quality requirement. Experimental results demonstrate that the proposed framework is capable of choosing a set of sensors corresponding to the most economically efficient disambiguation action and successfully sensing, mediating and predicting the patients' context state and situation. Energy-efficient determination of an individual's context (both physiological and activity) is an important technical challenge for this assisted living environments. Given the expected availability of multiple sensors, context determination is viewed as an estimation problem over multiple sensor data streams. We develop a formal, and practically applicable, model to capture the tradeoff between the accuracy of context estimation and the communication overheads of sensing. In particular, we propose the use of tolerance ranges to reduce an individual sensor's reporting frequency, while ensuring acceptable accuracy of the derived context. We introduce an optimization technique allowing the cont</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Mutual Information Based Non-rigid Image Registration using Adaptive Grid Generation: GPU Implementation and Application to Breast MRI</strong><br /> Tuesday, July 22, 2008<br /> Mei Yi Chu<br /><p><a href="#" id="449-show" class="showLink" onclick="showHide(449);return false;">Read More</a></p></p><div id="449" class="more"><p><a href="#" id="449-hide" class="hideLink" onclick="showHide(449);return false;">Hide</a></p><p><strong>Abstract: </strong>In this dissertation a new approach for non-rigid image registration using mutual information is introduced. A fast parametric method for non-rigid registration is developed by adjusting divergence and curl of an intermediate vector field from which the deformation field is computed using finite-central difference method. The similarity measure mutual information is employed in the gradient-based cost minimization (or mutual information maximization) of the registration. The huge amount of data associated with MRI is handled by fully automated algorithm optimized with a multi-resolution topology preserving regridding scheme. The adaptive grid system naturally distributes more grids to deprived areas. The positive monitor function disallows grid folding and provides a mean to control the ratio of the areas between the original and transformed domain. The flexibility of the adaptive grid allocation could dramatically reduce processing time with quality preserved. Mutual information facilitates robust registration between different image modalities. Different types of joint histogram estimation are compared and integrated with the system. The whole system is also implemented on GPU which allows efficient parallel computation of vast amount of 3D data in SIMD manner during different procedures. The GPU implementation offers up to <i>221</i> times speed up in the gradient normalization routine and around 40 times speed up in the overall calculation. This scheme is applied on dynamic contrast-enhanced breast MRI, which requires the registration algorithm to be non-rigid, contrast-enhanced features preserving and within clinical visit time limit. Experiments show promising results and great potential for future extension.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Eukasimbiosys: A Stochastic Discrete Event-Based Simulation Software For  In-Silico  Study Of Insulin Signaling And Metabolism In Cardiac Myocytes</strong><br /> Friday, June 27, 2008<br /> Amin Reza Mazloom<br /><p><a href="#" id="441-show" class="showLink" onclick="showHide(441);return false;">Read More</a></p></p><div id="441" class="more"><p><a href="#" id="441-hide" class="hideLink" onclick="showHide(441);return false;">Hide</a></p><p><strong>Abstract: </strong>The advent of human genome annotation in the early years of third millennium has enabled the scientist to interlink the processes of life at the molecular level. The coincidence of this breakthrough along with advances in computational technology and high-throughput experimental techniques has promoted the emergence of numerous <i>-omics</i> data resources. Although for years before such discovery, scientist believed that cellular processes are the product of interaction between genes and gene products; However, any effort to exploit a comprehensive picture of cellular processes had been obscured due to the knowledge gap that avoided to correlate the cellular processes at the lowest level. Having the organisms blueprint in hand has encouraged many researchers to study a biological process as a part of a whole rather than in isolation. From an engineering point of view, the biologists interests now revolve around comprehending the system level behavior of a biological process in a complex biological network. Studying biological systems demands for modeling and simulation tools that can capture the dynamics of these systems in time and space. Many variant of these tools have been proposed elsewhere which all try to approximate the Chemical Master Equation (CME). These modeling and simulation tools are broadly classified into deterministic and stochastic based on their temporal evolutions. In the former class, the tools that project the biological system into a set of Ordinary Differential Equations (ODE) are the most prevalent. However, it has been shown elsewhere that these models can not capture the nonlinearity and the deviant effects that exist in the biological processes, due to the inherent random environment of the cell. In latter class, majority of tools comprise strains of Gillespie algorithm, where the system is mapped into sets of chemical kinetic equations which evolves in Monte Carlo steps. The main problem that deteriorates the utilization of these simulation tools is their temporal complexity. A common drawback for both Gillespie and ODE based approaches is their oversimplification in abstracting the physiology of a process that is represented by an equation along with a single kinetic rate constant. In this dissertation we first elucidate how the Stochastic Discrete Event Simulation (SDES) could be applied in capturing the behavior of biological processes as a set of biological events (bioevents) with random <i>holding times</i>. Then we introduce the architecture of <i>?eukaSimBioSys?</i>  which is designed for system-wide simulation of a eukaryotic cell. The model repository is one of the essential components of our proposed architecture, which comprises reusable modules of parametric models. Each of these parametric models once coupled with a proper parameter set is then applied to capture the <i>holding time</i> of a specific bioevent. These models are physicochemical models that attempt to abstract bimolecular interactions (i.e. modifications, associations, translocation, localizations, etc.) into a parametric probability distribution function of time. Typical interactions include: reaction, receptor-ligand biding, protein-protein binding, chromatin remodeling, transcription, translation, splicing, etc. The previous researchers have already started building this model library and in this work we add four new models (i) ligand-receptor binding, (ii) DNA fluctuations, (ii) chromatin remodeling, and (iii) splicing. For the first one we have developed both the eukaryotic and prokaryotic variants of the model, where as the rest are specific to eukaryotes. These models have been validated with the published experimental data where empirical results were available. Cell activity is the product of an intricate interaction among three main cellular networks: Signal Transduction Network (STN), Transcription Regulatory Network (TRN), and Metabolic Network (MTN). Each cellular function composed of one or more edges within or across these networks. Hence, system-wide study of a cell requires clear and explicit definition of these networks. We have incorporated the semantic of these networks in <i>?eukaSimBioSys?</i>  by designing an object oriented database to hold the layout of these three networks along with their inter-relationships. We have populated these databases for <i>?human Bcell?</i> and <i>?human cardiac myocyte'</i> from data available in literature and other databases. Despite the advances in health science, and discovery of new drugs, still heart disease is the most life threatening disease in both industrial and developing countries. Cardiac myocytes are the main players of the heart contraction function and are among the most energy consuming tissues in the body. Any changes in their normal metabolism can lead to severe consequences for an individual. Glucose and fatty acids comprise the major sources of energy for the myocardial cells, the interplay between these two sources is predo</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Mass Spectrometry based Proteomic Biomarker Selection and Sample Prediction</strong><br /> Tuesday, June 17, 2008<br /> Jung Hun Oh<br /><p><a href="#" id="443-show" class="showLink" onclick="showHide(443);return false;">Read More</a></p></p><div id="443" class="more"><p><a href="#" id="443-hide" class="hideLink" onclick="showHide(443);return false;">Hide</a></p><p><strong>Abstract: </strong>High-resolution MALDI-TOF (matrix-assisted laser desorption/ionization) mass spectrometry is capable of collecting data over a broad mass range (100 to < 300,000 Dalton) in a single acquisition and has less measurement error (mass shift). Therefore, high-resolution MALDI-TOF mass spectrometry has increasingly been used to early disease diagnosis, monitoring disease progression and therapeutic effects of drugs. To discover and identify unique biomarker patterns hidden in the complex and high-dimensional mass spectra, robust computation algorithms have to be developed. For high-resolution mass spectrometry (MS) data, traditional machine learning algorithm may break down with high-dimensional input. Very limited research has been carried out to explore the computational challenges. <br /><br /> Disease progresses in several stages. Therefore, there exist biomarkers corresponding to each stage. To deal with such a multi-class problem, we propose a classification and a feature selection method. The proposed classification method consists of two schemes: error-correcting output coding (ECOC) and pairwise coupling (PWC). In prediction for a test sample, aggregated results of both schemes are considered. In PWC scheme, important features for each pair of classes are found by using extended Markov blanket (EMB) feature selection. <br /><br /> We propose a new wrapper-based feature selection algorithm called extended Markov blanket (EMB). Original Markov blanket is a filter-based feature selection method. Our algorithm considers reducing redundant features while selecting the most discriminant ones. In original Markov blanket, to rank all features Markov blanket iterations should be performed until one feature remains. In contrast, extended Markov blanket can obtain the feature ranking without running Markov blanket to the end where the last feature remains,. To be more specific, for a certain feature, two feature subsets are considered: the high correlated feature subset (HCFS) and the low correlated feature subset (LCFS). HCFS feature subset is used to remove redundant features as in the classical Markov blanket feature selection. Our contribution comes from utilizing LCFS to estimate the classification capability of each feature during the Markov blanket process. This is derived from the fact that mutually low correlated features, in general, lead to good classification performance. <br /><br /> To identify the molecular formulae of the biomarkers, we develop a two-way parallel searching method which applies a tandem mass spectrometry (MS/MS) technique.  In our approach, the positions of start and end nodes for b-ion and y-ion are determined in advance in the MS/MS spectrum. During our two-way parallel searching, these four initial nodes extend simultaneously by scanning the whole spectrum, where start nodes for b- and y-ion series proceed simultaneously in the forward direction, and end nodes in the backward direction at the same time. This procedure keeps going until some requirements are met. <br /><br /> LDA is a traditional statistical scheme for feature reduction which has been widely used in a diversity of application areas. In a case where the dimensionality exceeds the sample size, however, the classical LDA faces a problem known as singularity. Since the dimensionality of the mass spectrometry data is considerably huge, the singularity problem necessarily happens. Another drawback of the classical LDA is its linear property with which LDA fails for nonlinear problems. To solve the problem, nonlinear based LDA methods were proposed. However, they suffer from high cost in running. We propose a new fast kernel discriminant analysis (FKDA). Our algorithms were applied to several mass spectrometry datasets for biomarker selection, multi-class classification, and protein identification.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Comprehensive Data Analysis for Biomarker Pattern Discovery using DNA/Protein Microarrays</strong><br /> Tuesday, June 17, 2008<br /> Young Bun Kim<br /><p><a href="#" id="444-show" class="showLink" onclick="showHide(444);return false;">Read More</a></p></p><div id="444" class="more"><p><a href="#" id="444-hide" class="hideLink" onclick="showHide(444);return false;">Hide</a></p><p><strong>Abstract: </strong>During the last decade, the advent of microarray technology has stimulated rapid research advances in bioinformatics. Microarray data pose great challenges for computational data analysis, because of their large dimensionality (up to several tens of thousands of genes) and their small sample sizes. In order to deal with these particular characteristics of microarray data, the need and importance for feature selection techniques were realized. While a lot of research deals with classification methods and their application to microarray data, only a few approaches are explicitly designed to consider interaction among the investigated features. It is well known that the interactions between genes or proteins are important for many biological functions, i.e. signals from the outside of a cell are mediated to the core of the cell by protein-protein interactions of the signaling molecules. Hence, to achieve optimal classification accuracy, these interactions among features need to be taken into account. My research goal is to develop algorithms which not only effectively select the most informative features but also identify the relationship among those features. <br /><br /> For the clustering of the genes, researchers have attempted to apply feature subset selection to select a subset of genes that are common for all possible un-known classes. However, the fact that a certain set of genes may be only related to a subset of experiments due to experiment design and no enough knowledge on gene function is overlooked. In the thesis,  a new subspace semi-supervised clustering algorithm called EPSCMIX (Emerging Pattern Subspace Clustering by MIXure models) is designed. This algorithm is used to find gene expression patterns which in turn could be used to predict pathological phenotypes and identify genes that might anticipate the clinical behavior of diseases. Our method is based on feature saliency measure, the probability of feature relevance, which is estimated by an Expectation Maximization (EM) algorithm. This approach employs Emerging Patterns (EPs) to identify effectively relationships among genes. The best number of classes and the relevant set of genes are discovered by EPSCMIX. <br /><br /> To address the problem of identifying informative genes from a large amount of gene expression data when no prior knowledge is available, we develop a hybrid methodology for unsupervised gene (feature) selection and sample clustering. The algorithm, PFSBEM (hybrid PCA based Feature Selection and Boost-Expectation-Maximization clustering), introduces a new PCA (principal component analysis) based feature selection within a wrapper framework. PFSBEM uses a three-step approach to feature selection and data clustering. The first step initially reduces high-dimension feature space by retrieving feature subsets with original physical meaning based on their capacities to reproduce sample projections on PCs (principal components). Each feature subset corresponds to a certain PC. The second step then determines the important PCs that contribute to data clustering. A boost-EM (expectation-maximization) clustering method is developed to achieve stable data grouping. Finally, from the merged feature subsets of important PCs, the best feature subset that maximizes data clustering is selected. <br /><br /> Feature pattern (combination of features) identification techniques could be used to capture more underlying semantics than single feature. However, it is very hard to find meaningful patterns in large datasets like microarray data because of the huge search space. Furthermore, infrequent patterns are often irrelevant or do not improve the accuracy of the classification. To tackle these problems, we finally design a discriminative feature patterns identification system named DFPIS. Instead of simply identifying genes contributing to the network, this methodology takes into consideration of gene interactions which are represented as Strong Jumping Emerging Patterns (SJEP). Furthermore, infrequent patterns though occurred are considered irrelevant. The whole framework consists of three steps: feature (gene, protein) selection, feature pattern identification, and pattern annotation.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Architectures and Methods for Energy-Efficient Querying and Indexing in Wireless Sensor Networks</strong><br /> Monday, June 16, 2008<br /> Kyungseo Park<br /><p><a href="#" id="442-show" class="showLink" onclick="showHide(442);return false;">Read More</a></p></p><div id="442" class="more"><p><a href="#" id="442-hide" class="hideLink" onclick="showHide(442);return false;">Hide</a></p><p><strong>Abstract: </strong>Wireless sensor networks have been an active research area for about a decade due to their adaptability to applications involving long-term environmental monitoring. Recent technologies make it possible that a small device equipped with multi-purposed sensors, small CPU and memory, wireless transmitter and receiver, and software can form a network, measure some quantitative phenomena and communicate with each other seamlessly. In wireless sensor networks, one of the basic applications is to monitor events and measure values at places that people cannot reach easily or where a long term sensing task is required. In this case, we need to know the properties of diverse types of sensor network queries and data that are different from traditional ones. Also, we need to solve the intrinsic sensor network problem, energy saving, since users want to gather data for a long term and it is generally not feasible to replace batteries in sensor devices after the sensors are deployed.<br /><br />    In order to solve this problem, first, we classify sensor network queries and find a suitable network system that includes different routing and storage types for each type of query. Second, energy efficient indexing and data gathering methods for sensor networks are studied. In energy efficient indexing, we propose a sectioned tree index, which divides the network area into several squares and each square has a local index subtree organized within that square. Local trees are interconnected to form one big tree in the network. Local trees are also built based on any algorithm that is energy consumption aware at each sub-root node in a locally centralized way. In energy efficient data gathering, we create energy-efficient data gathering routing tree when we consider two-way communication for reliable transmission and collision prevention. This makes the problem intractable, and we prove our problem is NP-complete by showing that a known NP-complete problem is a special case of our problem. In order to get an energy-efficient routing tree, we propose several heuristic techniques that backtrack one or two steps (BT1 and BT2). We give various values as parameters in measuring energy equation and simulate our proposed algorithms in diverse conditions.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A MULTI-LEVEL BIOMEDICAL ONTOLOGY-ENABLED BROKER: DYNAMIC SERVICE-BASED DATA SOURCE INTEGRATION</strong><br /> Friday, May 02, 2008<br /> Sheng-Chieh Jack Fu<br /><p><a href="#" id="437-show" class="showLink" onclick="showHide(437);return false;">Read More</a></p></p><div id="437" class="more"><p><a href="#" id="437-hide" class="hideLink" onclick="showHide(437);return false;">Hide</a></p><p><strong>Abstract: </strong>Web services have recently become a new trend for gathering biomedical information. However, it is not easy to integrate and obtain a concise/complete query result among thousands of service operations. In this dissertation, we propose a multi-level ontology-enabled service broker system for dynamically integrating web services in the biomedical domain (BioServiceBroker). By introducing multi-level modeling concepts and intra/inter level relationships, our approach facilitates more accurate modeling of biomedical ontologies, which leads to a better understanding of the data stored in various biological data sources as well as the services provided by the data sources. In addition, we incorporate temporal concepts with new enhanced QoS measures, which allow service requesters to control more querying factors in order to precisely invoke corresponding service operations. We also define a Unfined Biomedical Service Interface (UBSI) as a proposed service deployment standard.   The multi-level ontology-enabled service broker system can be combined with other mediator systems for cross referencing the diverse bioinformatics sources and can also be utilized in other application domains. Our ultimate goal is to construct a public, scalable, and interoperable biomedical service platform based on UBSI to benefits scientists in data searching and publishing.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Energy-Efficient Connected k-Coverage, Duty-Cycling, and Geographic Forwarding in Wireless Sensor Networks</strong><br /> Monday, April 21, 2008<br /> Habib Ammari<br /><p><a href="#" id="436-show" class="showLink" onclick="showHide(436);return false;">Read More</a></p></p><div id="436" class="more"><p><a href="#" id="436-hide" class="hideLink" onclick="showHide(436);return false;">Hide</a></p><p><strong>Abstract: </strong>With the fast advances in inexpensive sensor technology and wireless communications, the design and development of large-scale wireless sensor networks has become cost-effective and viable enough to attract the attention of a wide range of civilian, natural, and military applications, such as health and environmental monitoring, seism monitoring, and battlefields surveillance. The main challenge in the design of wireless sensor networks is the limited battery power of the sensors and the difficulty of replacing and/or recharging these batteries due to the nature of the monitored field, such as hostile environments, and cost. Thus, it is necessary that the sensors be densely deployed and energy-efficient protocols be designed in order to maximize the network lifetime while meeting the specific application requirements in terms of coverage and connectivity. In this dissertation, we propose a continuum percolation-based approach to compute the critical sensor spatial density above which a field is almost surely covered and the network is almost surely connected. This approach helps network designers achieve full coverage of a field with a minimum number of connected sensors, thus maximizing the network lifetime. In order to support different applications and environments with diverse requirements in terms of coverage and connectivity, we extend our above analysis to k-coverage with k ? 3 using a deterministic approach so the network self-configures to meet these requirements. More specifically, we propose a unified, energy-efficient framework for connected k-coverage, duty-cycling, and geographic forwarding in wireless sensor networks. Our framework, called Cover-Sense-Inform (CSI), includes centralized, pseudo-distributed, and distributed protocols for connected k-coverage along with geographic forwarding protocols in duty-cycled, k-covered wireless sensor networks. We prove that k-covered wireless sensor networks have connectivity that is higher than their degree of coverage k, thus providing architectures with high degree of fault-tolerance. In the first protocol, called centralized randomized connected k-coverage, the sink is responsible for selecting a minimum number of active sensors to fully k-cover a field while maintaining connectivity between all active sensors. Each of the second and third protocols, called Reuleaux triangle-based clustered randomized connected k-coverage and disk-based clustered randomized connected k-coverage, is run under the control of a subset of sensors, called cluster-heads, which are selected by the sink in each scheduling round. Each cluster-head is responsible for selecting a subset of neighboring sensors to k-cover its cluster while guaranteeing connectivity between all active sensors. Both protocols consider different levels of network clustering. In the fourth protocol, called distributed randomized connected k-coverage, all the sensors are required to coordinate among themselves to fully k-cover a field while ensuring network connectivity. Simulation results show that our protocols select a minimum number of sensors, thus maximizing energy saving. Using a potential fields-based approach, we propose three geographic forwarding protocols for duty-cycled, k-covered wireless sensor networks with different levels of data aggregation. Simulation results show that CSI yields significant energy savings and guarantees high data delivery ratio. Besides, we extend CSI to address the problem of stochastic connected k-coverage in wireless sensor networks using a more realistic, stochastic sensing model that accounts for the properties of the sensors, which are irregular in nature. We also extend CSI to three-dimensional wireless sensor networks. We find that the extension of our analysis from two-dimensional to three-dimensional space is not straightforward due to the inherent properties of the Reuleaux tetrahedron. Finally, we propose a solution to the energy sink-hole problem, which is inherent to static wireless sensor networks, by exploiting energy heterogeneity, sink mobility, and energy aware Voronoi diagram. Simulation results show that our solution yields uniform energy consumption of all the sensors, thus extending the network lifetime.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Secure Data Aggregation in Wireless Sensor Networks</strong><br /> Thursday, April 10, 2008<br /> Wei Zhang<br /><p><a href="#" id="432-show" class="showLink" onclick="showHide(432);return false;">Read More</a></p></p><div id="432" class="more"><p><a href="#" id="432-hide" class="hideLink" onclick="showHide(432);return false;">Hide</a></p><p><strong>Abstract: </strong>Recent advances in micro-electro-mechanical systems (MEMS) technology and wireless communications technologies have enabled to deploy wireless sensor   networks (WSNs) in a plethora of applications, ranging widely from military surveillance to civilian applications. To protect the networks from different kinds of attacks, security in wireless sensor networks plays a crucial role and has received increased attention in the applications, especially those deployed in hostile environment, such as battlefield monitoring and home security. As the goal of a sensor network is to gather sensory data from the deployed sensor nodes, in-network processing, or aggregation, is often adopted for energy efficiency. How to guarantee the security of aggregation is an intriguing challenge. In this dissertation, we propose a novel framework for secure data aggregation in WSNs, which includes two approaches i) a watermark based approach for the aggregation supportive authentication and ii) a trust model based approach for securing data aggregation. We first propose an end-to-end authentication scheme based on digital watermarking. The key idea is to visualize the sensory data gathered from the whole network at a certain time snapshot as an image, in which, every sensor node is viewed as a pixel with its sensory reading representing the pixel's intensity. Under this mapping, the authentication information is modulated as watermark and superposed to the sensory data at the sensor nodes. The watermarked data then can be aggregated by the intermediate nodes without any enroute checking. Upon reception of the sensory data, the data sink is able to authenticate the data by validating the watermark. The watermark based scheme is further extended to esitmate the sensory data quality. The second approach aims to secure data aggregation and quantify the uncertainty in the aggregate results in the presence of compromised nodes (insider attacks). Instead of solely relying on cryptographic techniques, our proposed scheme solves the problem by utilizing multiple and yet closely coupled techniques to secure data aggregation against false data injection. Specifically, by examining every sensory data against each other, the redundancy in the gathered information is exploited to evaluate the trustworthiness of each individual sensor node. This trustworthiness is quantified as each node's reputation and serves as an input of a classification algorithm with the goal to detect any compromised nodes. Moreover, every aggregate result is associated with an opinion to represent the degree of belief, a measure of uncertainty, in the aggregate result. As multiple results and their corresponding opinions are disseminated and assembled through the routes to the sink, these opinions will be consolidated and propagated based on Josang's belief model so that the uncertainty inherent in the sensory data and aggregate results in the whole WSN can be reasoned.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Data Dissemination Protocols in Wireless Sensor Networks : Design, Modeling and Security</strong><br /> Wednesday, April 09, 2008<br /> Pradip De<br /><p><a href="#" id="431-show" class="showLink" onclick="showHide(431);return false;">Read More</a></p></p><div id="431" class="more"><p><a href="#" id="431-hide" class="hideLink" onclick="showHide(431);return false;">Hide</a></p><p><strong>Abstract: </strong>Wireless sensor and actuator networks have been one of the stepping stones towards the realization of a pervasive computing infrastructure. Incidentally, post-deployment updates and reconfigurations, on a network-wide scale, of these devices which are embedded in our environment is a non-trivial proposition. Wireless over-the-air techniques provide the only resort, and subsequently, the multihop broadcast based communication paradigm is of paramount importance in such an environment. Subsequently, the importance of an in-depth study and design of efficient and reliable data dissemination protocols for reprogramming a network of embedded sensor devices is underscored.  In this dissertation, we initially focus on the formal modeling and analysis of data dissemination in a wireless sensor network under a broadcast based multihop paradigm. The data propagating in the network could be either regular information to be shared by all the nodes, a malware propagating across the network from a compromised node or a large code image required for retasking or reprogramming the entire network. In this analysis, we construct a mathematical model that allows us to compare the performances of different dissemination protocols in terms of their speed of propagation and extent of network reachability.  Next, from a perspective of security, we investigate the potentially disastrous threat of node compromise originating from a single infected node and propagating to other nodes via communication amidst pre-established mutual trust in a securely communicating sensor network. Focussing on the possible epidemic breakout of such a propagation, we model and analyze this spreading process and identify key factors determining potential outbreaks. More importantly, we compare the propagation process based on different sensor deployment strategies, for instance uniform and group-based deployment. Subsequently, we delve onto a specific case of a malware spreading over existing data dissemination protocols in sensor networks. In order to better understand the vulnerability of such protocols to piggybacked virus attacks, we, again, construct a mathematical model for the propagation, incorporating important parameters derived from the communication pattern of the protocol under test. We further enrich our study by also observing the effects of a simultaneous recovery process on the malware propagation. The overall result is an approximate but convenient and flexible tool to characterize a broadcast protocol in terms of its vulnerability to malware propagation.   Having focused on analyzing data dissemination techniques in static sensor networks, we, observe that existing data dissemination protocols are inefficient in a mobile environment and require effective modifications to suit the uncertainties and demands of network mobility. Thus, we propose a novel over-the-air multihop data dissemination protocol, suitable to a mobile sensor network and evaluate it through extensive simulations as well as real testbed implementation on a network of SunSPOT devices.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Stochastic models for In-Silico Event-based Biological Network Simulation</strong><br /> Monday, November 26, 2007<br /> Preetam Ghosh<br /><p><a href="#" id="415-show" class="showLink" onclick="showHide(415);return false;">Read More</a></p></p><div id="415" class="more"><p><a href="#" id="415-hide" class="hideLink" onclick="showHide(415);return false;">Hide</a></p><p><strong>Abstract: </strong>The multi-scale biological system model is a new research direction to capture the dynamic measurements of complex biological systems. The current statistical thermodynamic models can not scale to this challenge due to the explosion of state-spaces of the system, where a biological organ may have billions of cells, each with millions of molecule types and each type may have a few million molecules. We seek to propose a phenomenological theory that will require a smaller number of state variables to address this multi-scaling problem. Discrete Markov statistical process is used to understand the system dynamics in the networking community for a long time. In this dissertation, we focus more specifically on a composite system by combining the state variables in the time-space domain as events, and determine the immediate dynamics between the events by using statistical analysis or simulation methods. In our approach the space-time behavior of the cell dynamics is captured by discrete state variables, where an event is a combined process of a large number of state transitions between a set of state variables. The execution time of these state transitions to manifest the event outcome is a random variable called event-holding time. The underlying assumption is that it will be possible to segregate the complete system state-space into a disjoint set of independent events and events can be executed simultaneously without any interaction once the execution conditions are satisfied (removal of resource bottleneck, collision).  In this dissertation, we present the event-time models for some biological functions that will be incorporated in the discrete-event based stochastic simulator. In particular, we present analytical models for the molecular transport event in cells considering charged/non-charged macromolecules. We show, that molecular transport event completion time can be approximated by an exponential distribution. Next we present stochastic models for biochemical reactions in the cell (that can be extended to reactions occurring in the cell cytoplasm, membrane or nucleus). We show that the reaction completion time follows an exponential distribution when one of the reactant molecules enter the cell one at a time, whereas, it follows a gamma distribution when a batch of the reactant molecules enter the cell. We also present stochastic models for the protein-DNA binding and protein-ligand docking events and show that both these events have an exponentially distributed event completion time. We also validate each of the models presented in the dissertation with experimental findings reported in the literature. Finally, we present a markov chain based stochastic biochemical system simulator which can give us the dynamics of more complex events and can be used to improve the scalability of the discrete-event based stochastic simulator. We propose to successfully demonstrate this technique by modeling the complete dynamics of one Salmonella cell.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A DISCRETE EVENT BASED STOCHASTIC SIMULATION APPROACH FOR STUDYING THE DYNAMICS OF BIOLOGICAL NETWORKS</strong><br /> Monday, November 19, 2007<br /> Samik Ghosh<br /><p><a href="#" id="418-show" class="showLink" onclick="showHide(418);return false;">Read More</a></p></p><div id="418" class="more"><p><a href="#" id="418-hide" class="hideLink" onclick="showHide(418);return false;">Hide</a></p><p><strong>Abstract: </strong>With increasing availability of data resources on the molecular parts of a living cell, biologists are focusing on holistic understanding of cellular mechanisms and the emergent dynamics arising out of their complex interactions. Comprehending the fine-grained signal specificity, gene regulation and feedback mechanisms of molecular interactions at a network level forms a central theme of systems biology. With the speed and sophistication of computational methods, in silico modeling and simulation techniques have become a powerful tool for biologists challenged with understanding the system complexity of biological networks. Numerical simulation of classical chemical kinetics (CCK), agent-based simulations of biological processes, and linear optimization models of metabolic networks, have been applied to the study of cellular behaviors with varying degrees of success. The spatio-temporal scales of cellular processes, coupled with the knowledge gap and complexity of biological networks limit the application of existing computational techniques.  In this thesis, we present a network-centric modeling and simulation approach to systematically study the stochastic dynamics of cellular processes at a molecular level. The central theme of our approach revolves around abstracting a complex biological process as a collection of discrete, interacting molecular entities driven in time by a set of discrete molecular events (bioEvents). We develop the discrete-event based simulation engine, called iSimBioSys, together with an integrated database of biological pathways, which captures the temporal dynamics of the molecular entities through stochastic interactions of different bioEvents.  With an illustrative case study of signal transduction networks in bacterial cells, we highlight the efficiency of a discrete event based approach in capturing high-level system dynamics of a biological process, particularly in reproducing the switching effect of the PhoPQ pathway in Salmonella cells as reported in experimental work. Next, we build a detailed stochastic model for the fundamental process of gene expression in prokaryotic cells and study the molecular events of transcription and translation using the proposed simulation framework. Our results identify the role of transcriptional and translation machinery in controlling the burstiness of protein generation. We extend our simulator to incorporate a hybrid algorithm which combines stochastic models of signaling and regulatory events with a flow-based model for metabolic networks. In order to validate the efficacy of the hybrid simulation approach, we develop an integrated database of signaling and metabolic networks in the bacterial cell Escherichia Coli. The hybrid simulation recreates experimentally observed regulation of metabolic flux distributions in the network while providing new insights into the mechanism of regulation.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Learning Video Preferences Using Visual Features and Closed Captions</strong><br /> Tuesday, November 13, 2007<br /> Darin Brezeale<br /><p><a href="#" id="417-show" class="showLink" onclick="showHide(417);return false;">Read More</a></p></p><div id="417" class="more"><p><a href="#" id="417-hide" class="hideLink" onclick="showHide(417);return false;">Hide</a></p><p><strong>Abstract: </strong>Viewers of video now have more choices than ever.  As the number of choices increases, the task of searching through these choices to locate video of interest is becoming more difficult. Current methods for learning a viewer&#039s preferences in order to automate the search process rely either on video having content descriptions or on having been rated by other viewers identified as being similar.  However, much video exists that does not meet these requirements.  To address this need, we use hidden Markov models to learn the preferences of a viewer by combining visual features and closed captions.  We validate our approach by testing the learned models on a data set composed of features drawn from movies and user ratings obtained from publicly available data sets. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Game Theoretical Data Replication Techniques for Large-scale Autonomous Distributed Computing Systems</strong><br /> Friday, July 20, 2007<br /> Samee U. Khan<br /><p><a href="#" id="400-show" class="showLink" onclick="showHide(400);return false;">Read More</a></p></p><div id="400" class="more"><p><a href="#" id="400-hide" class="hideLink" onclick="showHide(400);return false;">Hide</a></p><p><strong>Abstract: </strong>Data replication in geographically dispersed servers is an essential technique for reducing the user perceived access time in large–scale distributed computing systems. A majority of the conventional replica placement techniques lack scalability and solution quality. To counteract such issues, this thesis proposes a game theoretical replica placement framework, in which autonomous agents compete for the allocation or reallocation of replicas onto their representative servers in a self–managed fashion. Naturally, each agent’s goal is to maximize its own benefit. However, the framework is designed to suppress individualism and to ensure system–wide optimization. Using this framework as an environment, several cooperative and non–cooperative low–complexity, flexible, and scalable game theoretical replica placement techniques are proposed, analytically investigated, and experimentally evaluated. Each of these techniques supports different game theoretical (pareto–optimality, catering to agents’ interests, deliberate discrimination of allocation, budget balanced, pure Nash equilibrium, and Nash equilibrium) and system (link distance, congestion control, minimization of communication cost, and memory optimization) related properties. Using a detailed test–bed involving eighty various network topologies and two real–world access logs, each game theoretical technique is also extensively compared with conventional replica placement techniques, such as, greedy heuristics, branch–and–bound techniques and genetic algorithms. The experimental study confirms that in each case the proposed techniques outperform other conventional methods. The results can be summarized in four ways: 1) The number of replicas in a system self–adjusts to reflect the ratio of the number of reads versus writes access; 2) Performance is improved by replicating objects to the servers based on the locality of reference; 3) Replica allocations are made in a fast algorithmic turn–around time; 4) The complexity of the data replication problem is decreased by multifold.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>ENHANCED TARGETING IN A HAPTIC USER INTERFACE FOR THE PHYSICALLY DISABLED USING A FORCE FEEDBACK MOUSE</strong><br /> Friday, July 20, 2007<br /> Brian Holbert<br /><p><a href="#" id="413-show" class="showLink" onclick="showHide(413);return false;">Read More</a></p></p><div id="413" class="more"><p><a href="#" id="413-hide" class="hideLink" onclick="showHide(413);return false;">Hide</a></p><p><strong>Abstract: </strong>Although the human computer interface continues to evolve by engaging sight, voice, sound, and touch to manipulate the environment, the marriage between mouse and graphic based operating systems remains one of the primary relationships through which we interact with the computer.  The mouse/GUI union has remained undisturbed for nearly 30 years. However, with the advent of haptics it has become possible to enhance that relationship with the sense of touch opening new avenues of interaction, in particular for those with disabilities. The word haptics refers to the provision of tactile queues in a mechanical environment.  Many haptic devices exist for use with a computer in a variety of environments and have seen success in many special purpose areas such as remote surgical procedures, three dimensional protein visualization, and video games. However, while there has been a realization that haptics can improve the human computer interface for specific tasks, little has been done with haptics to benefit general use of a computer in everyday interaction.  Since the majority of these haptic devices are cost prohibitive our options are limited to only a few possibilities, one of which is the Wingman? Force Feedback Mouse from Logitech.  The Wingman? is a full force feedback haptic device capable of operating in two dimensions and will be device used throughout this research. For the majority of users the mouse is an effective and proven device for human computer interaction; however it is not as well suited for specific groups of individuals, particularly those with physical disabilities.  The dissolution of the mouse/GUI union often leads those with disabilities to search for an alternative input device.  The haptic mouse can exist as one of those alternatives if an effective interface can be designed that compensates for the disability of the user. Through the uses of haptic effects, movement profiles, and a prediction algorithm an environment has been constructed capable of improving targeting in both experimental and realistic situations for a group of users with physical disabilities. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Framework for Improving the Performance of Application Servers in Next Generation Networks (NGNs)</strong><br /> Wednesday, July 18, 2007<br /> Sumantra Raj Kundu<br /><p><a href="#" id="405-show" class="showLink" onclick="showHide(405);return false;">Read More</a></p></p><div id="405" class="more"><p><a href="#" id="405-hide" class="hideLink" onclick="showHide(405);return false;">Hide</a></p><p><strong>Abstract: </strong>Next generation networks (NGNs) such as IP Multimedia Subsystem (IMS) are completely built on the Internet Protocol (IP) suite. This has made IP the de facto standard for data networking, voice over IP (VoIP), and media rich applications such as streaming multimedia, ringtones, multi-player gaming, and high-definition video conferencing for remote interaction. A primary feature of such converged networks is that they use the same IP-based network for simultaneously delivering voice, video, and data. Such services are provided on application servers built using industry standard Advanced Telecom Computing Architecture (ATCA) based blade computing units with various flavors of commodity open source operating systems like Linux, xBSD, and OpenSolaris.      However, real-time and latency sensitive applications such as streaming multimedia require that the entire network path of packet delivery from the originating server to the end host be properly and appropriately configured so as to avoid unnecessary delay and jitter in the data transfer mechanisms. With the ease of deployment comes the challenge of delivering such rich multimedia applications in NGNs since there exists no separate paths for voice and data as present in existing circuit--switched public switched telephone network (PSTN). Packet delivery in such converged architectures involves interaction between the storage disks, operating system (OS), network interface cards (NICs), and the various switches and routers - each of which is independently capable of introducting delay in the data transfer mechanism.      In this talk, we focus on understanding and improving the performance of application servers present in high traffic content delivery networks (CDNs) and hoisting latency sensitive applications with heavy I/O requirements. It takes into consideration the closed form interaction between the host operating system, network interface cards, and the underlying network traffic for suggesting mechanisms that aim at improving the performance of such servers. We believe that our framework and approach of identifying the basic components in network data transfer mechanisms are for most part generic and can be used for performance tuning and deploying application servers in NGNs with a variety of different services. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Integrated Approach for Enhancing User Experience in Wireless Multimedia Systems</strong><br /> Tuesday, July 17, 2007<br /> Saurav Pal<br /><p><a href="#" id="406-show" class="showLink" onclick="showHide(406);return false;">Read More</a></p></p><div id="406" class="more"><p><a href="#" id="406-hide" class="hideLink" onclick="showHide(406);return false;">Hide</a></p><p><strong>Abstract: </strong>Technological advances in multi-rate wireless systems have made wireless data services an intrinsic part of human life. An abundance of wireless devices, both in the wireless enabled home multimedia systems as well as in corporate offices, have triggered an array of research in enhancing the wireless data services. The success and acceptance of these various rich data services depends on satisfying the user experience derived from such services.  In this thesis, we first focus on identifying and exposing the important parameters, specifically network parameters pertaining to the multi-rate wireless systems, which hinder user satisfaction for a wireless multimedia system. Subsequently, we provide an integrated framework which encapsulates channel estimation techniques, scheduling algorithms specifically catering to the issues in multi-rate wireless systems and mobility solutions for improving user experience. We start off by performing theoretical modeling of how user satisfaction for the various data services varies with different network impediments.  The insights reveal that the traditional metrics throughput, connectivity and delay constraints hold very much for the upcoming multi-rate wireless multimedia systems. However, the intelligence lies in tackling these constraints at specific layers in the protocol stack.  Experimental data reveals that loss and delay variations in the multimedia stream results in Audio-Video Synchronization issues leading to severe degradation of the multimedia experience. We develop accurate channel estimation techniques specifically for multi-rate wireless systems which enhance the throughput of the system. Thereafter we couple the proposed channel estimation technique with scheduling algorithms specifically designed for multi-rate wireless systems which vastly improves the performance of the system in terms of effective throughput and user satisfaction. Rate adaptation techniques are also proposed. Connectivity which completely breaks down the service is a serious issue with Wi-Fi devices since the transmission range is limited (100 meters) and wireless users tend to be inherently mobile. To ensure connectivity and honor the strict timing requirements demanded by streaming multimedia applications we have designed and implemented a client-end handoff framework for Wi-Fi systems using the Madwifi driver. In case of throughput constraint, we have focused on determining a theoretical upper bound to the number of satisfied users in comparison to existing schemes. All the proposed mechanisms jointly enhance the performance of the data services for multi-rate wireless systems and consequently maximize the number of satisfied user. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Information Theoretic, Probabilistic and Maximum Partial Substructure Algorithms for Discovering Graph-Based Anomalies</strong><br /> Monday, April 23, 2007<br /> William Eberle<br /><p><a href="#" id="396-show" class="showLink" onclick="showHide(396);return false;">Read More</a></p></p><div id="396" class="more"><p><a href="#" id="396-hide" class="hideLink" onclick="showHide(396);return false;">Hide</a></p><p><strong>Abstract: </strong>The ability to mine data represented as a graph has become important in several domains for detecting various structural patterns.  One important area of data mining is anomaly detection, particularly for fraud.  However, less work has been done in terms of detecting anomalies in graph-based data.  While there has been some previous work that has used statistical metrics and conditional entropy measurements, the results have been limited to certain types of anomalies and specific domains.  In this work we present graph-based approaches to uncovering anomalies in domains where the anomalies consist of unexpected entity/relationship alterations that closely resemble non-anomalous behavior.  We have developed three algorithms for the purpose of detecting anomalies using the minimum description length principle to first discover the normative substructure.  Once the common pattern is known, each algorithm then uses a different approach to discover particular types of anomalies. Using synthetic and real-world data, we evaluate the effectiveness of each of these algorithms.  Our approach  demonstrates the usefulness of examining a graph-based representation of data for the purposes of detecting fraud, where some individual or entity is cloaking their illegal activities through an attempt at closely resembling legitimate transactions. 	 </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient Multi-View Video Coding Scheme Based On Dynamic Video Object Segmentation</strong><br /> Friday, April 20, 2007<br /> Xiaohui Wei<br /><p><a href="#" id="395-show" class="showLink" onclick="showHide(395);return false;">Read More</a></p></p><div id="395" class="more"><p><a href="#" id="395-hide" class="hideLink" onclick="showHide(395);return false;">Hide</a></p><p><strong>Abstract: </strong>Multi-view video, which simultaneously acquires multiple video sequences from multiple viewpoints or view directions, is poised to become the next generation video technology. Exploiting redundancy is the hallmark of traditional video coding but is even more essential in multi-view video coding (MVC) where the data size is extremely large. The exploitation of additional redundancies, however, incurs extra computational overhead, thereby counteracting the benefits gained from coding efficiency. This dissertation proposes an efficient MVC scheme that provides a complete encoding solution with low complexity. This includes exploitation of inter- and intra-view redundancies for achieving high coding efficiency, and exploitation of inter-view and temporal domain coded information for lowering the coding complexity. In order to be compatible with single-view applications, a multi-view scene typically consists of at least one base view (BV) and multiple enhancement views (EVs). The proposed encoding scheme first segments the pre-encoded BV into objects and background using a proposed fast segmentation technique with low overhead. Next, it generates the initial disparity maps (DM) of each object and background as well as initial inter-view prediction frame for each EV using an object registration and warping algorithm. With this initial DM, fast EV coding can be realized by using the coded information in BV. This approach has several advantages: First, the DMs for EVs are refined based on the initial DM within small search range. Second, disparity vectors (DVs) are differentially encoded with respect to the initial DV, which leads to bit-rate savings and reduced computational complexity. Third, in addition to block-based disparity compensated prediction, one more inter-view prediction is provided, which enhances accuracy with low encoding overhead. Fourth, guided by the initial DM, the motion vectors of the EV are predicted from the motion vector field of the BV, which leads to lower complexity motion estimation. Another contribution of this work is an efficient frame-based segmentation algorithm for MVC. The algorithm combines the intensity of the reconstructed frame and motion vectors obtained from the pre-encoded base view in MVC to segment a video object in fast turnaround time. The proposed segmentation, object warping and registration methodologies, collectively provide a complete compression scheme.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>WiLDCAT: An Integrated Stealth Environment for Dynamic Malware Analysis</strong><br /> Friday, April 13, 2007<br /> Amit Vasudevan<br /><p><a href="#" id="386-show" class="showLink" onclick="showHide(386);return false;">Read More</a></p></p><div id="386" class="more"><p><a href="#" id="386-hide" class="hideLink" onclick="showHide(386);return false;">Hide</a></p><p><strong>Abstract: </strong>Malware - a term that refers to viruses, trojans, worms, spyware or any form of malicious code - is widespread today. Given the devastating effects that malware have on the computing world, detecting and countering malware is an important goal. Malware analysis is a challenging and multi-step process providing insight into malware structure and functionality, facilitating the development of an antidote. To successfully detect and counter malware, malware analysts must be able to analyze them in binary, in both a coarse- (behavioral) and fine-grained (structural) fashion. However, current research in coarse- and fine-grained code analysis (categorized into static and dynamic) have severe shortcomings in the context of malware. Static approaches have been tailored towards malware and allow exhaustive fine-grained malicious code analysis, but lack support for self-modifying code, have limitationsrelated to code-obfuscations and face the undecidability problem. Given that most if not all recent malware employ self-modifying code and code-obfuscations, poses the need to analyze them at runtime using dynamic approaches. Current dynamic approaches for coarse- and fine-grained code analysis are not tailored specifically towards malware and lack support for multithreading, self-modifying/self-checking (SM-SC) code and are easily detected and countered by ever-evolving anti-analysis tricks employed by malware.  To address this problem, we propose WiLDCAT, an integrated dynamic malware analysis environment that facilitates the analysis and combat of malware, that are ever-evolving, becoming evasive and increasingly hard to analyze. WiLDCAT cannot be detected or countered in any fashion and incorporates novel, patent pending strategies for both dynamic coarse- and fine-grained binary code analysis, while remaining completely stealth. The environment allows comprehensive analysis of malware code-streams while selectively isolating them from other code-streams in real-time. WiLDCAT is portable, efficient and easy-to-use supporting multithreading, SM-SC code and any form of code obfuscations in both user and kernel-mode on commodity operating systems. It advances the state of the art in research pertaining to malware analysis by providing the toolkit that was sorely missing in the arsenal of malware analysts, until now! </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Automatic Content Analysis of Endoscopy Video (Endoscopic Multimedia Information System)</strong><br /> Tuesday, April 03, 2007<br /> Sae Hwang<br /><p><a href="#" id="392-show" class="showLink" onclick="showHide(392);return false;">Read More</a></p></p><div id="392" class="more"><p><a href="#" id="392-hide" class="hideLink" onclick="showHide(392);return false;">Hide</a></p><p><strong>Abstract: </strong>Advances in video technology are being incorporated into today&#039s healthcare practice. For example, various types of endoscopes are used for colonoscopy, upper gastrointestinal endoscopy, enteroscopy, bronchoscopy, cystoscopy, laparoscopy, and some minimal invasive surgeries (i.e., video endoscopic neurosurgery). These endoscopes come in various sizes, but all have a tiny video camera at the tip of the endoscopes. During an endoscopic procedure, the tiny video camera generates a video signal of the interior of the human organ, for example, the internal mucosa of the colon. The video data are displayed on a monitor for real-time analysis by the physician. Diagnosis, biopsy and therapeutic operations can be performed during the procedure. We define endoscopy videos as digital videos captured during endoscopic procedures. Despite a large body of knowledge in medical image analysis, endoscopy videos are not systematically captured for real-time or post-procedure reviews and analyses. No hardware and software tools have been developed to capture, analyze, and provide user-friendly and efficient access to important content on such videos. To address this problem, a project has been proposed to develop an Endoscopic Multimedia Information System (EMIS) which captures high quality endoscopy videos, analyzes the captured videos for important contents, and provides efficient access to these contents. In this dissertation, we focus on the automatic analysis techniques of endoscopy videos for important contents by presenting (1) object &amp; frame recognition, (2) multi-level endoscopy video segmentation and (3) application for medical video analysis (Measurement of Endoscopy Quality). To analyze the contents of endoscopy videos, we first propose three object &amp; frame recognition algorithms: Endoscopy Video Frame Classification, Lumen Identification and Polyp Detection. The problem of segmenting visual data into smaller chunks is a basic problem in multimedia analysis, and its solution helps in problems such as video indexing and retrieval. However, traditional video segmentation techniques are not suitable for segmenting endoscopy video because endoscopy videos are generated by a single camera operation without shot, which makes it difficult to manage and analyze them. To address this problem, I propose a novel algorithm of multi-level segmentation for endoscopy video, which represents the semantic structure of endoscopy video: Video, Phase, Piece, and Object Shot. Based on the information obtained by object &amp; frame recognition and multi-level endoscopy video segmentation, we develop software tool to measure the quality of endoscopic procedure. The development of software tool will directly benefit endoscopic research, education, and training: especially for the research-based advanced training of students in graduate and undergraduate programs in medical informatics. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Complementing Current Active Queue Management Schemes With RECHOKe And Receiver-Window Modification (RWM)</strong><br /> Monday, November 20, 2006<br /> Victor Govindaswamy<br /><p><a href="#" id="378-show" class="showLink" onclick="showHide(378);return false;">Read More</a></p></p><div id="378" class="more"><p><a href="#" id="378-hide" class="hideLink" onclick="showHide(378);return false;">Hide</a></p><p><strong>Abstract: </strong>Explicit Congestion Notification (ECN) and Active Queue Management (AQM) schemes have been proposed for present-day TCP/IP networks to ease network congestion. Together with the ECN scheme, these AQMs had shown some promise over the existing drop-tail queues. However, when implemented, they encountered the following problems: 1) the timeout mechanism or the duration of the reception of three duplicate acknowledgements (ACKs), due to early-dropped packets by these AQMs, delays the response time of TCP in reducing the network congestion, 2) using ECN with these AQMs has its downsides: i) its messages may get delayed or dropped due to congestion in downstream routers; and ii) TCP implementations at both the source and the destination have to be ECN-compliant (which presents a significant problem in today's implementations) and 3) these AQM schemes, with or without ECN, fail to protect TCP-friendly flows adequately in the presence of non TCP-friendly or malicious flows. 	This dissertation presents solutions to these problems by proposing two novel AQM modification schemes called Receiver-Window Modification (RWM) and RECHOKe (REpeatedly CHOose and Keep for responsive flows, REpeatedly CHOose and Kill for unresponsive flows). By combining these two schemes with RED, we produce a new AQM scheme called RCUBE (Receiver-Window Modified Random Early Detection queues with RECHOKe). By using RECHOKe, RCUBE easily identifies, controls and punishes malicious flows, by requiring only a small amount of information, approximately proportional to the order of magnitude of malicious flows. By using RWM, we reduce the average TCP queue sizes in the queues and in doing so, not only make it easier to identify malicious flows but also reduce the queuing delay resulting in significant improvements in one-way end-to-end packet delays, delay jitter, throughput and number of dropped packets for TCP-friendly flows. We compare RED, CHOKe, xCHOKe, RECHOKe and RCUBE schemes and show that RCUBE easily outperforms these schemes in identifying, controlling and punishing malicious flows and in protecting TCP-friendly flows. Other similar proposed schemes either did not perform as well or incur too much overhead. We also provide theoretical analysis for both RECHOKe and RWM schemes to validate our claims.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Probabilistic Localization Of Mobile Ad Hoc Networks</strong><br /> Wednesday, November 15, 2006<br /> Rui Huang<br /><p><a href="#" id="379-show" class="showLink" onclick="showHide(379);return false;">Read More</a></p></p><div id="379" class="more"><p><a href="#" id="379-hide" class="hideLink" onclick="showHide(379);return false;">Hide</a></p><p><strong>Abstract: </strong>The mobile ad hoc network localization problem deals with estimating the physical location of the nodes that do not have a direct way (e.g., GPS) to determine their own location. Being an enabling technology that is considered essential to the success of future implementation of ad hoc networks in the real world, localization is a fundamental problem that needs to be solved with the best possible accuracy and efficiency. For this research, we study the localization problem in its various incarnations such as localization through static beacons, mobile beacons, dynamically deployed beacons and link longevity estimation based on relative locations. We will show the fundamentally difficulty of the localization problem using the theory of NP-Completeness. We will also propose a probabilistic framework that serves as an approximation framework for this difficult problem. We will demonstrate the effectiveness of this framework via analysis and simulation.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Learning State and Action Space Hierarchies for Reinforcement Learning Using Action Dependent  Partitions</strong><br /> Tuesday, July 25, 2006<br /> Mehran Asadi<br /><p><a href="#" id="368-show" class="showLink" onclick="showHide(368);return false;">Read More</a></p></p><div id="368" class="more"><p><a href="#" id="368-hide" class="hideLink" onclick="showHide(368);return false;">Hide</a></p><p><strong>Abstract: </strong>Autonomous systems are often difficult to program. Reinforcement learning (RL) is an attractive alternative, as it allows the agent to learn behavior on the basis of sparse, delayed reward signals provided only when the agent reaches desired goals.   Recent attempts to address the dimensionality of RL have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms.    This dissertation reviews several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed and presents a new method for the autonomous construction of hierarchical action and state representations in reinforcement learning, aimed at accelerating learning and extending the scope of such systems.    In this approach, the agent uses information acquired while learning one task to discover subgoals for similar tasks. The agent is able to transfer knowledge to subsequent tasks and to accelerate learning by creating useful new subgoals and by off-line learning of corresponding subtask policies as abstract actions (options).    At the same time, the subgoal actions are used to construct a more abstract state representation using action-dependent state space partitioning.   This representation forms a new level in the state space hierarchy and serves as the initial representation for new learning tasks. In order to ensure that tasks are learnable, value functions are built simultaneously at different levels of the hierarchy and inconsistencies are used to identify actions to be used to refine relevant portions of the abstract state space.    Together, these techniques permit the agent to form more abstract action and state representations over time. Experiments in deterministic and stochastic domains show that the presented method can significantly outperform learning on a flat state space representation. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Graph-Based Approach for Modeling and Indexing Video Data</strong><br /> Wednesday, June 21, 2006<br /> Jeongkyu Lee<br /><p><a href="#" id="363-show" class="showLink" onclick="showHide(363);return false;">Read More</a></p></p><div id="363" class="more"><p><a href="#" id="363-hide" class="hideLink" onclick="showHide(363);return false;">Hide</a></p><p><strong>Abstract: </strong>With the advances in electronic imaging, storage, networking and computing, the amount of digital video has grown tremendously. The proliferation of video data has led to significant amount of research on techniques and systems for efficient video database management.  In particular, extensive research has been done on video data modeling to manage and organize the data that is semantically rich and complicated. However, the enormous amount of data size and its complexity have restricted the progress on video data modeling, indexing and retrieval.  In order to get around the problems, we turn to a graph theoretical approach for video database. Since a graph is a powerful tool for pattern representation and classification in various applications, it can represent patterns and relationships of video objects easily. In this dissertation, in order to capture the spatio-temporal characteristics of video object, we first propose a new graph-based video data structure, called Spatio-Temporal Region Graph (STRG), which represents spatio-temporal features and the correlations among the video objects. A Region Adjacency Graph (RAG) is generated from each frame, and an STRG is constructed by connecting RAGs. An STRG is segmented into a number of pieces based on its content for efficient processing. Then, each segmented STRG is decomposed into its subgraphs, called Object Graph (OG) and Background Graph (BG) in which redundant BGs are eliminated to reduce index size and search time. Next, we propose a new indexing of OGs by clustering them using unsupervised learning algorithms for more accurate indexing. In order to perform the clustering, we need a distance measure between two OGs. For the distance measure, we propose a new measure, Extended Graph Edit Distance (EGED) because the existing measures are not very suitable for OGs. The EGED is defined in non-metric space for clustering OGs, and it is extended to metric space to compute the key values for indexing. Based on the clusters of OGs and the EGED, we propose a new indexing structure STRG-Index that provides efficient retrieval. Based on the STRG data model and STRG-Index, we propose a graph-based query language named STRG-QL, which is extended from object-oriented language by adding several graph operations.  To process the proposed STRG-QL queries, we introduce a rule-based query optimization that considers the hierarchical relationships among video segments.  For more efficient query processing, we present STRG-Index structure and show how to use it during query processing. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Development of Algorithms for Dermoscopy Image Analysis</strong><br /> Wednesday, June 07, 2006<br /> Emre Celebi<br /><p><a href="#" id="362-show" class="showLink" onclick="showHide(362);return false;">Read More</a></p></p><div id="362" class="more"><p><a href="#" id="362-hide" class="hideLink" onclick="showHide(362);return false;">Hide</a></p><p><strong>Abstract: </strong>As a result of the advances in skin imaging technology and the development of suitable  image processing techniques, during the last decade, there has been a significant increase  of interest in the computer-aided diagnosis of skin cancer. Dermoscopy is a relatively  recent, non-invasive skin imaging technique which permits visualization of features of  pigmented melanocytic neoplasms that are not discernable by examination with the naked eye.  This reduces screening errors, and provides greater differentiation between difficult lesions such as pigmented Spitz nevi and small, clinically equivocal lesions. However, it has been demonstrated that dermoscopy may actually lower the diagnostic accuracy in the hands of inexperienced dermatologists. Therefore, due to the lack of reproducibility and subjectivity of human interpretation, the development of computerized techniques is of utmost importance.   In this thesis, several algorithms for the analysis of dermoscopy images have been developed. These include automatic border detection, low-level (shape, color, and texture) feature extraction, classification, high-level (dermoscopic) feature extraction, and noise removal. Experimental results on a large and heterogeneous set of images demonstrate that the developed algorithms allow for fast and accurate classification of dermoscopy images.   </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>GENERALIZATION AND ENFORCEMENT OF ROLE-BASED ACCESS CONTROL USING A NOVEL EVENT-BASED APPROACH</strong><br /> Wednesday, May 10, 2006<br /> Raman Adaikkalavan<br /><p><a href="#" id="355-show" class="showLink" onclick="showHide(355);return false;">Read More</a></p></p><div id="355" class="more"><p><a href="#" id="355-hide" class="hideLink" onclick="showHide(355);return false;">Hide</a></p><p><strong>Abstract: </strong>Protecting information against unauthorized access is a key issue in information system security. Advanced access control models and mechanisms have now become necessary for applications and systems due to emerging acts such as the Health Insurance Portability and Accountability Act (HIPAA) and the Sarbanes-Oxley Act. Role-Based Access Control (RBAC) is a viable alternative to traditional discretionary and mandatory access control. It has been shown as cost effective and is being employed in various application domains on account of its characteristics: rich specification, policy neutrality, separation of duty relations, principle of least privilege, and ease of management. Existing RBAC approaches support time-based, context-aware and other forms of access control policies that are useful for developing secure systems. Although considerable amount of effort has been spent on policy specification aspects, relatively much less research is available on the flexible enforcement of various aspects of RBAC approaches. Furthermore, current approaches are inadequate as many applications and systems require the more dynamic and expressive event pattern constraints. In this thesis we have focused on several aspects of RBAC including generalization and enforcement of RBAC by exploiting and extending Sentinel - a well-established event-based framework. Specifically we have addressed the following problems: <br /><br />Enforcement of Existing RBAC Approaches: Security mechanisms are required for enforcing security policies. We have provided a flexible event-based technique for enforcing RBAC standard and other current extensions in a uniform manner using Sentinel. We have extended Sentinel with intervalbased semantics for event operators and alternative actions for active rules. <br /><br />Generalization of RBAC and Sentinel: We have generalized RBAC policies with expressive event pattern constraints. We have shown how to model diverse constraints such as precedence, dependency, non-occurrence, and so forth and their combinations using event patterns that are not available in existing RBAC approaches. Event patterns are event expressions that have simple and complex events as constituent events and they control the state change. Generalization of RBAC with constraints based on event patterns can be accomplished by combining the key features of the current RBAC approaches with Sentinel. For facilitating the generalization of RBAC, current event specification and detection approaches, including Sentinel, have to be generalized due to various limitations. <br /><br />Enforcement of Generalized RBAC: We have shown the modeling and enforcement of generalized RBAC policies using extended active (or ECA) authorization rules. We have introduced event registrar graphs for capturing simple and complex event occurrences and keeping track of event  patterns. We have also shown how RBAC with expressive event pattern constraints can be enforced using event registrar graphs. When compared to other mechanisms, our event-based enforcement mechanism is not disjoint from policy specification. We have briefly explored identification and handling of policy conflicts. This requires further investigation. <br /><br />Usability in RBAC: We have enhanced the usability of RBAC by adding an intelligent module for discovering role and guide/prompt the user to acquire appropriate roles for performing operations on objects. This approach relieves the user from the details of role-permission assignment knowledge and allows to concentrate on their task. We have developed several algorithms for discovering roles and analyzed their complexity and effectiveness. <br /><br />Novel Applications: We have developed various applications for demonstrating the applicability of the results obtained in this thesis. <br />We have shown how role -based security policies can be supported in web gateways using a smart push-pull approach.<br />We have shown how event operators based on interval-based semantics can be utilized for information filtering. <br />We provided an integrated model for advanced data stream applications that supports not only stream processing, but also complicated event and rule processing. We have also shown how the integrated model can be utilized for a network fault management system. <br /><br />This thesis tries to bridge the gap that currently exists between policy specification and enforcement. By mapping RBAC policies using a framework (event-based in our case) which also can be supported efficiently in the system in various ways (integrated, layered, wrapper-based, and distributed), we have not only extended RBAC to make it more useful, but also shown how the extended specifications can be mapped and enforced. This combination of specification and enforcement using a common framework forms the core contribution of the thesis.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>NOVEL ARCHITECTURES FOR EXPLOITING HETEROGENIETY IN PERVASIVE COMPUTING ENVIRONMENTS</strong><br /> Friday, April 28, 2006<br /> Mi Jeom Kim<br /><p><a href="#" id="351-show" class="showLink" onclick="showHide(351);return false;">Read More</a></p></p><div id="351" class="more"><p><a href="#" id="351-hide" class="hideLink" onclick="showHide(351);return false;">Hide</a></p><p><strong>Abstract: </strong>Pervasive computing is an emerging new paradigm with applications in military, crisis management, telemedicine and other critical areas. In mission-oriented applications, there is a need to create and utilize fundamental services such as QoS provisioning, auto-configuration, resource management, and node authentication. In the past, researchers have developed techniques for providing basic services to applications based on software middleware frameworks through interoperability among devices, but failed to address the issue of node heterogeneity in terms of capability and mobility.  The thesis aims to provide such fundamental services through exploitation of node heterogeneity such that relatively stable and capable devices cooperate and assist resource-poor devices in order to provide the basic services in the system. The main contributions of the thesis are as follows: •	Development of Configuration and Registration Scheme (CoReS) to provides auto-configuration and service location in local environments; •	Design of Network of Volunteers (NeVo) architecture to provide general services to application and end users; and •	Development of trust management and authentication protocols to facilitate secure and autonomous collaboration among entities. CoReS supports automatic network formation and node collaboration without any prior infrastructure. NeVo architecture achieves high flexibility and scalability targeting dynamic pervasive environments. CoReS system exhibits high efficiency and cross-layer optimization, compared to existing address auto-configuration protocols and service location schemes. The utility of NeVo as a flexible and adaptable architecture for creating and provisioning fundamental services is validated through extensive simulation studies. By exploiting NeVo and CoReS, applications and users can benefit from node heterogeneity, one of the most difficult challenges in pervasive environments. We develop a service discovery protocol, and security mechanisms including trust management and node authentication in the NeVo architecture. We define trust notation and operators, and develop trust evolution processes. In addition, efficient user-transparent authentication protocols are proposed to aid the trust management process. Finally, we investigate integration of the NeVo architecture into the Pervasive Information Community Organization (PICO) middleware framework. PICO creates mission-oriented communities of autonomous software entities that perform tasks for users and devices. Through the integration, trust based community formation can be achieved by utilizing trust management.   The architectures and algorithms developed in the thesis, are not limited to any specific routing protocols or physical network media, but can be applied to a variety of network environments including heterogeneous sensor networks and hybrid wired/wireless networks. CoReS and NeVo can be complementary to any pervasive computing middleware framework, providing greater flexibility by utilizing node unevenness.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SEAMLESS SERVICE COMPOSITION IN PERVASIVE COMPUTING ENVIRONMENTS</strong><br /> Wednesday, April 26, 2006<br /> Swaroop Kalasapur<br /><p><a href="#" id="350-show" class="showLink" onclick="showHide(350);return false;">Read More</a></p></p><div id="350" class="more"><p><a href="#" id="350-hide" class="hideLink" onclick="showHide(350);return false;">Hide</a></p><p><strong>Abstract: </strong>Pervasive computing promises to create a paradigm shift in the way computing technology is perceived amongst users. The vision of &#039disappearing technology&#039 will become a reality when features available from a number of devices can be utilized in a cooperative manner to accomplish user tasks. Features available from various devices can be exported as services and through service composition they can be combined to work cooperatively toward achieving user tasks. In the past, service composition has been treated as an extended service discovery mechanism where multiple services are discovered with an emphasis on achieving coordination among discovered services. Due to the dynamisms involved in pervasive computing, it is often required to dynamically construct complex services using the available basic services. Traditional service composition mechanisms fall short of constructing such compositions to create required services. In this dissertation, a novel scheme called Seamless Service Composition (SeSCo) for service description, aggregation and composition has been proposed. The proposed scheme is based on a graph model and is capable of dynamically weaving complex services. SeSCo is supported by a hierarchical service overlay designed with the intention of exploiting the heterogeneity in pervasive computing environments. The hierarchy is generated automatically through a process called latching that considers the state of resources at each node in the environment to form a hierarchical overlay. The hierarchical overlay is used to (i) maintain service information, (ii) resolve service requests, (iii) support resource poor devices in their operation, and (iv) provide seamless support to composite service sessions. To design and deploy pervasive computing systems in practice, service providers need to understand the system in terms of the level of support that can be provided, mechanisms to maximize resource utilization, optimize service deployment and minimize faults. At the same time, the users of such systems would be interested in service support guarantees, fault tolerance of the system and available service alternatives. To enable such characterization, we have developed an evaluation mechanism for pervasive computing systems that can be used to compare schemes for service provisioning. SeSCo has been successfully incorporated into the Pervasive Information Community Organization (PICO) middleware framework. The PICO middleware is aimed at creating computing communities that work cooperatively to achieve a common goal. SeSCo provides advanced service support to create and operate communities of software services on the PICO middleware framework. The techniques developed through SeSCo can be effectively applied to build and operate pervasive computing systems in various application domains including telemedicine, manufacturing, entertainment, etc. Prototype systems have been built to demonstrate the applicability of SeSCo in the field of telemedicine. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Power and Distortion Optimized Video Coding for Pervasive Computing Applications</strong><br /> Friday, April 14, 2006<br /> Yongfang Liang<br /><p><a href="#" id="340-show" class="showLink" onclick="showHide(340);return false;">Read More</a></p></p><div id="340" class="more"><p><a href="#" id="340-hide" class="hideLink" onclick="showHide(340);return false;">Hide</a></p><p><strong>Abstract: </strong>As the multimedia applications become more sophisticated and complex, video coding technologies have to evolve in order to meet the ever-increasing processing requirements. In the coming ubiquitous computing environments, the design of a video encoder must satisfy several goals: low power consumption, high compression efficiency, and content adaptive, which are addressed in this dissertation.  The work in this dissertation consists of several parts. First, we study the complexity distribution of a typical video encoder, and propose a video encoding architecture that is fully scalable in power consumption. Specifically, we introduce several control parameters into the video encoder to control the power consumption of the major encoding modules. Second, we develop an analytic framework to model, control and optimize the Power-Rate-Distortion (P-R-D) behavior of video coding. Based on the P-R-D model, we develop a quality optimization scheme to determine the best configuration of the complexity control parameters according to the power supply level of the device to maximize the video presentation quality. Third, we address the issue of how to efficiently manage the power consumption while preserving high video quality. We formulate an optimization problem that corresponds to minimize both the power consumption and video distortion. A joint Rate-Complexity-Distortion (R-C-D) model is developed to describe and control the general R-C-D behavior of video coding. The model facilitates to develop effective schemes to achieve our goals. Based on the model, we propose dynamic complexity control schemes and perform simulations on an instruction set simulator to demonstrate their effectiveness. In addition, a framework is proposed to obtain and maintain the “motion history” of a video sequence in a hierarchical fashion, and is utilized effectively for power and distortion optimized video coding, combined with the dynamic complexity control schemes. By adaptively adjusting the complexity parameters according to the motion history information gained from the environment, the power consumption is significantly further reduced. Extensive experiments have been performed to show the validities of the proposed techniques. The results certify that the work in this dissertation is able to meet the goals of power and distortion optimized video coding for the upcoming pervasive computing applications.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SUPERVISED LEARNING FROM EMBEDDED SUBGRAPHS</strong><br /> Wednesday, April 12, 2006<br /> Joe Potts<br /><p><a href="#" id="345-show" class="showLink" onclick="showHide(345);return false;">Read More</a></p></p><div id="345" class="more"><p><a href="#" id="345-hide" class="hideLink" onclick="showHide(345);return false;">Hide</a></p><p><strong>Abstract: </strong>We develop a machine learning algorithm which learns rules for classification from training examples in a graph representation. However, unlike most other such algorithms which use one graph for each example, ours allows all of the training examples to be in a single, connected graph. We apply the Minimum Description Length principle to produce a novel performance metric for judging the value of a learned classification. We implement the algorithm by extending the Subdue graph-based learning system. Finally, we demonstrate the use of the new system in two different domains, earth science and homeland security.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Game Theoretic Framework for Security Enforcement in Wireless Sensor Networks</strong><br /> Monday, November 14, 2005<br /> Afrand Agah<br /><p><a href="#" id="331-show" class="showLink" onclick="showHide(331);return false;">Read More</a></p></p><div id="331" class="more"><p><a href="#" id="331-hide" class="hideLink" onclick="showHide(331);return false;">Hide</a></p><p><strong>Abstract: </strong>Due to resource limitation and often lack of infrastructure, providing security in wireless sensor networks is a great  challenge. Node misbehavior due to selfish or malicious  reasons or faulty nodes can significantly degrade the  performance of such networks. So countermeasures against denial of service (DoS) attack and node misbehavior are mandatory requirements. The objective of this dissertation is defining a new set of security metrics. We argue that  the conventional view of security based on cryptography  is not sufficient for the unique characteristics encountered in sensor networks. The fundamental observation is that  cryptography can not prevent malicious behavior of internal adversaries and recognize faulty nodes.  We investigate a novel framework for security services in  wireless sensor networks that ranges from prevention of  DoS attack to secure routing. Prevention of DoS attack  focuses on the formal assessment of the properties of  cooperation enforcement mechanisms used to detect and  prevent selfish behavior of sensor nodes. This approach is based on non-cooperative nonzero-sum game theory,  where players of the game are sensor nodes. These players can occasionally misbehave and act selfishly. In this  non-cooperative game, we demonstrate that the equilibrium exists where no rational player has any incentive to  deviate from its own strategy; and to maximize the amount of profit with the least amount of false detections,  sensor network shall isolate particular nodes that act maliciously.   Our proposed approach for secure routing provides an  automatic method for the social mechanisms of reputation and cooperation. To limit or localize the damage caused by malicious nodes, our approach incorporates a security mechanism based on game theory that enforces cooperation among nodes and punishment for non-cooperative behavior. The proposed approach is called Secure Auction based  Routing (SAR). Here the assumption is that rational  players always plan to maximize their profit over time. Also to cope with misbehavior in such networks, nodes  need to be able to automatically adapt their strategy  to change levels of cooperation.  A malicious node can misrepresent its identity in the network and issue route error messages to misdirect the path or drop incoming  packets. The key to solve this problem is when nodes of a network use resources they have to contribute to the  network life in order to be entitled to use resources  in the future. To enable such networks to keep functioning despite the presence of misbehaving nodes, we propose a  reputation mechanism, where nodes prefer to gain reputation in the network. Nodes willing to do so must compete  against each other, where competition is based on auction theory. Node&#039s truthful bidding remains a dominant strategy and to have a secure routing protocol, malicious nodes who  do not bid truthfully shall be isolated.   We also propose a game theoretic framework for detecting malicious nodes, based on theory of repeated games. The  benefit from using the proposed framework shows the impact of the dynamics of large group of players, and that the  strategy chosen by a player does not only depend on one malicious node perception of the game but also takes into account a group policy of all players. The proposed  framework describes the strategy of a sensor node that has to take the decision whether to cooperate with the rest of the network, and thus encourages sensor nodes to  cooperate with other nodes. It also identifies non  participating nodes and isolates them. We also investigate that infinite repetition can be the key for obtaining  behavior, which could not be the equilibrium if the game were played once or for a known finite number of times.  Implementation results indicate that by using repeated game theory framework, which is conditioned on past histories of players, the malicious nodes are detected more accurately.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient and Adaptive Schemes for Consistent Information Sharing in Wireless Mobile and Peer-to-Peer Networks</strong><br /> Monday, August 08, 2005<br /> Zhijun Wang<br /><p><a href="#" id="328-show" class="showLink" onclick="showHide(328);return false;">Read More</a></p></p><div id="328" class="more"><p><a href="#" id="328-hide" class="hideLink" onclick="showHide(328);return false;">Hide</a></p><p><strong>Abstract: </strong>With the tremendous growth of applications in wireless mobile and Peer-to-Peer (P2P) networks, significant research efforts have been made to improve the communication quality. Caching and replicating frequently used data objects or files in user&#039s local buffers are popular mechanisms to effectively reduce the effects of communication bandwidth and improve overall system performance. However, the frequent disconnections of users make data consistency a difficult task in wireless mobile and P2P networks.<br /> <br /> In this dissertation, we first propose a Scalable Asynchronous Cache Consistency Scheme (SACCS) for wireless cellular networks. SACCS is a highly scalable, efficient, and low complexity scheme and works well in error-prone wireless mobile environments. SACCS as well as other existing mobile cache consistency schemes are focused on the single cell mobile environments. However, a scheme designed for single cell environments may not be efficient for multi-cell environments. Hence it is necessary to develop efficient cache consistency schemes for multi-cell mobile environments. We develop a Dynamic SACCS (DSACCS) for multi-cell mobile environments. Based on our best knowledge, DSACCS is the first cache consistency scheme targeting at the optimized cache performance in multi-cell mobile environments. An improvisation of DSACCS, called DSACCS-G, is also proposed for grouping cells in order to facilitate effective cache consistency maintenance in multi-cell environments. The comprehensive simulation results show that SACCS offers more than 50\% performance gain than that of existing Timestamp (TS) and Asynchronous Stateful (AS) schemes; DSACCS achieves optimized performance in multi-cell environments.<br /><br /> In P2P networks, some files are heavily replicated to enhance the file availability and reduce file search cost. Initially, P2P systems are designed for downloading static media files. Hence, the file update issues have not received much attention in P2P network design. With the dramatic growth in P2P applications, the file consistency maintenance issues become critical. To effectively propagate update to replica peers, we propose an Update Propagation Through Replica Chain (UPTReC) algorithm for decentralized and unstructured P2P networks. UPTReC is an efficient algorithm and provides weak file consistency. To provide strong file consistency, a file consistency algorithm, called file Consistency Maintenance through Virtual servers (CMV), is also developed for P2P networks. In CMV, each dynamic file has a virtual server, any file update must be accepted through the virtual server to maintain one copy serilizability of the file. To the best of our knowledge, CMV is the first strong file consistency algorithm for decentralized and unstructured P2P networks. The simulation results show that UPTReC algorithm outperforms the existing algorithms, and CMV is an efficient file consistency algorithm with very low overhead messages for file consistency maintenance. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Utility Based Resource Aware Framework for Information Caching and Sharing in Mobile and Distributed Systems</strong><br /> Monday, August 08, 2005<br /> Huaping Shen<br /><p><a href="#" id="329-show" class="showLink" onclick="showHide(329);return false;">Read More</a></p></p><div id="329" class="more"><p><a href="#" id="329-hide" class="hideLink" onclick="showHide(329);return false;">Hide</a></p><p><strong>Abstract: </strong>Internet is evolving into an indispensable service delivery infrastructure and infinite information database. Along with the technology advancements in mobile and wireless networks, ubiquitous information service is becoming a reality in which users can access information anytime anywhere. However, the user mobility, network heterogeneity and resource constraints impose significant challenges to provide ubiquitous information services. In this dissertation, a utility based resource aware framework is proposed to enhance ubiquitous information availability to mobile users through data caching and peer-to-peer sharing. The framework considers the constrained resources of mobile and distributed environments and provides flexible, efficient and scalable data access services to the mobile users. The major contributions of this framework are as follows. First, we introduce a novel energy and bandwidth efficient data caching mechanism, called GreedyDual Least Utility (GD-LU), to enhance dynamic data availability to mobile users in cellular networks. Based on the utility functions derived from a utility based analytical model, we propose algorithms for cache replacement and passive prefetching of data objects. Second, we introduce a novel scheme called Energy Efficient Peer-to-Peer Caching with Optimal Radius (EPCOR) to enable peer-to-peer information sharing in multi-hop hybrid networks. In EPCOR, a peer-to-peer overlay network is built among the mobile users to facilitate cooperative sharing of data based on network proximity and data preference. In order to conserve energy, each mobile user shares a data item in a cooperation zone. An algorithm is developed to determine the optimal radius of the cooperation zone. Third, we investigate location-aided information retrieval in large-scale mobile peer-to-peer (MP2P) networks. A novel scheme, called Proximity Regions for Caching in Cooperative MP2P Networks (PReCinCt) is designed to utilize location information to support scalable data retrieval. In the PReCinCt scheme, the network topology is divided into geographical regions where each region is responsible for a set of keys representing the data. Each key is then mapped to a region location based on a geographical hash function. We evaluate and validate the proposed framework both analytically and experimentally. We have conducted extensive experiments using large scale simulations to evaluate the performance of proposed framework. All analytical and experimental results show that the proposed framework can efficiently provide ubiquitous information services in mobile and distributed environments.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Machine Learning in the Intelligent Environment</strong><br /> Tuesday, July 19, 2005<br /> Michael Youngblood<br /><p><a href="#" id="326-show" class="showLink" onclick="showHide(326);return false;">Read More</a></p></p><div id="326" class="more"><p><a href="#" id="326-hide" class="hideLink" onclick="showHide(326);return false;">Hide</a></p><p><strong>Abstract: </strong>Markov models provide a useful representation of system behavioral actions and state observations, but they do not scale well. Utilizing a hierarchy and abstraction through hierarchical hidden Markov models (HHMMs) improves scalability, but these structures are usually constructed manually using knowledge engineering techniques. We introduce a new method of automatically constructing HHMMs using the output of a sequential data-mining algorithm, Episode Discovery, and apply it to solving automation problems in the intelligent environment domain. Repetitive behavioral actions in sensor rich environments such as smart homes can be observed and categorized into periodic and frequent episodes through data-mining techniques utilizing the minimum description length principle. Utilizing this approach, we provide an architecture and a set of algorithms for a pervasive computing system showing that inhabitant interactions in home and workplace environments can be accurately automated through sensor observation and intelligent control using a data-driven approach to automatically generate hierarchical inhabitant interaction models in the form of HPOMDPs and these models may be modified using temporal-difference reinforcement learning techniques to continually adapt to changes in the inhabitant&#039s patterns until a new model should be generated. We present our life-long learning system and apply this work in our MavPad and MavLab environments where we have been successful at automating up to 40% of the life of a real inhabitant and 76% of a virtual inhabitant as well as dynamically adapting to concept changes over time. Findings from several case studies are provided to show the feasibility of this approach. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Game Theoretical Models and Algorithms for Rate Control in Video Compression</strong><br /> Thursday, July 14, 2005<br /> Daniel Jiancong Luo<br /><p><a href="#" id="327-show" class="showLink" onclick="showHide(327);return false;">Read More</a></p></p><div id="327" class="more"><p><a href="#" id="327-hide" class="hideLink" onclick="showHide(327);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis investigates game theory based on rate control schemes for optimizing the bit allocation in video compression. The first algorithm utilizes the cooperative bargaining game in a MB level rate control algorithm to optimize the perceptual quality while guaranteeing “fairness” in bit allocation among macroblocks.  The algorithm first allocates the target bits to frames based on their coding complexity; a method to estimate the coding complexity of the remaining frames is proposed. Next, macroblocks of a frame play cooperative games such that each macroblock competes for a share of resources (bits) to optimize its quantization scale while considering the human visual system (HVS) perceptual property. Since the whole frame is an entity perceived by viewers, macroblocks compete cooperatively under a global objective of achieving the best quality with the given bit constraint. The major advantage of the proposed approach is that the cooperative game leads to an optimal and fair bit allocation strategy based on the Nash Bargaining Solution. Another advantage is that it allows multi-objective optimization with multiple decision makers (e.g., macroblocks). The algorithm is able to achieve accurate bit rate with good perceptual quality, and to maintain a stable buffer level. The second algorithm based on a non-cooperative strategy game is aimed for multiple video object level bit allocation. We formulate a two-player bi-matrix game. The utilities of the players are pre-determined a set of available strategies, the possible quantization parameters. The game is non-deterministic in which the players’ strategies are bound to a probability distribution over the set of available actions. The outcome of the game is a mixed strategy Nash equilibrium. The proposed algorithm achieves accurate bit rate regulation and smooth buffer occupancy.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Relational Discovery in Sequentially-Connected Data Streams:  Efficient Algorithms for Lossless Pattern Discovery and Change Detection</strong><br /> Monday, April 25, 2005<br /> Jeff Coble<br /><p><a href="#" id="315-show" class="showLink" onclick="showHide(315);return false;">Read More</a></p></p><div id="315" class="more"><p><a href="#" id="315-hide" class="hideLink" onclick="showHide(315);return false;">Hide</a></p><p><strong>Abstract: </strong>We are developing relational data mining techniques that discover structural patterns consisting of complex relationships between entities.  Our research is particularly applicable to domains in which the data is event driven, such as counter-terrorism intelligence analysis.  Such analytical tasks require discovery of relational patterns between events and actors so that these patterns can be exploited for prediction and action.  An additional complexity of these event-driven problems is that they are often continuous<br> -- with data streaming in over a long period of time or even indefinitely -- presenting the need to repeatedly assimilate new data into the discovery process.  However, reprocessing the accumulated data after receiving each new increment is often an intractable task because of the computational demands of most relational discovery methods. <br><br> Our work has resulted in an algorithm to mine relational data streams by summarizing discoveries from previous data increments so that the globally-best patterns can be computed by examining only the new data increment and isolated sets of sequentially-connected data spanning increment boundaries.  This algorithm includes a targeted, localized search based on the set of globally-best substructures and a graph exploration technique that restricts the range of the graph that must be explored to find pattern instances spanning increment boundaries. <br><br> Many continuous problems are dynamic in nature, requiring discovery algorithms to be capable of recognizing and adapting to change over time. We introduce an algorithm with which we are able to compute a representative point in graph space for sequential sets of the best patterns discovered from successive data increments.  We use these points, along with a distance metric, to apply statistical measures to detect and assess change.  The objective of this work is to enable a method for measuring pattern drift in relational data streams, where the salient patterns may change in prevalence and structure over time.  With a measure of central tendency for graph data, along with a method for calculating graph distance, we can begin to adapt time-series techniques to relational data streams.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Supervised Learning from Embedded SUBGRAPHS</strong><br /> Friday, April 22, 2005<br /> Joe Potts<br /><p><a href="#" id="316-show" class="showLink" onclick="showHide(316);return false;">Read More</a></p></p><div id="316" class="more"><p><a href="#" id="316-hide" class="hideLink" onclick="showHide(316);return false;">Hide</a></p><p><strong>Abstract: </strong>We develop a machine learning algorithm which learns rules for classification from raining examples in a graph representation. However, unlike most other such algorithms which use one graph for each example, ours allows all of the training examples to be in a single, connected graph. We apply the Minimum Descriptive Length principle to produce a novel performance metric for judging the value of a learned classification. We implement the algorithm by extending the Subdue graph-based learning system. Finally, we demonstrate the use of the new system in two different domains, earth science and homeland security.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Query Processing on Streamed XML Data</strong><br /> Wednesday, April 20, 2005<br /> Sujoe Bose<br /><p><a href="#" id="312-show" class="showLink" onclick="showHide(312);return false;">Read More</a></p></p><div id="312" class="more"><p><a href="#" id="312-hide" class="hideLink" onclick="showHide(312);return false;">Hide</a></p><p><strong>Abstract: </strong>In an era of unprecedented access and the resulting reliance on significant real-time information, a robust and powerful framework is required to harvest useful knowledge from the overwhelming overload of information. As data from disparate sources are frequently interrelated, it is often desirable to assimilate such correlations to deduce causal implications. The main focus of this thesis is in defining a framework to support continuous query processing of real-time data with powerful query constructs that correlate information across data streams. Since XML is adopted as the defacto standard for information exchange and is complemented with a rich set of supporting specifications and standards, we have chosen XML as the underlying data model for our framework with XQuery as the query language. We also address the challenge of executing powerful continuous queries on streaming data in hand-held mobile devices, which are inherently associated with low processing power, memory capacity, bandwidth, and a high degree of intermittent connectivity.  <br><br> One of the distinct contributions of this thesis is in the provision of a fragmented model of XML data and the seamless processing of fragments instead of processing complete XML documents. The XML fragments are processed in a continuous pipelined fashion without the need of materializing the entire document. By incorporating pro-active techniques to discard fragments that are neither part of the query result nor contribute to the query result, we achieve memory efficiency. Although there has been some work done in defining algebras that model XQueries on stored XML documents, no work has been done in defining query algebras for fragmented XML stream data. We developed an algebra for stream processing of XML fragments, provided denotational semantics of the processing primitives, implemented it, and evaluated the memory efficiency gained using the XMark benchmark. <br><br> In addition, we extended our framework to handle continuous querying of time-varying streamed XML data.  A continuous stream in our framework consists of a finite XML document followed by a continuous stream of updates. The reconstruction of temporal data from continuous updates at a current time is never materialized and historical queries operate directly on the fragmented streams.  We are incorporating temporal constructs to XQuery with minimal changes to the existing language structure to support continuous querying of time-varying streams of XML data.  Our extensions use time projections to capture time-sliding windows, version control for tuple-based windows, and coincidence queries to synchronize events between streams.  These XQuery extensions are compiled away to standard XQuery code and the resulting queries operate continuously over the fragmented streams. Finally, we present a novel hash-count technique to help in streamed query processing by delivering query results faster and flushing buffers sooner. This technique addresses the challenge of dealing with blocking queries that wait for the end of stream and those classes of queries that require unbounded buffer state. <br><br> This thesis will have a broader impact on a wide range of applications that rely on stream processing, especially in electronic commerce and mobile applications, since it will improve the way services are provided to and processed by clients. We expect our constructed prototype systems to drastically improve performance when compared with other stream processing systems.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Novel Framework for Energy and Application-Aware Data Gathering in Wireless Sensor Networks</strong><br /> Monday, April 18, 2005<br /> Wook Choi<br /><p><a href="#" id="318-show" class="showLink" onclick="showHide(318);return false;">Read More</a></p></p><div id="318" class="more"><p><a href="#" id="318-hide" class="hideLink" onclick="showHide(318);return false;">Hide</a></p><p><strong>Abstract: </strong>A wireless sensor network is an application-specific information gathering platform where sensors are required to sense their vicinity (sensing coverage) continuously, consuming highly limited resources such as energy which may not often be replenishable. Thus, an important issue in sensor networks is to design energy-aware algorithms and protocols that optimize energy consumption with a goal to extend the network lifetime while meeting the user requirements such as coverage and data reporting latency. The sensitivity to these requirements varies depending on the type of applications, implying that the designed algorithms and protocols must also be application-aware. In this thesis, we propose a novel framework for energy and application-aware data gathering in wireless sensor networks.<br> More specifically, our framework includes two strategies: i) a cluster-based delay-adaptive data gathering strategy (CD-DGS) and ii) a coverage-adaptive data gathering strategy (CA-DGS).<br><br>  The first strategy, called CD-DGS, is based on a two-phase clustering scheme that requests sensors to construct two types of links: direct and relay links. The direct links are used for control and time-critical sensed data forwardings. On the other hand, the relay links are used only for sensed data forwarding based on the user delay constraints, thus allowing the sensors to opportunistically use the most energy-saving links and forming a multihop path. Simulation results demonstrate that CD-DGS saves a significant amount of energy for dense sensor networks by adapting to the user delay constraints.<br><br>  The second strategy, called CA-DGS, is based on a trade-off between sensing coverage and data reporting latency. The basic idea is to select in each round k data reporters (sensors) which are sufficient for the desired sensing coverage (DSC) specified by the users/applications. For selecting k reporters in a round, we make use of three efficient coverage-adaptive random sensor selection (CANSEE) schemes. These reporters form a data gathering tree and are scheduled to remain active for that round only. This process incurs some delay but saves energy. We derive a probabilistic bound on k and also measure the probability for almost surely having k data reporters in each round. Finally, we investigate the Poisson sampling technique to improve the spatial regularity of the selected k sensors and then propose an enhanced random sensor selection scheme, called constrained random sensor selection (CROSS). Probabilistic analysis shows that the CROSS scheme improves the connectivity of the selected sensors and reduces the variance of the sensor covered area in each round. Simulation results demonstrate that CA-DGS results in a significant conservation of energy with a small trade-off in terms of data reporting latency. In particular, the higher the network density, the higher is the energy conservation without any additional computation cost. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Using Information-Theoretic Principles To Discover Interesting Episodes In A Time-Ordered Input Sequence</strong><br /> Friday, November 19, 2004<br /> Edwin Heierman<br /><p><a href="#" id="278-show" class="showLink" onclick="showHide(278);return false;">Read More</a></p></p><div id="278" class="more"><p><a href="#" id="278-hide" class="hideLink" onclick="showHide(278);return false;">Hide</a></p><p><strong>Abstract: </strong>Knowledge discovery techniques can be applied to discover interesting patterns of interactions contained in a temporal sequence. Existing approaches use frequency, and sometimes length, as measurements for interestingness. Because these are temporal input sequences, additional characteristics, such as periodicity, may also be interesting. In addition, current techniques do not provide a means of evaluating one collection of interesting patterns versus another. Such a value would be useful to determine if one collection of discovered patterns is more interesting than another, which is the case when a technique can produce more than one set of interesting patterns depending on the algorithm parameters. We propose that information-theoretic principles can be used to evaluate interesting characteristics of time-ordered input sequences. By using such an approach, additional characteristics can be discovered and a measure for the discovered patterns can be provided. In this dissertation, we present a novel data mining technique, called Episode Discovery (ED), based on the Minimum Description Length (MDL) principle. ED discovers patterns with interesting features in a time-ordered sequence by computing a compression ratio for a description of the input sequence based on the discovered patterns. First, we present the MDL foundation of our approach, as well as the details of our algorithm. Multiple capabilities of the algorithm are also demonstrated, such as the use of the evaluation measure for the patterns discovered and using the algorithm in a real-time environment. Finally, we present two case studies where ED was integrated with components from an intelligent environment. The first case study shows that ED can be used to improve the performance of a predictor, while the second case study shows the benefits of integrating the technique with a decision maker. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Game Theory Based Resource Management Framework for Revenue Maximization and Differentiated Services in Competitive Wireless Data Networks</strong><br /> Wednesday, November 10, 2004<br /> Haitao Lin<br /><p><a href="#" id="284-show" class="showLink" onclick="showHide(284);return false;">Read More</a></p></p><div id="284" class="more"><p><a href="#" id="284-hide" class="hideLink" onclick="showHide(284);return false;">Hide</a></p><p><strong>Abstract: </strong>ABSTRACT: The past decade noticed the explosive growth of the Internet and the TCP/IP infrastructure. The tremendous demand for the Internet access in the wireless domain, particularly for data services, is bringing more opportunities to both the technology development and business expansion. However, providing satisfactory data services in wireless networks is very challenging due mainly to (i) the nature of wireless media -- large variance in the link quality, scarce bandwidth because of the sharing of media, and (ii) unprecedented competition due to customer churning owing to wireless number portability and competitive offering of services from multiple providers. Though market competition has been recognized as a major source of revenue loss for the wireless service providers, this fact has not been addressed in depth at the level of wireless network architecture and algorithm design.  To address the above challenges elegantly, this dissertation takes a game theoretic approach to model the competitiveness and non-cooperative relationship between the service providers and the customers, and utilizes the game formulation to propose an integrated framework to solve the resource management problem in code division multiple access (CDMA) wireless data networks. This framework provides flexibility to support differentiated quality of service (QoS) to different classes of users, and incorporates the impact of service competitiveness into the per-user resource management policy, with a goal to maximize the aggregated revenue for the provider.    More specifically, the proposed framework manages the media resource over the air interface at two levels. At the session (or macro) level, the admission control problem is formulated as a non-cooperative game between the players, a service provider and the user(s) requesting admission. Two modes of admission are considered: admitting the users one at a time, and in batch mode.  The user behavior of migrating between service providers, which is called, churning, is modeled with the help of the utility function for the players. It is shown that each instance of such games has an equilibrium, or a dominant strategy, which clearly defines the service provider&#039s admission strategy. It is also shown that the batch admission mode yields better overall utility, with the price of longer admission delay. At the medium access control (MAC)  frame (or micro) level, a multi-rate CDMA system model is employed. In such model, different groups of users are allowed to have different subscribed transmission rates, and it is allowed to go below the subscribed rate when congestion happens at the air interface. Unlike other existing schemes defining a hard limit on the rate reduction, the proposed optimal rate allocation strategy is guided by the user churn rate, derived directly from the users subjective satisfaction (utility). It is proved that such an optimal rate allocation problem is NP-Complete, followed by the design of a highly efficient heuristic that produce high quality  sub-optimal solutions. The admission control and rate allocation schemes at two levels interact with each other through the use of the utility functions, namely the rate allocation decides the instantaneous utility for the service provider and the user, while the admission control makes use of such instantaneous utility and makes sure the overall utility is maximized. The performance of the framework is studied in detail with extensive simulations with realistic traffic models. Analytical and simulation results demonstrate that the proposed framework is able to manage wireless resources in such a way that the total revenue for the service provider is improved as much as 45%. The differentiated QoS is achieved for different classes of users, both at the session level and the frame level. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>On The Mobility and QoS Support in Wireless IP Networks</strong><br /> Monday, November 01, 2004<br /> Wei Wu<br /><p><a href="#" id="275-show" class="showLink" onclick="showHide(275);return false;">Read More</a></p></p><div id="275" class="more"><p><a href="#" id="275-hide" class="hideLink" onclick="showHide(275);return false;">Hide</a></p><p><strong>Abstract: </strong>As the Internet is gaining more and more popularity and wireless networks are evolving towards the third and fourth generations (3G/4G), we are witnessing the convergence of wireless networks and the Internet. Two of the most challenging and mutually related issues in the wireless Internet to be resolved are mobility and quality of service (QoS) provisioning. Wireless Internet users require not only seamless mobility support but also certain level of QoS assurance. However, uncertainty in the availability of resources and services mainly due to user mobility as well as error-prone and bandwidth-limited wireless communication links make the QoS provisioning in mobile Internet more challenging than that in the wired Internet.  This dissertation proposes novel architectures and protocols for the mobility and QoS support in wireless IP networks. Mobility management solutions have been proposed for mobile nodes that change points of attachment frequently within a local domain and also roam between networks using different wireless access technologies. For the former problem dealing with intra-domain mobility, we propose a network architecture to provide the IP mobility support in wireless local area networks (WLAN) while requiring minimal changes to the underlying network infrastructure as well as the mobile nodes. It has been shown that the proxy processing for mobility-aware packet forwarding only adds nominal delay to the total handoff latency, which is mainly caused by the inherent system latency of link layer association. For the latter problem dealing with inter-system or vertical mobility, a mobile agent-assisted mobility scheme is proposed to address the needs of a mobile user for seamless roaming in heterogeneous wireless overlay networks. New mobile software entities or agents are introduced into the wired networks communicating with the network on such tasks as handoff decision and QoS negotiation on behalf of the mobile users. The signaling analysis shows that most of the signaling interaction between mobile devices and networks is constrained to the wired links, thus reducing the dependence on the limited wireless bandwidth. Finally, we propose the Intra-Domain QoS Architecture (IQoSA) for integrated QoS and mobility support for fast moving mobile users within an IP-based wireless access domain. This framework is based on the Differentiated Services (Diffserv) model with a centralized Bandwidth Broker (BB) performing admission control and resource provisioning for different traffic classes. The IQoSA architecture is derived by enhancing the Intra-Domain Mobility Management Protocol (IDMP) with the QoS functions, which is used for QoS signaling as well as mobility signaling. Analytical and experimental results show that signaling traffic in IQoSA results in less overhead compared to alternative solutions for fast and active intra-domain mobility. Moreover, the QoS processing overhead does not result in the increase of handoff latency. Aggregate resource reservation and failure recovery schemes in IQoSA have also been proposed for achieving better system scalability and resilience.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Mobility and Resource Adaptive Architecture and Protocols for Multimedia Applications in Ubiquitous Computing</strong><br /> Thursday, October 21, 2004<br /> Nilanjan Banerjee<br /><p><a href="#" id="271-show" class="showLink" onclick="showHide(271);return false;">Read More</a></p></p><div id="271" class="more"><p><a href="#" id="271-hide" class="hideLink" onclick="showHide(271);return false;">Hide</a></p><p><strong>Abstract: </strong>Ubiquitous computing is the notion of building an invisible computing environment by providing the users a wide variety and large number of wireless enabled devices to interact with an activated world. Spectacular advancements in such technologies as wireless communications, sensors and portable devices are the driving forces in the realization of such environments. However, supporting mobile multimedia applications in ubiquitous computing environments still remains a challenging issue. Apart from the problems arising out of dynamic and resource constrained wireless networks and/or devices, heterogeneity due to different prevailing wireless technologies proved to be a major barrier. In this dissertation, we propose an adaptive architecture which addresses the heterogeneity issues along the two dimensions of ubiquitous computing, viz. mobile computing and pervasive computing. The proposed architecture uses open IP-based standardized protocols to facilitate the notion of IP convergence and adapts to the changes in access network technology and the available resources by means of its mobility management and resource management frameworks. An application layer mobility management framework based on the Session Initiation Protocol (SIP) is proposed for supporting mobility across heterogeneous wireless access networks. This framework is enhanced by a soft handoff based mechanism to ensure smooth multimedia streaming. The SIP session setup and mobility support are extended to infrastructure-less ad hoc networks with a basic broadcast based search technique and integrated with a cluster-based routing protocol that we propose. Finally, a novel control theory based cross-layer flow and rate control protocol is proposed for adaptive streaming multimedia applications. Theoretical foundations for a pro-active, network resource-aware middleware assisting the control protocols of the adaptive applications, is built as a part of the resource management framework of the proposed architecture. The control protocols have been shown to optimize resource utilization, while providing the necessary quality of service (QoS) such as minimum delay jitter and bandwidth guarantees, to the multimedia applications.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Rate Control for Video Coding and Communication</strong><br /> Friday, June 25, 2004<br /> Sun Yu<br /><p><a href="#" id="254-show" class="showLink" onclick="showHide(254);return false;">Read More</a></p></p><div id="254" class="more"><p><a href="#" id="254-hide" class="hideLink" onclick="showHide(254);return false;">Hide</a></p><p><strong>Abstract: </strong>Rate Control (RC) is one of the crucial techniques for  video compression and communication because it regulates  the output bit rates of a video encoder in order to obtain  optimum visual quality within the available budget of  transmission bit rate. However, due to the inaccuracy of  the existing Rate-Distortion models and their additional  drawbacks, the control abilities of traditional RC  algorithms are not effective enough. In this dissertation,  the key RC technique is to exploit prediction mechanism to  initialize bit allocation, and then apply various feedback  control tactics to compensate prediction errors. Further,  we adopt a novel Proportional-Integral-Derivative (PID)  buffer controller to effectively reduce the fluctuation  of buffer fullness and avoid the buffer overflow and  underflow. Specifically, this dissertation makes four  contributions. First, we propose a RC algorithm for  synchronous object-based video coding. The algorithm  estimates the bit budget for a frame and objects based on  their coding complexity, and dynamically adjusts parameters using feedback information to further improve the performance. Second, we analyze the framework of asynchronous object-based video coding to develop an asynchronous RC scheme. The scheme divides objects into relatively independent object streams, such that the complicated asynchronous problem is decomposed into several simplified sub-problems. The major advantage of this approach is that at the top level the algorithm dynamically distributes the bit budget among object streams by jointly adjusting the visual qualities. At the lower level one can adopt efficient single-object RC algorithm to solve the sub-problem. Third, for video transcoding, we propose a new frequency domain complexity estimation scheme and an algorithm for adaptive bit allocation during transcoding. This algorithm adaptively determines spatial coding parameters to realize accurate target bitrate transcoding with better visual quality.  Finally, by taking into account the characteristics  of wireless channels, we propose a joint source-channel RC  strategy for real-time wireless video transmission.  Accordingly, we adopt a Region-Based segmentation method  to further improve the quality of the Regions of Interested. </p></div></div></div></article>
</section>
<section>
<article>
<header>
<h2><!--section 2 heading--></h2>
</header>
<!--section 2 article-->
</article>
</section>
 <script>
togglemenu('submenu-seminars','seminars-link')
</script></div>
<footer><div id="footer-extra"><a href="https://twitter.com/intent/follow?original_referer=http%3A%2F%2Fcse.uta.edu%2Fresearch%2Fcenters-labs.php&amp;screen_name=cseuta&amp;tw_p=followbutton"><img alt="Follow " src="../_images/content/twitter.png" style="float: none; margin: 0 0 7px  5px;"/></a> <a href="https://www.facebook.com/groups/119117011465771/"><img alt="Facebook" height="30" src="../_images/content/icon-facebook.png" style="float: none; margin: 0 0 7px  5px;" width="30"/></a> <a class="align-right" href="https://www.linkedin.com/groups?trk=myg_ugrp_ovr&amp;gid=4120776"> <img alt="Linked In" height="30" src="../_images/content/icon-linkedin.png" style="float: none; margin: 0 0 7px 5px;" width="33"/></a><br/> <a href="../faculty-resources.php">Faculty Resources</a></div>
<div><img alt="The University of Texas at Arlington" height="73" src="../_images/elements/uta_sm_logo.png" width="82"/>
<p><strong>Department of Computer Science and Engineering</strong> <span class="print-only">[<a href="http://www.uta.edu/bioengineering">cse.uta.edu</a>]</span><br/> Engineering Research Building, Room 640, Box 19015, Arlington, TX 76010<br/> Phone: 817-272-3785 | Fax: 817-272-3784 <br/> <a href="http://www.uta.edu/"> &#169;  2015  The University of Texas at Arlington</a>.</p>
</div>
<div class="mobile-only">
<div style="clear: both; margin: 10px auto 10px auto; width: 90%;"><a href="https://twitter.com/intent/follow?original_referer=http%3A%2F%2Fcse.uta.edu%2Fresearch%2Fcenters-labs.php&amp;screen_name=cseuta&amp;tw_p=followbutton"><img alt="Follow " src="../_images/content/twitter.png" style="float: none; margin: 0 0 7px  5px;"/></a> <a href="https://www.facebook.com/groups/119117011465771/"><img alt="Facebook" height="30" src="../_images/content/icon-facebook.png" style="float: none; margin: 0 0 7px  5px;" width="30"/></a> <a class="align-right" href="https://www.linkedin.com/groups?trk=myg_ugrp_ovr&amp;gid=4120776"> <img alt="Linked In" height="30" src="../_images/content/icon-linkedin.png" style="float: none; margin: 0 0 7px 5px;" width="33"/></a><br/> <a href="../faculty-resources.php">Faculty Resources</a></div>
</div></footer>
</div>
</div>
</body>
</html>