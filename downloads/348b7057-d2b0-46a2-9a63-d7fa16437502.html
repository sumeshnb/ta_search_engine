<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<meta content="IE=10" http-equiv="X-UA-Compatible"/>
<meta charset="utf-8"/>
<title> UTA CSE  Masters Thesis Defenses</title>
<meta content=" UTA CSE  Masters Thesis Defenses" name="description"/>
<meta content="Computer, Science, Engineering, Department, university, texas, arlington" name="keywords"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1" name="viewport"/><link href="../_css/screen.css" media="screen" rel="stylesheet"/> <link href="../_css/print.css" media="print" rel="stylesheet"/>
<script src="../_js/menu.js" type="text/javascript"></script>
<noscript>
<link href="../_css/noscript.css" rel="stylesheet"/>
</noscript>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js" type="text/javascript"></script>
<script src="http://malsup.github.io/jquery.cycle2.js" type="text/javascript"></script>
<script src="http://malsup.github.io/jquery.cycle2.tile.js" type="text/javascript"></script>
<script language="javascript" type="text/javascript">
function showHide(shID) {
   if (document.getElementById(shID)) {
      if (document.getElementById(shID+'-show').style.display !== 'none') {
         document.getElementById(shID+'-show').style.display = 'none';
         document.getElementById(shID).style.display = 'block';
      }
      else {
         document.getElementById(shID+'-show').style.display = 'inline';
         document.getElementById(shID).style.display = 'none';
      }
   }
}

function newDoc(page) 
{

    var url = location.href
    if(url.indexOf("devel") != -1)
    {
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            page = page + "-list"
            window.location.assign("http://cse-devel.uta.edu" + page + ".php");
        
        }
        else{
        window.location.assign("http://cse-devel.uta.edu" + page + ".php");
        }
    }else{
    
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            page = page + "-list"
            window.location.assign("http://cse.uta.edu" + page + ".php");
        
        }
        else{
        window.location.assign("http://cse.uta.edu" + page + ".php");
        }
    
    } 
}

function sortOption(page) 
{

    var url = location.href
    if(url.indexOf("devel") != -1)
    {
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            //page = page + "-list"
            window.location.assign("http://cse-devel.uta.edu" + page );
        
        }
        else{
        window.location.assign("http://cse-devel.uta.edu" + page );
        }
    }else{
    
        var cookie = document.cookie;
        if(cookie.indexOf("list") != -1)
        {
            //page = page + "-list"
            window.location.assign("http://cse.uta.edu" + page );
        
        }
        else{
        window.location.assign("http://cse.uta.edu" + page );
        }
    
    } 
}

</script>
<style type="text/css">
.more {
      display: none;
}
</style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-32021828-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
<p class="skip"><a accesskey="C" href="#content-area">Skip to content</a>. <a accesskey="N" href="#nav">Skip to main navigation</a>.</p>
<div id="wrapper">
<header id="page-header"><div id="desktop-header"><a href="http://www.uta.edu/uta/"><img alt="The University of Texas at Arlington" height="48" id="UTA-logo" src="../_images/elements/uta.png" width="327"/></a> <a href="http://www.uta.edu/engineering/"><img alt="College of Engineering" height="48" id="COE-logo" src="../_images/elements/college_of_engineering.png" width="311"/></a> <a href="../index.php"><img alt="Department of Computer Science and Engineering" id="logo" src="../_images/elements/department_of_computer_science_and_engineering.png"/></a></div>
<div id="mobile-header"><!-- It is important that some images have no size attributes for rendering on different screen sizes -->
<div id="mobile-uta-logo"><a href="/uta/index.php"><img alt="The University of Texas at Arlington" class="UTA-logo-m" src="../_images/elements/mobile-header.png"/></a><br/><a href="../index.php"><img alt="Department of Computer Science and Engineering" class="UTA-logo-m" src="../_images/elements/mobile-department-header.png"/></a></div>
<div id="mobile-menu"><a href="#_top" onclick="toggle('navcontainer');" onkeypress="toggle('navcontainer');"><img alt="Menu" src="../_images/elements/menu.png"/></a></div>
</div>
<img alt="Department of Bioengineering at The University of Texas at Arlington" class="print-only" src="../_images/elements/print-header.png"/></header>
<div id="top-line"></div>
<div id="content-wrapper">
<nav id="navcontainer"><ul id="nav">
<li><a href="../index.php">Home</a></li>
<li><a href="../news-highlights/index.php">News</a></li>
<li><a class="with-submenu" href="#_top" id="admission-link" onclick="togglemenu('submenu-admission','admission-link');" onkeypress="togglemenu('submenu-admission','admission-link');">Future Students</a>
<ul class="submenu" id="submenu-admission">
<li><a href="../future-students/under-app.php">Undergraduate Applicants</a></li>
<li><a href="../future-students/grad-app.php">Graduate Applicants</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="current-link" onclick="togglemenu('submenu-current','current-link');" onkeypress="togglemenu('submenu-current','current-link');">Current Students</a>
<ul class="submenu" id="submenu-current">
<li><a href="../current-students/undergraduate-studies.php">Undergraduate Study</a></li>
<li><a href="../current-students/graduate-studies.php">Graduate Study</a></li>
<li><a href="../current-students/courses-catalog-schedules.php">Courses, Catalog &amp; Schedules</a></li>
<li><a href="../current-students/student-clubs.php">Student Clubs/Organizations</a></li>
<li><a href="../current-students/student-services.php">Student Services</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="people-link" onclick="togglemenu('submenu-people','people-link');" onkeypress="togglemenu('submenu-people','people-link');">People</a>
<ul class="submenu" id="submenu-people">
<li><a href="../faculty-directory.php">Faculty Directory</a></li>
<li><a href="../staff-directory.php">Staff Directory</a></li>
<li><a href="../current-students/phd-students/directory-grid.php">PhD Student Directory</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="research-link" onclick="togglemenu('submenu-research','research-link');" onkeypress="togglemenu('submenu-research','research-link');">Research</a>
<ul class="submenu" id="submenu-research">
<li><a href="../research/centers-labs.php">Centers &amp; Labs</a></li>
<li><a href="../research/research-areas.php">Research Areas</a></li>
<li><a href="../research/technical-reports.php">Technical Reports</a></li>
</ul>
</li>
<li><a class="with-submenu" href="#_top" id="seminars-link" onclick="togglemenu('submenu-seminars','seminars-link');" onkeypress="togglemenu('submenu-seminars','seminars-link');">Seminars</a>
<ul class="submenu" id="submenu-seminars">
<li><a href="invited_talks.php">Invited Talks</a></li>
<li><a href="ms-defenses.php">MS Defenses</a></li>
<li><a href="phd-defenses.php">PhD Defenses</a></li>
</ul>
</li>
<li><a href="../staff-directory.php">Contacts</a></li>
</ul>   <div id="menu-bottom-extra">
<div id="talks-and-defenses">
<p><strong>Invited Talks</strong></p><img src="/talks/images/majewicz.png" style="margin-left:10px;"  /><p><b>Fri 05/01</b> 1:30pm - 3:00pm<br /> Ann Majewicz, PhD - <i>Designing Human-in-the-Loop Systems for Surgical  Training and Intervention - <b>ERB 103</b></i> <img id="refreshment" src="/talks/images/pizza.jpeg" /></p><hr /><p><a href="/talks/invited_talks.php">Details and More Talks ></a></p><hr /></div>
<p><strong>Computer Science<br/>and Engineering</strong></p>
<p><span class="underline" data-mce-mark="1">Administration</span><br/> Dr. Lynn Peterson<br/> Interim Chair, Associate Dean<br/> 817-272-3605 <br/> <a href="mailto:peterson@uta.edu">peterson@uta.edu</a><br/><br/>Dr. Ramez Elmasri<br/> Associate Chair<br/> 817-272-2337 <br/> <a href="mailto:chuong@uta.edu">elmasri@uta.edu</a></p>
<p><span class="underline">Deliveries</span><br/> Engineering Research Building<br/> Room 640<br/>500 UTA Blvd.<br/> Arlington, TX 76010<br/><br/><span class="underline">Mailing Address</span><br/> Box 19015<br/> Arlington, TX 76019<br/><br/> <span class="underline">Phone/Fax/Email</span><br/> Phone: 817-272-3785<br/> Fax: 817-272-3784<br/><br/> <span class="underline">Visitors</span><br/><a href="http://www.uta.edu/maps/index.php?id=23">Map</a><br/><a href="http://www.uta.edu/pats/parking/guest-parking.php">Parking</a> <br/><a href="http://www.uta.edu/admissions/visit/tours-virtual/">Virtual Tour</a></p>
</div></nav>
<aside><h1>Faculty Highlight</h1>
<div class="cycle-slideshow" data-cycle-fx="fade" data-cycle-next="#next" data-cycle-pause-on-hover="true" data-cycle-prev="#prev" data-cycle-random="true" data-cycle-slides="&gt; div" data-cycle-timeout="3000">
<div><img align="middle" alt="Dr. Taylor Johnson" src="../_images/faculty/johnson.jpg"/>
<h3><a href="../faculty-highlights/2014-8-3-taylor-johnson-highlight.php">Taylor Johnson, PhD</a></h3>
<p>Developing novel formal verification methods to ensure correct operation of cyber-physical systems.</p>
</div>
<div><img align="middle" alt="Dr. Junzhou Huang" src="../_images/faculty/jzhuang.jpg" style="max-height: 150px;"/>
<h3><a href="../faculty-highlights/2015-2-5-junzhou-huang-highlight.php">Junzhou Huang, PhD</a></h3>
<p>Developing efficient algorithms with nice theoretical guarantees to solve practical problems involved huge scale data.</p>
</div>
</div>
<hr/>
<h6>Undergraduate Applicants</h6>
<ul>
<li><a href="../contact-undergraduate-engineering.php">Let Us Know About You</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/">University Catalog</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/#courseinventory">Courses</a></li>
<li><a href="../future-students/under-app.php">Apply</a></li>
</ul>
<h6>Master's Applicants</h6>
<ul>
<li><a href="../contact-graduate-engineering.php">Request Information</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/">University Catalog</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/#courseinventory">Courses</a></li>
<li><a href="../future-students/grad-app.php">Apply</a></li>
</ul>
<h6>Ph.D. Applicants</h6>
<ul>
<li><a href="../contact-graduate-engineering.php">Request Information</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/">University Catalog</a></li>
<li><a href="http://catalog.uta.edu/engineering/computer/#courseinventory">Courses</a></li>
<li><a href="../future-students/grad-app.php">Apply</a></li>
</ul>  
<div id="aside-clear"></div>
</aside>
<div class="with-aside" id="content-area"> 
<section>
<article>
<header>
<h1> Masters Thesis Defenses</h1>
</header>
<div>
<p><h2>Past Defenses</strong></h2><div style="border-bottom: 1px dotted #CCC;"><p><strong>PERFORMANCE COMPARISION OF SPATIAL INDEXING STRUCTURES FOR DIFFERENT QUERY TYPES</strong><br /> Monday, April 20, 2015<br /> Neelabh Pant<br /><p><a href="#" id="610-show" class="showLink" onclick="showHide(610);return false;">Read More</a></p></p><div id="610" class="more"><p><a href="#" id="610-hide" class="hideLink" onclick="showHide(610);return false;">Hide</a></p><p><strong>Abstract: </strong>R-Trees are among the most popular multidimensional access methods suitable for indexing two dimensional spatial data and point data. R-Trees are found in most of the spatial database systems for indexing the spatial data. The data include points, lines and polygons which are retrieved and stored efficiently. There are many Spatial Database Systems which have incorporated R-Trees, for example, IBM Informix, Oracle Spatial, PostgreSQL and many others. Another version of R-Tree is R*-Tree which is also used for the same purpose i.e. indexing spatial data. R*-Tree has also been incorporated in an open source software SQLite with an extension of Spatialite.</p>   <p>Several techniques have been proposed to improve the performance of spatial indexes, but none showed the comparative studies in their performance with the different categories of spatial and non-spatial queries.</p>  <p>In this work, we compare the performance of three spatial indexing techniques: R-Tree (Rectangle Tree), GiST (Generalized Search Tree) and R*-Tree (A variant of R-Tree).</p> <p>We have five categories of spatial and non-spatial queries, namely, Simple SQL, Geometry, Spatial Relationship, Spatial Join and Nearest Neighbor  search. We perform extensive experiments in all these five categories and record the execution time.</p>  <p>The spatial data that are used for the experiments is the set of a benchmark data of New York City that include Point data: Subway stations, Line data: Streets and Subway lines, Polygon data: Boroughs and Neighborhoods plus non-spatial data such as Population data: Racially categorized.</p> <p>The comparison done in the experiments will give the reader performance criteria for selecting the most suitable index structure depending on the types of queries in the application. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>BIOMEDICAL PARAMETER MONITORING USING VIDEO PROCESSING</strong><br /> Monday, April 20, 2015<br /> Negar Ziaee Nasrabadi<br /><p><a href="#" id="613-show" class="showLink" onclick="showHide(613);return false;">Read More</a></p></p><div id="613" class="more"><p><a href="#" id="613-hide" class="hideLink" onclick="showHide(613);return false;">Hide</a></p><p><strong>Abstract: </strong>For quite some time, patients' cardiovascular parameters have been measured by sensors connected to their body. One way of measuring these parameters is pulse oximetry which uses an optical technique. It uses a photodetector to detect light absorption changes of red and infrared light, which serves as two sources of illumination.  These current types of methods for monitoring parameters are often viewed as uncomfortable by the patient, and therefore are not desirable for frequent and long periods of monitoring. Furthermore, these methods can potentially produce a psychologically influenced bias from the patient because the patient is physically involved with the monitoring. As a result, there is a desire for a more patient friendly method for measuring cardiovascular parameters.</p>  <p>Recent research has shown that the cardiovascular pulse can be measured by using a camera's digital video of a person's face and daylight as an illumination source. This research opens a vast opportunity for remote, low cost, and convenient monitoring of cardiovascular parameters.  Using the optical technique, these novel methods extract the cardiovascular signal using light reflected from the face, and it ultimately allows important data about cardiovascular parameters to be extracted from a distance. These parameters include the blood flow signal, heart rate, blood oxygen, and blood pressure. Furthermore, these methods allow frequent and remote monitoring a patient in a given environment while also guaranteeing that parameters are autonomous from patient bias. As result, this method can routinely measure the patient's parameter during long period of time which is more desirable.</p> <p>This study has been extended on this research area by designing  a system to measure the changes of blood related health conditions more conveniently. The low cost system analyzes the full face and processes the reflected light signal of a local area on the face. In this study we use visible and infrared light as a source of illumination to measure the parameters at different light spectrum wavelengths which include red, green, and infrared light.</p> <p>This method is advantageous because the patients' comfort level is not sacrificed for accurate monitoring, and allows for routine monitoring. Routine monitoring of cardiovascular parameter changes is desired because the parameters that are influenced by illness are frequently tracked. Ultimately, it allows the tracking of the illness' progression and a more accurate diagnosis.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>THE IMPACT OF DIFFERENT CUES ON THE MEMORABILITY OF SYSTEM-ASSIGNED RECOGNITION-BASED TEXTUAL PASSWORDS</strong><br /> Monday, April 20, 2015<br /> Kanis Fatema<br /><p><a href="#" id="614-show" class="showLink" onclick="showHide(614);return false;">Read More</a></p></p><div id="614" class="more"><p><a href="#" id="614-hide" class="hideLink" onclick="showHide(614);return false;">Hide</a></p><p><strong>Abstract: </strong>To ensure memorability, users employ predictable patterns when creating new passwords. These patterns make passwords easy to guess, which not only increases risks to users but also for the entire system. In contrast, system-assigned textual passwords offer security but suffer from poor memorability. Previous research investigated the idea of textual recognition, in which the user would be assigned a word from a list and asked to recall the word later. The recall rate for this scheme was shown to be no better than memorizing a sequence of randomly assigned letters, but the researchers suggested that memorability may be improved if the words always appear in the same position in the list. This proposal to leverage spatial cues inspired us to explore the use of cues, including spatial but also verbal and graphical cues, to improve memorability in textual recognition. In particular, we design three schemes with different cues to see the impact of each cue. TextS is a textual recognition scheme with spatial cues, such that the words appear in the same position each time. TextV is similar to TextS, but it also offers verbal cues, i.e., phrase/facts related to keywords for textual recognition. Finally, GraphicV is similar to TextV, but it includes graphical cues, images representing each keyword to go along with verbal and spatial cues. In our multi-session lab study, all 52 participants were assigned three different passwords types (TextS, TextV and GraphicV), one for each study condition. One week after registration, the login success rate for TextS was 62\%, while the login success rate for the TextV and GraphicV schemes were 94\% and 96\% respectively. The login success rate for TextV was significantly higher than TextS, while there was no significant difference between the TextV and GraphicV schemes. These results give us insight about the impact of the different types of cues and also provide a potential future direction to attain adequate memorability for system-assigned passwords.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>GAIT ANALYSIS ON A SMART FLOOR FOR HEALTH MONITORING</strong><br /> Monday, April 20, 2015<br /> OLUWATOSIN E. OLUWADARE<br /><p><a href="#" id="615-show" class="showLink" onclick="showHide(615);return false;">Read More</a></p></p><div id="615" class="more"><p><a href="#" id="615-hide" class="hideLink" onclick="showHide(615);return false;">Hide</a></p><p><strong>Abstract: </strong>Gait analysis is the investigation of an individual pattern of walking. Based on studies in Psychophysics, it can be concluded that the human gait contains unique information that is useful for the evaluation of foot and gait pathologies. The goal of this project is to use a floor mounted Pressure Sensor (FMPS) system capable of measuring a significant number of parameters relevant to gait to predict and detect anomalous behavior. The system consists of an array of pressure sensors mounted under floor tiles and computer hardware responsible for data collection. The method used in this project is unique since most systems that perform similar functions are "on-body" systems using leg attached sensors, body tags or "off-body" using shoe integrated sensors or vision (camera). Our approach uses FMPS which are designed to collect data unobtrusively, over long periods of time without interfering with gait or inconveniencing the user.</p>   <p>The core of this thesis is aimed at the design of algorithms capable of differentiating parameter values that could be considered normal or abnormal for an individual and from these values make further conclusions. To achieve this, data obtained from the FMPS were calibrated and analyzed to extract information about the gait of a user. From this analyzed data, the center of pressure trajectories for each phase of the user's gait cycle was obtained as well as the user's weight, and dynamic characteristics of balance and step impact. With this information we intend to provide a new way for gait analysis, in order to predict fall risk and health issues and to improve elder care by constant monitoring and by reducing the white-coat syndrome that inhibits clinical examinations.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>MICROBLOG ANALYZER: Aggregate Estimation over a micro blog platform</strong><br /> Friday, April 17, 2015<br /> Satishkumar Masilamani<br /><p><a href="#" id="618-show" class="showLink" onclick="showHide(618);return false;">Read More</a></p></p><div id="618" class="more"><p><a href="#" id="618-hide" class="hideLink" onclick="showHide(618);return false;">Hide</a></p><p><strong>Abstract: </strong>Microblogging is a new mode of communication in which users can share their current status in brief and agile way in the form of text, image, video etc over smart phones, email or web. Recently, Microblogs such as Twitter, Tumblr, Google+ have experience phenomenal growth and are regularly used by millions of users. The data from microblogs is very useful for researchers to analyze various facets such as user behaviors, user intentions (like daily chatter, conversations, sharing information and reporting news), microblog social network structure etc. For example, a sociologist might want to use the microblog postings to analyze the popular opinion about a particular topic. However, existing approach to facilitate such analytics has certain limitations due to the various restrictions imposed by microblogs. Restristions include API rate limits(that restricts the amount of queries issued in a day) or other limits (Twitter search API only provides results for last few days and so on).</p>  <p>In this thesis, we build an efficient microblog analytics platform -   MICROBLOG-ANALYZER to enable the approximate estimation of aggregate queries over an online microblogging service. MICROBLOG-ANALYZER works by leveraging user timeline access offered by online microblogs. It dynamically constructs a level by level sub-graph of the microblog and performs sampling by a novel topology aware random walk. MICROBLOG-ANALYZER can handle a number of online microblogs such as Twitter, Google+, Weibo, Tumblr, Instagram etc. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Faster Sampling over Theoretical Graphs and Online Social Networks</strong><br /> Friday, April 17, 2015<br /> Ramakrishna Aduri<br /><p><a href="#" id="619-show" class="showLink" onclick="showHide(619);return false;">Read More</a></p></p><div id="619" class="more"><p><a href="#" id="619-hide" class="hideLink" onclick="showHide(619);return false;">Hide</a></p><p><strong>Abstract: </strong>Online social networks have become very popular recently and are used by millions of users. Researchers increasingly want to leverage the rich variety of information available. However, social networks often feature a web interface that only allows local-neighborhood queries - i.e., given a user of the online social network as input, the system returns the immediate neighbors of the user. Additionally, they also have rate limits that restrict the number of queries issued over a given time period. These restrictions make third party analytics extremely challenging. The traditional approach of using random walks is not effective as they require significant burn-in period before their stationary distribution converges to target distribution. In this thesis, we build a prototype system SN-WALK-ESTIMATER that starts with a much shorter random walk and uses acceptance-rejection sampling to get samples according to a desired distribution. Using only minimal information about the graph such as diameter, SN-WALK-ESTIMATER produces high quality samples with a much lower query cost. We test the system over several theoretical graph families and real world social networks.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A real-time embedded data acquisition system for surface measurements using multiple line lasers.</strong><br /> Wednesday, April 15, 2015<br /> Akkihebbal Padmanabha Vikram Simha<br /><p><a href="#" id="617-show" class="showLink" onclick="showHide(617);return false;">Read More</a></p></p><div id="617" class="more"><p><a href="#" id="617-hide" class="hideLink" onclick="showHide(617);return false;">Hide</a></p><p><strong>Abstract: </strong>In the last few years there has been a significant increase in the number of hand held devices. These devices boast of delivering features such as high performance, low power consumption, high memory availability, serial and parallel interfacing capability, connectivity through the Ethernet, wireless, etc., at a very low cost. These embedded devices powered by open source software like Linux and Arduino have paved the way for the development of high efficiency, low cost portable products in a very short period of time.</p> <p>The objective of this thesis is to enhance the portability, efficiency, and other features of the existing Roline Laser profiling system used by the Texas Department of Transportation. The focus of this research is to replace the slow and bulky processing procedure of the Roline laser profiling system with new mobile technology. This includes the use of the Intel Galileo which features the SoC Quark 1000. The improvements include extending the capabilities for multiple laser surface measurement methods and GPS tracking information.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Transmission of URL using Bluetooth Low Energy Technology</strong><br /> Tuesday, April 14, 2015<br /> Arjun Kumar Bhaskar Shetty<br /><p><a href="#" id="608-show" class="showLink" onclick="showHide(608);return false;">Read More</a></p></p><div id="608" class="more"><p><a href="#" id="608-hide" class="hideLink" onclick="showHide(608);return false;">Hide</a></p><p><strong>Abstract: </strong>This is an effort to extend the core superpower of the web - the URL - to everyday physical objects. The core premise is that you should be able to walk up to any "smart" physical object (e.g. a vending machine, a poster, a toy, a bus stop, a rental car) and interact with it without first downloading an app. The user experience of smart objects should be much like links in a web browser: i.e., just tap and use. It is a discovery service: a smart object broadcasts relevant URLs that any nearby device can receive. This simple capability can unlock exciting new ways to interact with the Web. The number of smart objects is going to explode, both in our homes and in public spaces. Much like the web, there is going to be a long tail of interactivity for smart objects. But the overhead of installing an app for each one just doesn't scale. We need a system that lets you walk up and use a device with just a tap. This isn't about replacing native apps; it's about allowing interaction for the times when native apps just aren't practical. The URL is the fundamental building block of the web, giving remarkable flexibility of expression. It can be, a fully interactive web page, or a deep link into a native application. It can be anything. Compared to classic Bluetooth, BLE has a very much reduced power consumption hence making it feasible and portable. The entire latest Smart devices are now BLE enabled. Hence making this implementation a feasible one too. A Smart phone when in proximity with a BLE device will be able to pick up its signal and will inform you regarding the distance and also give you the URL and the information.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Advanced Sparsity Techniques in Medical Imaging and Image Processing</strong><br /> Thursday, February 05, 2015<br /> Chen Chen<br /><p><a href="#" id="604-show" class="showLink" onclick="showHide(604);return false;">Read More</a></p></p><div id="604" class="more"><p><a href="#" id="604-hide" class="hideLink" onclick="showHide(604);return false;">Hide</a></p><p><strong>Abstract: </strong>In the past decades, sparsity techniques has been widely applied in the fields of medical imaging, computer vision, image processing, compressive sensing, machine learning etc., and gained great success. In this work, we propose new models of sparsity techniques, which is an extension to the standard sparsity used in the existing works and in the vein of structure sparsity families. First, we introduce the wavelet tree sparsity in natural images. It shows that the tree sparsity regularization often outperforms the existing standard sparsity based techniques in magnetic resonance imaging. Second, we extend the tree sparsity to forest sparsity on multi-channel data. A new theory is developed for forest sparsity, which is compared with the standard sparsity, tree sparsity and joint sparsity both empirically and theoretically. Motived by the special datasets in remote sensing, we propose a new sparsity model called dynamic gradient sparsity to improve the fusion results. Moreover, a novel model called deep sparse representation is investigated and successfully used in image registration. Finally, we propose a set of fast reweighted least squares algorithms for different optimization problems based on sparsity regularization.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Attempt to obfuscate keyword Based Censorship</strong><br /> Monday, December 08, 2014<br /> Ritu Rudragowda Patil<br /><p><a href="#" id="603-show" class="showLink" onclick="showHide(603);return false;">Read More</a></p></p><div id="603" class="more"><p><a href="#" id="603-hide" class="hideLink" onclick="showHide(603);return false;">Hide</a></p><p><strong>Abstract: </strong>Many countries block the content of web pages which are deemed against the morals, religious rules or policies set by government or organization. Countries like China block the post which is against their government interest.  Germany blocks contents related to neo-Nazi group. Most of these web pages are subjected to IP address blocking, DNS poisoning and keyword based filtering. We mainly focus on keyword filtering; it is fine grained filtering technique where the contents of web pages are filtered using blacklisting. So with increase in surveillance over network, arms race for circumvention techniques has also increased. We propose an application framework which provide you the obfuscation techniques without the use of any pseudo random payload or secure channel. It does not require any installation of software on client side to access blocked content nor requires any modification to internet topology.</p><p>This framework allows posting content on sites without being blocked by use of images or less common dictionary words. There is no trace of any random data or use of encryption or decryption of data packet. The traffic that is passed across the network is plain text without any cryptographic parameters, thus creating an obfuscation pattern which is not easily identified by censors. We performed within group-experiments to confirm if the users were able to adopt to this techniques.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Estimation Myopia</strong><br /> Friday, November 14, 2014<br /> Hazem Hasan Yassin<br /><p><a href="#" id="601-show" class="showLink" onclick="showHide(601);return false;">Read More</a></p></p><div id="601" class="more"><p><a href="#" id="601-hide" class="hideLink" onclick="showHide(601);return false;">Hide</a></p><p><strong>Abstract: </strong>The goal of this study is to explore an effective way to provide timely and accurate estimates for an enterprise data warehouse (EDW). With the exception of a few papers that attempted to adapt function point (FP) analysis to EDW, there was not much research in way of a comprehensive technique to estimate large EDW projects. Despite the generality of FP, they are not always easy to apply in an EDW environment to produce high level estimates. This thesis describes such a technique. Additionally, the thesis provides an overview of general estimating approaches, techniques, models, and tools.</p>  <p>This work presents a tool that is a custom built estimation utility that takes into account various nuances of an EDW. Some of these nuances include type of technology being built; build object complexity, data complexity, and source to target mappings. The utility then uses these components to estimate project effort, and at the same time, provides a common mechanism to communicate such mission-critical estimates to planning teams, delivery teams, managers, and software architects.</p>   <p>To evaluate the effectiveness of the tool, the research then shifts to a quantitative analysis section that compares the estimated numbers from multiple large-scale projects, with data from actuals.  Specifically, the analysis examines estimates produced by expert judgment techniques and then compares these estimates to ones produced by the estimation utility.</p>   <p>After that, the differences between the two data sets provide a foundation for some noteworthy statistical analysis and some thought-provoking comparisons of numerous behavioral drivers. Similarly, evaluating 3 large commercial EDW projects at a national airline, the tool predicted the actual project level of effort within ten to twenty percent accuracy.</p>     <p>Finally, the research concluded that with some recommendations on how the utility could provide useful estimates at different stages of early requirement gathering and likewise enhance these initial estimates as requirements were solidified overtime time.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A HMM-BASED PREDICTION MODEL FOR SPATIO-TEMPORAL TRAJECTORIES</strong><br /> Friday, November 14, 2014<br /> SAKTHI KUMARAN SHANMUGANATHAN<br /><p><a href="#" id="602-show" class="showLink" onclick="showHide(602);return false;">Read More</a></p></p><div id="602" class="more"><p><a href="#" id="602-hide" class="hideLink" onclick="showHide(602);return false;">Hide</a></p><p><strong>Abstract: </strong>Spatio-temporal trajectories are time series data that represent movement of an object over the time. Hidden Markov Models (HMM), a variant of Markov Models (MM), were first applied at a large scale to speech recognition but have also been used in time series prediction by analyzing trends in historical time series data. In this research, we propose a storm prediction model using a HMM built from overall storm trajectories derived by Kulsawasd et. al. This HMM is built by assuming the states are associated with clusters created by clustering the locations of each storm from the overall storm trajectories. Then we learn the variation of transitional information and spatial information present in the given set of trajectories using Baum-Welch algorithm. This learning is performed by building a HMM that for each cluster contains multiple state instances that represent this cluster and can learn to reflect variations in the information within a cluster. Results from experiments showed that the prediction gets better when the number of state instances representing each cluster increases. For example, the average distance value between actual location and location predicted by a model with 5 clusters and 5 state instances per cluster is approximately 15% smaller than the average distance value between actual location and location predicted by a model with 5 clusters and 3 state instances per cluster, which means that the predicted location gets closer to the actual location with more state instances. It was also found that the prediction gets better when the number of clusters increases. Apart from introducing a new prediction model for this type of data, we also contribute to work of Kulsawasd et. al. by proposing a modified algorithm that creates overall storm trajectories much faster than existing algorithms.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Cloud Based Automated Anomaly Detection Framework</strong><br /> Thursday, October 16, 2014<br /> Prathibha Datta Kumar<br /><p><a href="#" id="599-show" class="showLink" onclick="showHide(599);return false;">Read More</a></p></p><div id="599" class="more"><p><a href="#" id="599-hide" class="hideLink" onclick="showHide(599);return false;">Hide</a></p><p><strong>Abstract: </strong>A machine-to-machine (M2M) communications network hosts millions of heterogeneous devices such as for vehicle tracking, medical services, and home automation and security services. These devices exchange thousands of messages over cellular networks. These messages are Signaling System No. 7 (SS7) messages of various types like authentication, mobility management, and many more, resulting in tera bytes of SS7 signaling traffic data over a period of days. The data generated is diverse, depending on several factors like device activity, hardware manufacturers, and radio / tower interaction. This inherent diversity makes anomaly detection in a M2M network challenging. With millions of messages to analyze, high computation machines are necessary.<br /><br />In this thesis, an automated data mining framework on the cloud to detect anomalous devices in the traffic data is presented. Unsupervised learning is a useful tool here given the lack of static behavioral patterns of devices and numerous ways in which they can fail. Datasets extracted from a leading M2M service provider's network featuring millions of devices are analyzed. The case studies illustrate the anomaly patterns found. One case study identified 27% of devices with an unusual behavior in a dataset of 23k devices. A second case study spotted 0.07% of devices with anomalous behavior in a dataset with 350k devices. This places the spotlight on a small subset of devices for further investigation. In order to achieve the goal of finding anomalous devices, the clusters are labeled based on a few assumptions which are part of unsupervised anomaly detection and cluster analysis techniques. Then, k-nearest neighbors (k-NN) binary classification is applied to evaluate the labelling. This categorizes each device as either "anomalous" or "normal" with quantitative results like accuracy. To generate actionable information, the devices identified are analyzed in light of domain expertise, past events and auxiliary information like the type of the device and its purpose among others.???   </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Graph Embedding Discriminative Unsupervised Dimensionality Reduction</strong><br /> Thursday, October 02, 2014<br /> Yun Liu<br /><p><a href="#" id="598-show" class="showLink" onclick="showHide(598);return false;">Read More</a></p></p><div id="598" class="more"><p><a href="#" id="598-hide" class="hideLink" onclick="showHide(598);return false;">Hide</a></p><p><strong>Abstract: </strong>In this thesis, a novel graph embedding unsupervised dimensionality reduction method was proposed. Simultaneously, we assigned the adaptive and optimal neighbors on the basis of the projected local distances, thus we developed the dimensionality reduction along with the graph construction. The clustering results could be directly exhibited from the learnt graph which has the explicit block diagonal structure.<br /><br />The analysis of experimental result on different databases also determines that the proposed dimensionality reduction method is superior to other related dimensionality reduction methods, like PCA and LPP. In this study, we use synthetic data and real-world benchmark data sets. Also experimental results from the clustering experiments revealed the proposed dimensionality reduction method outperformed other clustering methods, such as K-means, Ratio Cut, Normalized Cut and NMF.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Linking Entity Profiles</strong><br /> Monday, July 14, 2014<br /> Ramesh Venkataraman<br /><p><a href="#" id="596-show" class="showLink" onclick="showHide(596);return false;">Read More</a></p></p><div id="596" class="more"><p><a href="#" id="596-hide" class="hideLink" onclick="showHide(596);return false;">Hide</a></p><p><strong>Abstract: </strong>Entity linking allows us to have collection of data from multiple sources as a global dataset and query those data. Entity linking allows us to do knowledge discovery on this global dataset which might result in the discovery of some interesting facts and information. Microsoft Academic Search is a free public search engine for academic papers and contains the bibliographic information for academic papers published in journals, conference proceedings and citations between them. As of February 2014, it has indexed over 39.9 million publications and 19.9 million authors. LinkedIn is the social networking service used for professional networking. LinkedIn has an estimated 259 million users all over the world. If we can link the author from MAS to the person from LinkedIn, we can have a much bigger dataset. This dataset would enable us to find more interesting measures about the author such as the author?s educational institutions, author?s previous work experiences and author?s social groups. We are effectively collecting the missing piece of information about an author from LinkedIn as part of forming a huge dataset. In this process, we are resolving the ambiguity of the multiple persons with the same name as the author and classifying them based on various features. Our experimental results indicate that we can attain a higher precision if we have a higher threshold.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Remapping Generalized Features to Equivalent Local Channel Sets for Knowledge Transfer</strong><br /> Wednesday, July 17, 2013<br /> Houtan Rahmanian<br /><p><a href="#" id="576-show" class="showLink" onclick="showHide(576);return false;">Read More</a></p></p><div id="576" class="more"><p><a href="#" id="576-hide" class="hideLink" onclick="showHide(576);return false;">Hide</a></p><p><strong>Abstract: </strong>In many real-world modeling applications, it is needed to detect the origin of the patterns of the input data in addition to find the patterns themselves. Having the input data generated by a systematically organized set of input channels is very common in these applications. These input channels might also be of the same type. Therefore, the same pattern might be observed on different sets of input channels of the data, while it is caused by the one source in different localities.     Sparse coding is a very powerful method for learning high-level patterns (i.e. high-level features) from raw data input. It is capable of learning an overcomplete basis which has the capacity to capture robust and discriminative patterns within the data. However, like many other feature learning algorithms, it is unable to detect identical features or stimuli on different sets of input channels. In this work, we propose a novel method to build general features that can be applicable to different sets of channels. This succinct representational model will express the stimuli independent of the locality in which they appeared. Simultaneously, different equivalent localities (equivalent channel sets) will be detected. As a result, when a feature is recognized on a channel set it can be transferred to the other equivalent channel sets. This enables the method to model and represent a pattern on localities where it has never been observed before.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Finger Detection and GUI Annotation Tool</strong><br /> Friday, July 12, 2013<br /> Sanjay Vasudeva Iyer<br /><p><a href="#" id="575-show" class="showLink" onclick="showHide(575);return false;">Read More</a></p></p><div id="575" class="more"><p><a href="#" id="575-hide" class="hideLink" onclick="showHide(575);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis presents a method for finger detection system. It is assumed that the user taps fingers on a table, and that the camera is placed on the same table in front of the fingers. This setup is motivated by the application of analyzing the movement of fingers in patients engaging in physical therapy. Fingers are detected in static images, which is a more challenging task than detecting and tracking fingers in videos, based on motion.  The Kinect sensor has been used as a source for data, and it provides color and depth images at each video frame.  Detection of fingers is performed using two different methods: Template Matching and Principal Component Analysis (PCA). Additional information present in the image, such as skin color and depth data, is used to improve accuracy and efficiency. The depth frames are used to separate the foreground from the background, and also to provide additional features for detecting the hands. A face detector is also utilized, and the position of face is used as a reference to determine where the hands are located.                  An additional contribution of the thesis is a graphical interface, developed in Matlab, for annotating finger positions. This tool provides abilities for users to load various sequences of images and annotate manually the position of fingers in those images. Using this tool, we have annotated a large number of video frames, and these annotations have been used for training and testing the proposed method. In addition, these annotations remain as a valuable resource for future research on finger detection and tracking. For testing purposes, the Matlab system also allows running the proposed method and measuring the accuracy of the results, based on the manual annotations. The thesis includes a comprehensive study on the effect of possible design decisions, as well as accuracy on user-dependent and user-independent settings.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Development and Simulation of Focus of Attention Using Reinforcement Learning and Function Approximation</strong><br /> Wednesday, April 17, 2013<br /> Stephen Ratz<br /><p><a href="#" id="571-show" class="showLink" onclick="showHide(571);return false;">Read More</a></p></p><div id="571" class="more"><p><a href="#" id="571-hide" class="hideLink" onclick="showHide(571);return false;">Hide</a></p><p><strong>Abstract: </strong>Without short-term memory, people would have little hope to learn and accomplish tasks.  The same can be said for artificially intelligent agents.  Often referred as Miller’s Law, the number of working objects that a human can hold in working memory is around sever. For an AI agent, the cost of keeping addition memory blocks is exponential.  Other issues to consider are what to keep in memory and for how long. Only a few of many of an agent’s previous steps may be important to hold on to.  This thesis project attempts to train an intelligent agent to learn what to hold onto in memory using Reinforcement Learning and Focus of Attention to accomplish a task.  Function Approximation is used to mitigate the memory requirements of a task as simple as block copying.  The concepts used in this thesis can be applied to any task that requires memory management.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Sign Gesture Spotting in American Sign Language using Dynamic Space Time Warping</strong><br /> Thursday, April 11, 2013<br /> Srujana Gattupalli<br /><p><a href="#" id="569-show" class="showLink" onclick="showHide(569);return false;">Read More</a></p></p><div id="569" class="more"><p><a href="#" id="569-hide" class="hideLink" onclick="showHide(569);return false;">Hide</a></p><p><strong>Abstract: </strong>American Sign Language (ASL) is the primary sign language used by approximately 500,000 deaf and hearing-impaired people in the United States and Canada. ASL is a visually perceived, gesture-based language that employs signs made by moving the hands combined with facial expressions and postures of the body. There are several software tools available online to learn signs for a given word but there is no software that gives the meaning of any sign video. If we need to search or look up documents containing any word we can just type it in search engines like Google, Bing etc. but if we need to search for videos containing a given sign we cannot do that by simply typing a word. One solution to this problem can be adding English tags to each of these ASL videos and do a keyword based search on it. This method can be inefficient as each sign video will need to be tagged manually with approximate English translations for each of the ASL gesture. The objective is to develop a system that lets users efficiently search through videos of ASL database for a particular sign. Given an ASL story dataset the user can use the system to know the temporal information (start and end frame) about the occurrence of the sign in the dataset. Recognizing gestures when the start and end frame of the gesture sign in the video is unknown is called Gesture spotting. The existing system evaluates the similarity between the query video and the sign video database using Dynamic Time Warping (DTW). DTW measures similarity between two sequences which may vary in time or speed. In this paper we have used a previously defined similarity measure called Dynamic Space Time Warping Algorithm (DSTW). DSTW was defined as an extension of DTW in order to deal with a more than one hand candidate by frame. This provides a method to find a better optimal match by relaxing the assumption of correct hand detection. This paper contributes by establishing a baseline method for measuring state of the art performance on this problem. We have performed extensive evaluations of DSTW on a real world dataset. We have achieved this by implementing a DSTW gesture spotting algorithm and evaluating it on a dataset built on ASL stories. We have used single handed queries from the ASL dictionary and used a symmetric rule for evaluation of the classification accuracy of our system for each of these queries.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A SOURCE CODE SEARCH ENGINE FOR KEYWORD BASED STRUCTURAL RELATIONSHIP SEARCH</strong><br /> Tuesday, March 26, 2013<br /> Asheq Hamid<br /><p><a href="#" id="567-show" class="showLink" onclick="showHide(567);return false;">Read More</a></p></p><div id="567" class="more"><p><a href="#" id="567-hide" class="hideLink" onclick="showHide(567);return false;">Hide</a></p><p><strong>Abstract: </strong>In an Object Oriented Program, we often see that a package contains several classes, a class contains several methods, a method calls other methods. We may say, there is a contains relationship between a package and a class or a calls relationship between two methods. We refer to these relationships as structural relationships. There may be other structural relationships apart from contains or calls in the source code. A software developer may sometime want to search for structural relationships within source code. She may prefer using Google like free form query to do so. To facilitate free form query based structural relationship search in the source code, we have proposed a code search system. Our system allows a user to type free form query and find structural relationships from the code repository relevant to that query. We consider an Entity Relationship Model where class, method etc. are entities and structural relations between entities are the actual relations. We construct a directed data graph taking each entity element as a node and each relation between elements as an edge. Each node in the data graph has a name (such as foo) and a type (such as method). Each edge has a type (such as calls). For each node, we construct a neighborhood document that lists the names and types of the nodes and types of the edges within edge length d from that node along with their respective distance score. The closer is the node or edge from the given node, the higher is the distance score. We use these neighborhood documents to construct inverted index and then do a Google like search. We find the nodes in the data graph where all the query keywords can be found in the neighborhood and order the nodes based on a simple ranking mechanism. To verify the effectiveness of our proposed code search model, we have implemented a prototype considering one structural relation. We have come up with some possible user queries and evaluated our system for each of those queries. Our experiment findings show that, our system is fairly successful in finding out the related structural relationships the user may be looking for.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Shaping Network Topology for Privacy and Performance</strong><br /> Wednesday, November 21, 2012<br /> Md Monjurul Hasan<br /><p><a href="#" id="560-show" class="showLink" onclick="showHide(560);return false;">Read More</a></p></p><div id="560" class="more"><p><a href="#" id="560-hide" class="hideLink" onclick="showHide(560);return false;">Hide</a></p><p><strong>Abstract: </strong>Low-latency anonymous communication systems provide privacy to Internet communication and are becoming popular. However, the users experience slow Internet because traffic paths in existing systems are selected randomly without considering latency between nodes. To improve traffic speeds while maintaining the privacy, it is significant to select suitable network topology and maintain the security properties of restricted-route topologies. In our work, we aim to design a better anonymous communication system with improved privacy and higher performance. We propose a network topology on top of stratified networks by efficiently reducing the number of links while maximizing throughput. First, we use Tabu search to build a latency-aware topology. Then we extend this approach by considering bandwidth constraint and propose a multi-link Stratified Restricted topology, with bandwidth capacity equally shared by each link. We use Reduced Overhead DLP scheme for padding and Bayesian Inference technique to measure anonymity of our topologies. We evaluate our system by running traces of real Tor traffic inside the network. We show that our topology provides considerable gain in performance with no loss in dummy traffic overhead and maintains reasonable levels of anonymity at the same time. We further compare our results with several restricted topologies based on bruteforce and greedy approaches.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A PREDICTIVE SCHEME FOR BLUETOOTH CONTACT PREDICTION IN OPPORTUNISTIC NETWORKS USING CONTACT PATTERNS</strong><br /> Wednesday, November 21, 2012<br /> Sujoy Kumar Bhattacharya<br /><p><a href="#" id="565-show" class="showLink" onclick="showHide(565);return false;">Read More</a></p></p><div id="565" class="more"><p><a href="#" id="565-hide" class="hideLink" onclick="showHide(565);return false;">Hide</a></p><p><strong>Abstract: </strong>In the opportunistic network(ON) paradigm information is exchanged between two devices as they encounter each other. For such information exchange to take place the devices must be aware of presence of other devices in the neighborhood.  A very fundamental problem in ON is predicting future opportunistic contacts in the dynamic and uncertain environments. An accurate predictor which takes into account the long time history can benefit from multiple objectives. Such a predictor switches to the data transfer mode in anticipation of a contact. Also  it maximizes the number of opportunistic contacts while spending minimal energy.     In this thesis, we have designed a novel scheme called K-Fold Predictor (KFP) and evaluated it using data mining methodologies to accurately predict opportunistic contacts. For evaluation of our scheme, we have used the Bluetooth traces collected by University of Illinois at Urbana Champaign movement (UIM) framework using Google Android phones for a period of 3 weeks. Extensive simulation of our scheme using these real life traces show that the precision and recall values are close to 40% higher compared to the previous schemes. Also the energy usage, is 25% lower for KFP making it an attractive option for predicting opportunistic contacts, to obtain efficient routing as well as swift  information dissemination in ONs.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>UERYING ENTITY-RELATIONSHIP GRAPHS BY EXAMPLE TUPLES: EXPERIMENTAL EVALUATION AND USER STUDY</strong><br /> Monday, November 19, 2012<br /> Mahesh Gupta<br /><p><a href="#" id="559-show" class="showLink" onclick="showHide(559);return false;">Read More</a></p></p><div id="559" class="more"><p><a href="#" id="559-hide" class="hideLink" onclick="showHide(559);return false;">Hide</a></p><p><strong>Abstract: </strong>he World Wide Web today has evolved into a rich repository of entities where many knowledge bases containing entity-related information are directly available. Such knowledge bases are often in the form of entity-relationship graphs. To query entity-relationship graphs, users need to provide input entities, attributes and relationships by complex query graphs. To improve the usability of graph database systems, we study a novel mechanism that queries entity-relationship graphs by example tuples. It allows users to express a query in the form of one or more tuples consisting of entities. The underlying query system automatically builds a query graph based on the example tuples and ranks matching answer tuples.   The focus of this thesis is to evaluate our query system&#039s accuracy and efficiency. To evaluate accuracy we employ two methods. In the first method we evaluate queries whose ground truths are known and calculate system?s precision and recall. In the second method we conduct user study on ranked answer lists and calculate rank correlation co-efficient. The run time efficiency of the system is measured with respect to the size of the query graph.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Development of a detached vital sign home monitoring management system</strong><br /> Monday, November 19, 2012<br /> John Anderson<br /><p><a href="#" id="563-show" class="showLink" onclick="showHide(563);return false;">Read More</a></p></p><div id="563" class="more"><p><a href="#" id="563-hide" class="hideLink" onclick="showHide(563);return false;">Hide</a></p><p><strong>Abstract: </strong>Patients coping with chronic illnesses are often inconvenienced with routine doctor visits for the sole purpose of conducting basic vital sign monitoring. Such visits tend to be disruptive, and are more so when the patient requires a lengthy commute or has a mobility disability. As technology progresses, advances in low powered integrated circuits and network connectivity has opened the doors to a new world of remote patient monitoring. This thesis explores the possibility of using low powered technology to develop a patient monitoring system that’s completely detached, thus allowing patients to move around their homes while the monitoring continues. We discuss the development of such a system with various monitoring subsystems incorporated into the device. This system, being modular and flexible, will enable any configuration of monitoring subsystems to be present.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Shaping and Maintenance of Low-Latency Anonymity Systems</strong><br /> Monday, November 19, 2012<br /> Harsha Doreswamy<br /><p><a href="#" id="564-show" class="showLink" onclick="showHide(564);return false;">Read More</a></p></p><div id="564" class="more"><p><a href="#" id="564-hide" class="hideLink" onclick="showHide(564);return false;">Hide</a></p><p><strong>Abstract: </strong>With the growing Internet, network surveillance on users also continues to grow. This makes it increasingly important to protect users’ identity, especially for users like journalists, whistleblowers, and military officials. Anonymity systems can conceal users’ identities and provide anonymity for their online activities. Low-latency anonymity systems like Tor are designed to provide support for applications like Web browsing, video streaming, and online chat. To make the commutation between source and destination unlinkable, Tor routes all the traffic through three Tor relays (routers), which are spread across the globe. These three relays are usually chosen uniformly at random. If the relays selected are located too far from each other, it could reduce the performance of the system. One of biggest challenges in low-latency anonymity systems is maintaining the balance between anonymity and performance. In this thesis, we try to improve the performance of Tor-like low-latency anonymity systems, with little or no effect on anonymity. The current version of Tor tries to improve the performance of the system in the relay selection process by biasing towards bandwidth. But the performance gain from bandwidth biasing alone is not sufficient. To increase the performance, we built restricted network topologies (arrangement of nodes and links of the network in particular structure) with Tor relays, through which Tor traffic is routed. While building these restricted topologies, we bias the construction towards properties like latency and bandwidth to increase the performance. And restricted topologies also provide cover traffic for users, which improve the anonymity as well. We propose two network topologies, the expander graph topology and a clustering model topology. In the expander graph topology, we construct an expander graph with Hamiltonian cycles, where the graph edges are biased towards latency which is similar to prior work of Mallesh et al. In the clustering model topology, we make clusters of relays based on bandwidth and then the nodes selection is done in two parts: first, the number of nodes per cluster is calculated based on the current node’s bandwidth multiplied by the ratio of the bandwidth of cluster to the total bandwidth of all nodes. Then based on the number of nodes calculated, that many numbers of nodes are selected from that cluster by biasing towards the latency. This allows us to consider both the latency and the bandwidth while constructing the topology. The biggest issue with network topologies in real-time system is maintaining the structure and properties of the topology despite of the churn operation (the process of node joining and leaving the network). We have shown that our topologies, of size 750 nodes, have maintained the structure and properties of the topology up to 25000 churn operations, without much change to the anonymity and the performance of the initial topology. We also compare the anonymity and the performance of the expander graph topology versus the clustering model topology, on various parameters such as variable node degree of the graph, latency biasing values, and number of churn operations.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Implementation of a Middleware Architecture for Collaboration among Mobile Devices in Opportunistic Networks</strong><br /> Monday, November 19, 2012<br /> SAMAYA MADHAVAN<br /><p><a href="#" id="566-show" class="showLink" onclick="showHide(566);return false;">Read More</a></p></p><div id="566" class="more"><p><a href="#" id="566-hide" class="hideLink" onclick="showHide(566);return false;">Hide</a></p><p><strong>Abstract: </strong>Opportunistic networks provide a viable platform for mobile communications due to the ubiquity of smart phones. These networks are characterized by lack of end to-end reliable connections. In such networks, establishing collaboration among nodes is a challenge. In this thesis, a middleware architecture for opportunistic communication and collaboration has been designed, developed, and implemented. The middleware architecture supports a protocol for efficiently exchanging information between mobile nodes during an opportunistic contact.  Message formats for differentiating the various kinds of messages that will be transferred across the network are defined to maintain consistency and reduce redundancy. The architecture consists of several modules equipped with system software to differentiate, compute, update and merge information acquired at each participating node. The middleware has been developed with a view to support a variety of application services on opportunistic networks. In particular, the middleware performs service composition utilizing basic services available in different devices. A sample language translation application has been implemented involving several android devices to test service composition in an opportunistic network created among mobile devices. Finally experimental results are carried out to measure success rates of service composition. The experimental studies include results with the following scenarios: parallel service requests; varying service composition lengths; varying content size; varying number of nodes.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Prominent Streaks Discovery on Blog Articles</strong><br /> Wednesday, November 14, 2012<br /> Jijo John Philip<br /><p><a href="#" id="561-show" class="showLink" onclick="showHide(561);return false;">Read More</a></p></p><div id="561" class="more"><p><a href="#" id="561-hide" class="hideLink" onclick="showHide(561);return false;">Hide</a></p><p><strong>Abstract: </strong>We are surrounded by data in various forms such as instant messages, Twitter tweets, Facebook status updates, news, media, blogs and much more.  Extracting meaning from such A massive collection of unstructured data would lead to interesting stories.  Examples of such stories can be ?Who was the most popular actor in a particular month? or ?Which diseases Were people most concerned about in year 2008?.  In this thesis, we propose to discover popular entities mentioned in blog articles based on the concept of prominent streak.  Given A sequence of values for a named entity (e.g., a person, a place, etc.), where each value is the occurrence frequency of the entity in blog articles during a corresponding period of time, A prominent streak is a long consecutive subsequence of only large (small) values.  Whether a streak is prominent also depends on how it fares against streaks for comparable entities. Using the distributed data processing framework Mapreduce, particularly Hadoop which is one of its open-source implementations, we find entity occurrences in a set of blog articles With a trie-based data structure.  Prominent streak discovery algorithms are applied over the detected sequences of entities occurrences to derive interesting stories.  Our experiments and evaluation are done over the ICWSM?09 Spinn3r blog dataset, which contains over 44 million blog articles for the months of August and September in 2008.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>INTERPRETATION OF REVIEWER FEEDBACK ON SOCIAL WEB</strong><br /> Friday, November 09, 2012<br /> Shrikant Desai<br /><p><a href="#" id="557-show" class="showLink" onclick="showHide(557);return false;">Read More</a></p></p><div id="557" class="more"><p><a href="#" id="557-hide" class="hideLink" onclick="showHide(557);return false;">Hide</a></p><p><strong>Abstract: </strong>The increasing popularity of social media web sites such as Amazon, Yelp and others has influenced our online decision making. ?Before making selection decisions on movies and restaurants, we investigate its reviewer feedback.? Social media web sites provide reviewer feedback in the form of ratings, tags, and user reviews. However, overwhelming feedback details will leave the user in a quandary as to decide whether the item is desirable or not. Potential buyers either make a snap judgment based on the aggregate ratings/tags or spend a lot of time in reading reviews. In this thesis, we build a system that can analyse the reviewer feedback in the form of ratings or tags and generate meaningful interpretations. One of major component is rating interpretation that generates meaningful interpretation of the reviewer ratings associated with the item of interest. For example, given the movie &quot;Titanic&quot;, our system returns results such as, &quot;Young female from California like this movie&quot; instead of average rating 7.6 from all reviewers. Furthermore, end users will be allowed to systematically explore, visualize, and observe rating patterns. Additionally, our system can also explore the social tagging behavior on the input items. For example, our system can identify movies where similar users have assigned similar tags on diverse items. The tagging behavior of different sub population is compared using tag clouds. We use IMDb movie data set to demonstrate our experiments.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SUBROSA 2 : AN EXPERIMENTAL EVALUATION OF TIMING ANALYSIS ATTACKS AND DEFENSES IN ANONYMITY SYSTEMS</strong><br /> Monday, August 06, 2012<br /> Payap Sirinam<br /><p><a href="#" id="553-show" class="showLink" onclick="showHide(553);return false;">Read More</a></p></p><div id="553" class="more"><p><a href="#" id="553-hide" class="hideLink" onclick="showHide(553);return false;">Hide</a></p><p><strong>Abstract: </strong>A circuit-based low-latency anonymous communication service such as Tor helps  Internet users hide their IP addresses and thereby conceal their identities when communicating online. However, this kind of service is vulnerable to timing analysis attacks that can discern the relationship between incoming and outgoing messages in order to find the correlation of them. The attacker can use this information to reveal the identity of users without essentially knowing the IP addresses concealed in the anonymous communication services.  Dependent link padding(DLP) is a scheme proposed to enable anonymity systems to resist these attacks. However, DLP adds high dummy packets overhead in the network systems, resulting in poor quality of service.  We have developed a Tor-like experimental evaluation platform for studying and investigating the overall dummy packets overhead on each scheme that is used to prevent timing timing analysis attacks. We have developed our platform on real distributed networks by using the DETERLab network testbed, which is a public facility for medium-scale repeatable experiments in computer security. In our experiments, we evaluated DLP and reduced overhead dependent link padding(RO-DLP).  Furthermore, We compared these schemes to a recently-proposed technique called selective grouping(SG) that aims to further reduce overheads in dummy packets padding algorithms at the cost of some anonymity.  Through evaluations of the whole anonymity systems, we validated that RO-DLP could significantly reduce dummy packet overhead and enable large numbers of users to be protected from timing analysis attacks in comparison to DLP implementation. Besides, we also showed that SG could practically reduce the network overhead with lower ratio of dummy packets overhead reduction than the previous work proposed. We also deeply investigated the factors and causes to explain the lower ratio of reduction when we implemented SG on the real distributed networks. Furthermore, we performed the partial implementation of SG on only mix nodes to compare the results with full implementation of SG. Finally, we showed that SG could enable larger numbers of users participated in the systems when compared with DLP and RO-DLP without SG.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>MEASUREMENTS OF A LATENCY-BIASED EXPANDER TOPOLOGY IN THE TOR ANONYMITY SYSTEM</strong><br /> Monday, August 06, 2012<br /> Subhasish Dutta Chowdhuri<br /><p><a href="#" id="554-show" class="showLink" onclick="showHide(554);return false;">Read More</a></p></p><div id="554" class="more"><p><a href="#" id="554-hide" class="hideLink" onclick="showHide(554);return false;">Hide</a></p><p><strong>Abstract: </strong>Anonymous communication systems protect the privacy of their users by hiding who is communicating with whom. With the widespread use of the Internet, anonymity systems are all the more essential to support applications having strong privacy requirements such as intelligence gathering, military communications, or e-voting protocols. Anonymity systems must balance security and performance to remain popular with their users. In this work, we perform measurements on anonymity systems to improve their performance. We use the Vivaldi network coordinate system to efficiently map out the relative delays between hosts. Using this data, we create an overlay expander network topology that is biased to use lower latency links instead of randomly selecting nodes. Our primary contribution is the design and execution of a set of experiments to evaluate the performance of this approach. These experiments are performed using a private deployment of Tor, a popular anonymity system, running on PlanetLab, a globally distributed testbed. Our testbed is comprised of 100 Tor relay nodes, five trusted directory servers and 10 geographically distributed clients, with each of the relays running a common implementation of Vivaldi to compute its virtual coordinates and reporting the same to a trusted directory server. The directory server uses this information to construct an expander graph topology with a bias towards faster links. We show that when the network topology is created with a bias towards lower latency edges, there is a significant improvement in performance compared to using random links on our topology.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Hydrological Visualization and Analysis System and Drought Related Feature Selection Based on Sectional Correlation Measurement</strong><br /> Thursday, July 19, 2012<br /> Piraporn Jangyodsuk<br /><p><a href="#" id="551-show" class="showLink" onclick="showHide(551);return false;">Read More</a></p></p><div id="551" class="more"><p><a href="#" id="551-hide" class="hideLink" onclick="showHide(551);return false;">Hide</a></p><p><strong>Abstract: </strong>Because of the larger data storage and the faster computational power, computer can process and store much finer resolution data. Aside from data analysis, data visualization is also an important task to understand the data. In this work, the Hydrological Visualization and Analysis System is developed to help both hydrologists and local people view and examine the high resolution hydrological data. Then, this data is analyzed to determine which variables are related to the change of drought condition in Arlington, Texas. A new correlation measurement method called sectional correlation is proposed and used as an objective function of the drought related feature selection. The proposed sectional correlation algorithm has good performance in terms of computational efficiency and the accuracy. The result error is quite low, about 2% of the range of value.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Reducing the Complexity of Reinforcement Learning in POMDPs by Decomposition it into Decision and Perceptual Processes.</strong><br /> Thursday, July 12, 2012<br /> Rasool Fakoor<br /><p><a href="#" id="550-show" class="showLink" onclick="showHide(550);return false;">Read More</a></p></p><div id="550" class="more"><p><a href="#" id="550-hide" class="hideLink" onclick="showHide(550);return false;">Hide</a></p><p><strong>Abstract: </strong>Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) are very powerful and general frameworks to model decision and decision learning tasks in a wide range of problem domains. As a result, they are widely used in complex and real-world situations such as robot control tasks. However, the modeling power and generality of the frameworks comes at a cost in that the complexity of the underlying models and corresponding algorithms grows dramatically as the complexity of the task domain increases. To address this , this work presents an integrated and adaptive approach that attempts to reduce the complexity of the decision learning problem in partially Observable Markov Decision Processes by separating the overall model into decision and perceptual processes.  The goal here is to focus the decision learning on the aspects of the space that are important for decision making while the observations and attributes that are important for estimating the state of the decision process are handled separately by the perceptual process. In this way, the separation into different processes can significantly reduce the complexity of decision learning. In the proposed framework and algorithm, a Monte Carlo based sampling method is used for both the perceptual and decision processes in order to be able to deal efficiently with continuous domains. To illustrate the potential of the approach, we show analytically and experimentally how much the complexity of solving a POMDP can be reduced to increase the range of decision learning tasks that can be addressed.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Experiment in Developing Small Mobile Phone Applications Comparing On-Phone to Off-Phone Development</strong><br /> Monday, July 09, 2012<br /> Tuan Anh Nguyen<br /><p><a href="#" id="549-show" class="showLink" onclick="showHide(549);return false;">Read More</a></p></p><div id="549" class="more"><p><a href="#" id="549-hide" class="hideLink" onclick="showHide(549);return false;">Hide</a></p><p><strong>Abstract: </strong>TouchDevelop represents a radically new mobile application development model, as TouchDevelop enables mobile application development on a mobile device. I.e., with TouchDevelop, the task of programming say a Windows Phone is shifted from the desktop computer to the mobile phone itself. We describe a first experiment on independent, non-expert subjects to compare programmer productivity using TouchDevelop vs. using a more traditional approach to mobile application development.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>CONTROL FRAMEWORK FOR DYNAMIC WALKING OF A BIPEDAL ROBOT</strong><br /> Wednesday, May 09, 2012<br /> Kishan Prabhakar<br /><p><a href="#" id="547-show" class="showLink" onclick="showHide(547);return false;">Read More</a></p></p><div id="547" class="more"><p><a href="#" id="547-hide" class="hideLink" onclick="showHide(547);return false;">Hide</a></p><p><strong>Abstract: </strong>Despite significant progress in recent years, the capabilities of today?s Humanoid robots still lack behind the walking abilities of humans in terms of competence, robustness, flexibility, and speed. Furthermore, unknown environmental conditions and related constraints imposed on the robot significantly increase the complexity of locomotion control and decision making for such systems, easily making planning-based approaches intractable. Part of the reason for this is that the control of a humanoid encompasses observation, processing the observed data, and decision making in terms of locomotion gait and body pose parameters. This key information derived from observations and internal robot information finally allows calculating the appropriate signals to be fed to the actuators which in turn move and adjust the mechanical joints with respect to the environment. The complexity of biped walking is also driven by the kinematic structure of the robot. If the robot has a large degree of freedom, the parameters that can be used to affect the robustness of the robot increase along with the number of controls. This, in turn, can lead to a significant increase in computation cost in monolithic control approaches that compute gait and control for the entire kinematic mechanism.  As a contribution towards the objective of developing useful walking machines, the work presented in this thesis takes a modular approach to locomotion control where the overall control task is decomposed into elements with individual subtask responsibilities. The goal here is to break the overall complexity into manageable parts by relying on the robustness and reactivity of the other modules. This thesis presents a basic overview of this approach and then focuses on the development of the parts of this approach centered around flexible gait generation. In this part it focuses on modules that address very specific problems of walking such as permitting dynamically changing step lengths, stepping frequencies, height of the body, and stance stability during the walk cycle, in order to adjust itself to the environment, prevent it from falling down, and address foothold and pace parameters provided by higher-level, environment-dependent modules. This thesis proposes a control framework that stabilizes a humanoid robot while these characteristics of the walk are changed. In the modules developed to achieve this, methods such as position control, flexible walking pattern generation using parametric trajectories, and zero-moment control for reactive stabilization are used to generate a dynamically walk. The resulting controller is demonstrated using a simulated humanoid model taking into account the natural dynamics, torque limits and the model of the walking surface.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Comparing web application scanners for XSS attacks</strong><br /> Tuesday, May 08, 2012<br /> Dengfeng Xia<br /><p><a href="#" id="546-show" class="showLink" onclick="showHide(546);return false;">Read More</a></p></p><div id="546" class="more"><p><a href="#" id="546-hide" class="hideLink" onclick="showHide(546);return false;">Hide</a></p><p><strong>Abstract: </strong>As industry is paying increasing attention to web application security, various testing tools with black box testing feature have been developed. To better evaluate their performance, researchers have made efforts in several ways, and most projects only compare about final test results and draw conclusions. In this paper, we evaluate 4 tools mostly, and we are trying to not only compare their performance, but also find out the reasons causing their differences and propose our suggestions. First we use real life vulnerable web application to evaluate tools? performance in different testing phases, including crawling. Then we use JSP test cases we controlled to focus on testing their ability of sending fuzzed data and analyzing response. At last we try to explain their performance differences by comparing their injection patterns. Our test results indicate that their performance differences in various phases have influenced their final test results. However, the performance of crawling does not seemed to be a key factor, which is different from conclusions of many related work. Our deeper study about injection patterns suggest that all scanners have certain variety of their injection patterns, and their final detection ability may result from multiple factors.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Selective Grouping Algorithm for Low Latency Anonymous Systems</strong><br /> Monday, April 30, 2012<br /> Vishal Gupta<br /><p><a href="#" id="545-show" class="showLink" onclick="showHide(545);return false;">Read More</a></p></p><div id="545" class="more"><p><a href="#" id="545-hide" class="hideLink" onclick="showHide(545);return false;">Hide</a></p><p><strong>Abstract: </strong>Low latency anonymous communications are prone to timing analysis attacks. It is a technique by which the adversary can de-anonymize the user by correlating packet timing patterns. A recent proposal to stop these attacks is called Dependent Link padding. However, it creates high dummy packets overhead in the network. In this work we propose selective grouping, a padding scheme that protects users in an anonymity system from those attacks with minimal overhead. The aim is to decrease overhead by dividing users in different groups while maintaining good anonymity. The key idea of our approach is to group clients with similar timing patterns together by providing a strict delay bound. We ran simulation experiments to test the effectiveness of these techniques and to measure the amount of extra network congestion. We have also statistically analyzed bursty traffic in the network by using the mean and standard deviation of inter packet delays over a fixed duration. The result of bursty traffic analysis added one more dimension to the count of packets for grouping clients efficiently. To analyze anonymity, we ran a statistical disclosure attack against our selective grouping defense. We performed extensive sets of experiments to find a threshold value at which selective grouping achieves good profiling without adding excess dummy packets. We show that selective grouping is very effective at resisting timing analysis attacks and are still able to provide good anonymity with minimal overhead added to the network.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Dynamic cache reconfiguration for energy optimization in chip multiprocessors</strong><br /> Monday, April 23, 2012<br /> Gaurav Puri<br /><p><a href="#" id="544-show" class="showLink" onclick="showHide(544);return false;">Read More</a></p></p><div id="544" class="more"><p><a href="#" id="544-hide" class="hideLink" onclick="showHide(544);return false;">Hide</a></p><p><strong>Abstract: </strong>Cache reconfiguration for chip multiprocessors (CMP) is quite unexplored. This thesis presents a dynamic heuristic and L1 cache design where the processor can, through the use of simple on-chip monitoring hardware, dynamically switch to a lower energy configuration. The heuristic can either be run as part of the firmware or implemented in hardware with very low additional overhead. After every pre-defined fixed interval for each processor core, the heuristic examines the statistics calculated over previous intervals and switches the cache to a more energy-optimal configuration if it determines so. This approach has been evaluated extensively by running benchmarks from the PARSEC 2.1, MiBench and SPLASH-2 benchmark suites on a simulated 4-core CMP using a modified Multi2Sim 3.2.1 simulator and has been observed to provide on average a 16.92% savings in energy consumption with only a 3.01% reduction in IPC compared to a regular 16KB 4-way L1 cache.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Point-of-Regard Aware User Interfaces</strong><br /> Thursday, April 19, 2012<br /> Jonathan Rich<br /><p><a href="#" id="540-show" class="showLink" onclick="showHide(540);return false;">Read More</a></p></p><div id="540" class="more"><p><a href="#" id="540-hide" class="hideLink" onclick="showHide(540);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis presents a new, publicly available eye tracking dataset, aimed to be used as a benchmark for Point-of-Regard detection algorithms.  Two hardware systems are also presented, a low-cost solution for real-time tracking of a human user?s head position with respect to a video display source for eye gaze estimation, and a low-cost solution for object recognition within a 3D environment. The dataset consists of a set of videos recording the eye motion of human test subjects as they were looking at, or following, a set of predefined points of interest on a computer visual display unit. The eye motion was recorded using a Mobile Eye XG, head mounted, infrared monocular camera. The ground truth of the point of gaze and head location and direction in the three dimensional space are provided together with the data. The ground truth regarding the point of gaze at is known in advance since the subjects are always looking at predefined targets, whereas, the head position in 3D is captured using a Vicon Motion Tracking System. The solution utilizes a wearable headset equipped with sensors found in commercially available off-the-shelf video gaming devices in order to minimize hardware complexity and expense. A pair of Nintendo Wii remote imaging sensors are used to create a stereo camera for 6DOF position tracking of the headset, while a modified Playstation Eye monocular camera is used to track the pupil position.  When interacting with objects within a 3D environment, the Nintendo Wii remote imaging sensors are replaced with an Asus Xiton Pro sensor. The resulting tracking hardware is able to measure the 3D position of four infrared LEDs mounted at known locations on the video display using triangulation of the stereo camera data. Once the marker positions are obtained in the stereo camera perspective, the rotation and translation of the sensing headset relative to the display is estimated using a least-squares solution technique. This information, along with a priori knowledge of the necessary coordinate frames, can be used to estimate the intersection of the user?s eye gaze vector with the display surface. When using the Asus Xiton Pro, a point cloud containing RGB-XYZ data is retrieved, and object segmentation can be performed.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Choices and Consequences: A Game-Based Risky Behavior Prevention Program</strong><br /> Wednesday, April 18, 2012<br /> Amanda Vines<br /><p><a href="#" id="543-show" class="showLink" onclick="showHide(543);return false;">Read More</a></p></p><div id="543" class="more"><p><a href="#" id="543-hide" class="hideLink" onclick="showHide(543);return false;">Hide</a></p><p><strong>Abstract: </strong>Serious games have been defined as a mental contest played with a computer in accordance to specific rules that use entertainment to further government or corporate training, education, health, public policy and strategic communication objectives. There have been various studies done to gauge the effectiveness of serious games to teach and engage. In collaboration with professors in social work and computer science, we have designed a multiplayer, social, serious game called Choices and Consequences (C&amp;C) in an effort to better reach the audience of pre-teens and teens in order to reduce the risk taking behaviors of youth today. The game specifically targets the subjects of healthy relationships, conflict management, and giving and receiving advice from peers and adults. The game was created with a game engine, made to run on Android tablets, and incorporates such technologies as an Apache server, a social network, XML files, and MYSQL database. C&amp;C was tested at a local high school with 40 students playing the game and resulted in showing the efficacy of game implementation in the classroom. Through competition, teamwork and peer and teacher interaction, C&amp;C was designed to teach students to think before they act, evaluate their options and that it is possible to live a healthy and risk-free lifestyle while still having fun in their daily lives.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Communicator 2.0</strong><br /> Tuesday, April 17, 2012<br /> Shekhar Menon<br /><p><a href="#" id="539-show" class="showLink" onclick="showHide(539);return false;">Read More</a></p></p><div id="539" class="more"><p><a href="#" id="539-hide" class="hideLink" onclick="showHide(539);return false;">Hide</a></p><p><strong>Abstract: </strong>XYZ Company is in the business of providing IT based software consulting services to its corporate clients. Its activities include providing software implementation services, custom software development, training and recruitment. In today?s world the service requested by the clients is vastly increasing. Few years back the client was only involved at the end after the application development. These days due to the ever growing IT market and quality products available in the market, the clients want to be involved in the process from the word go . Here the concept of Remote Desktop access comes into picture.  To meet this challenge Communicator 2.0 is developed. It is basically a virtual desktop application. It is a simple, less bloated, less featured and remotely?accessible operating environment. It delivers a rich desktop-like experience, coupled with various built-in applications.?In theory, this means that wherever you went, you?d be able to access your work through a common interface and set of tools. All with a single login, too. The biggest advantage of this application is that one can remotely control any PC anywhere on the Internet. No installation is required, just run the application on both sides and connect.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>PushPop: A Card Game</strong><br /> Monday, April 16, 2012<br /> Vishal Subramani<br /><p><a href="#" id="537-show" class="showLink" onclick="showHide(537);return false;">Read More</a></p></p><div id="537" class="more"><p><a href="#" id="537-hide" class="hideLink" onclick="showHide(537);return false;">Hide</a></p><p><strong>Abstract: </strong>In this thesis we have designed and implemented an Android card game called PushPop. This game is meant to stimulate the interest of kids in math learning. The Android game has cards with mathematical signs instead of the traditional suits. To reach the goal kids have to perform a lot of mathematical operations which helps them learn maths in a fun and effective way. According to game theory concepts this game is a non-co-operative, asymmetric, non-zero sum, simultaneously, imperfect information. All these properties make this game very interesting for research.?We have designed and implemented 4 levels of the game with respect to the computer as a player. The simulation results include the win loss ratio of each level against the other as well as the win loss ratio of human against each level. Through these simulations, we conclude that level 4 is the most optimal approach out of the 4 levels followed by level 3, level 2 and finally level 1.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Automatic Scheduler</strong><br /> Thursday, April 12, 2012<br /> Amey Bhalerao<br /><p><a href="#" id="534-show" class="showLink" onclick="showHide(534);return false;">Read More</a></p></p><div id="534" class="more"><p><a href="#" id="534-hide" class="hideLink" onclick="showHide(534);return false;">Hide</a></p><p><strong>Abstract: </strong>XYZ Software is a company based in the east coast and now has offices in Las Vegas and Olympia. XYZ provides transit management software for Para - Transit services. The software features focus on recording, reporting, automating and collecting data surrounding the activities of providing transportation service to passengers who are unable to use traditional public transit. XYZ?s customers are transit managers dealing with Para ? Transit service. Their duties include coordinating transportation providers (i.e. bus fleets, taxi, volunteers) reporting to transit funders and managing transportation requests for their passenger clients. Many of these agencies act as brokers or call centers, taking passenger requests and working to fill the clients? needs by assigning the request to the appropriate provider. Several of the agencies manage programs that are funded with federal dollars or insurance and they must track performance indicators and file reports.  The work to be presented includes a different approach for scheduling rides for the clients keeping in mind customer satisfaction and company revenue.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>On Feasibility of Fingerprinting Wireless Sensor Nodes Using Physical Properties</strong><br /> Friday, April 06, 2012<br /> Xiaowei Mei<br /><p><a href="#" id="536-show" class="showLink" onclick="showHide(536);return false;">Read More</a></p></p><div id="536" class="more"><p><a href="#" id="536-hide" class="hideLink" onclick="showHide(536);return false;">Hide</a></p><p><strong>Abstract: </strong>Fingerprinting wireless devices using physical properties has been recently suggested as an alternative for device identification and authentication. It has been found that the clock skew caused by the frequency discrepancy of the quartz crystals in different devices can be used as a reliable source for fingerprinting. Researchers have studied the application of the clock skew-based fingerprinting in sensor networks and claimed that it can detect fake identities, wormholes, and node replicas. However, the study in this paper draws a completely opposite conclusion, i.e., the clock skew of sensor nodes can be easily forged by adversaries to evade the detection. This paper then studies the feasibility of using the distribution of signal power in space to fingerprint sensor nodes. The result shows that a sensor node&#039s signal power distribution in space is not only reliable for being used as a source for fingerprinting but also very hard to forge. Finally, the paper discusses the application of using signal power distribution for detecting various attacks as well as the limitations and open problems.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SYBILINFER WITH PARTIAL GRAPH INFORMATION</strong><br /> Friday, November 11, 2011<br /> Vritant Naresh Jain<br /><p><a href="#" id="533-show" class="showLink" onclick="showHide(533);return false;">Read More</a></p></p><div id="533" class="more"><p><a href="#" id="533-hide" class="hideLink" onclick="showHide(533);return false;">Hide</a></p><p><strong>Abstract: </strong>Online social networks (OSNs) today are proprietary, in the sense that communication between users requires the users to be part of the same OSN. This raises privacy issues and reliability concerns among users, and calls for an open, interoperable, and distributed OSN infrastructure that is similar to email and would link different OSNs together. Any decentralized system, however, is vulnerable to Sybil attacks, in which an attacker claims multiple identities, called Sybils, to overwhelm the OSNs and defeat standard techniques used to protect against attacks such as message spam.  The state of the art defense against these attacks is SybilInfer, which utilizes the fast mixing property of social networks to distinguish between Sybil nodes and honest nodes. SybilInfer, however, assumes a centralized system with a complete view of the social network. In this thesis, we investigate the effectiveness of applying SybilInfer on open and decentralized networks, and we propose improvements that would make SybilInfer deployable in such a scenario. These improvements facilitate a user of one OSN to listen to messages from other users of another OSN without the fear of spam due to a Sybil attack. We show that the proposed improvements greatly reduce the number of Sybil nodes misclassified as honest users and make SybilInfer more accurate in classifying members of other OSNs.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>AUTOMATIC DISCOVERY OF SIGNIFICANT EVENTS FROM DATABASES</strong><br /> Tuesday, July 26, 2011<br /> Avinash Bharadwaj<br /><p><a href="#" id="528-show" class="showLink" onclick="showHide(528);return false;">Read More</a></p></p><div id="528" class="more"><p><a href="#" id="528-hide" class="hideLink" onclick="showHide(528);return false;">Hide</a></p><p><strong>Abstract: </strong>The advent of the internet has caused enormous amounts of data available online causing many significant facts to be hidden within this data. Searching for a significant fact within these large datasets is a query intensive process involving large amounts of queries which needs to be executed hence slowing the process of finding the significant facts from a large dataset. In this thesis, a novel approach has been designed exploiting the mathematical characteristics of the data present in the dataset to reduce the number of queries on the dataset. A two phased approach is considered for making fact finding more efficient. The approach consists of design and implementation of the prediction and the decision making algorithm. The prediction algorithm predicts the time frame for a significant event to happen and the decision algorithm uses the results from the prediction algorithm to decide whether to check for a significant event or not. We compare our results obtained after the implementation of the designed algorithms and found that queries are executed lesser number of times compared to the other existing solutions to this problem.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Delay Tolerant Lazy Release Consistency for Distributed Shared Memory in Opportunistic Networks</strong><br /> Friday, July 15, 2011<br /> Chance Eary<br /><p><a href="#" id="527-show" class="showLink" onclick="showHide(527);return false;">Read More</a></p></p><div id="527" class="more"><p><a href="#" id="527-hide" class="hideLink" onclick="showHide(527);return false;">Hide</a></p><p><strong>Abstract: </strong>Opportunistic networking (ON), a concept which allows a mobile wireless device to dynamically interact with other wireless devices in its immediate vicinity, is a field with great potential to improve the utility of itinerant computational platforms.  While ONs increase a device&#039s ability to interact with its peers, the fleeting and intermittent connections between devices make many traditional computer collaboration paradigms, such as distributed shared memory (DSM), virtually untenable.  DSM systems, developed for traditional networks, rely on relatively stable, consistent connections among participating nodes.  In an ON, connectivity among nodes is distributed in time and space making it useful for delay tolerant applications only.  When devices are disconnected their ability to collaborate is interrupted and their computing power underutilized until such time as they encounter their computing partners again.  Because nodes within an ON are assumed to have no advance knowledge that they will encounter a specific device in the future, periods of disconnection between nodes could be lengthy or potentially infinite.   This thesis proposes delay tolerant lazy release consistency (DTLRC) for implementing distributed shared memory in opportunistic networking environments.  DTLRC permits mobile devices to remain independently productive while separated and provides a mechanism for nodes to regain coherence of shared memory if and when they meet again. DTLRC allows nodes to make use of results produced on shared memory while disconnected.  Simulations using experimentally determined data traces demonstrate that DTLRC is a viable concept for enhancing cooperation among mobile wireless devices in an opportunistic networking environment.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Performance Tuning of an Embedded Template Analysis Tool</strong><br /> Monday, April 18, 2011<br /> Ashwin Arikere<br /><p><a href="#" id="521-show" class="showLink" onclick="showHide(521);return false;">Read More</a></p></p><div id="521" class="more"><p><a href="#" id="521-hide" class="hideLink" onclick="showHide(521);return false;">Hide</a></p><p><strong>Abstract: </strong>As multi core processors become ubiquitous, programs must be designed to utilize the additional power at their disposal. Extracting maximum performance out of these processors requires utilizing latest parallel programming techniques as well as using the latest tools available to increase performance. This thesis delves into enhancing performance of existing programs by applying parallel techniques, utilizing performance analyzers and various other tools developed specially for multi-core system programming. We discuss these techniques and tools and also demonstrate how a Template Analysis program was modified to take advantage of the additional power a multi-core machine has over single core machines to enable its use in an embedded application. The overall efficiency of the application was increased using these techniques, and then tested using several data sets as well as various core machines. More information was gathered using this data and the program was fine tuned to extract maximum performance. Additional considerations were made towards developing a real time Template Analysis program.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Privacy for Location Based Services on Smartphones</strong><br /> Monday, April 18, 2011<br /> James Spargo<br /><p><a href="#" id="522-show" class="showLink" onclick="showHide(522);return false;">Read More</a></p></p><div id="522" class="more"><p><a href="#" id="522-hide" class="hideLink" onclick="showHide(522);return false;">Hide</a></p><p><strong>Abstract: </strong>In recent years, mobile devices and smartphones enabled with GPS and Internet access have become extremely common. People use these devices as they would a computer for easy access to information.  Location Based Services (LBS) provide customized information based on a user&#039s geographic location that has been retrieved from a dedicated spatial database such as Google Places, Yahoo&#039s Local Search Web Services and Yelp.com.  This information can include nearby hotels and restaurants, gas stations, banks or other Points of Interest (POIs). Since most search engines and databases are known to store previous queries in order to improve future search results and other data analysis on previous search queries, many researchers have expressed concerns and proposed solutions to protect a user&#039s location privacy. Research has shown that a significant amount of information, such as medical conditions, political or religious affiliations and more can be inferred based on a person&#039s previous location tracks. Many methods proposed by researchers rely on the use of trusted third parties such as Anonymizing Servers, other nearby mobile devices, or the LBS itself. CAP (Context-Aware Privacy), introduced in 2008 by A. Pingley et. al., is a system that was designed to protect a user&#039s location without having to rely on a trusted third party or interfere with the operation of the LBS. A desktop prototype was made, yet it was never implemented on a mobile device or smartphone until now. Preliminary tests of CAP with lower privacy settings proved to be effective, although when the privacy settings were increased, the results seemed to deteriorate. Closer examinations of the algorithm indicate that it is effective when compressing contextual map data for use by a mobile device, as well as effective perturbation of the user&#039s location.  The POI results returned from the LBS tell a different story.  While the POI results at low privacy levels seemed to be accurate (i.e. the POIs returned from the LBS are in fact the POIs that are closest to the user), when the privacy settings were increased, the results would degrade (i.e. the POIs returned were, as expected, further away from the user&#039s actual location).  This is effective in the sense that the user&#039s actual location is not able to be divulged to an adversary, but is not very effective in terms of usability and convenience for the user. In this thesis, we review CAP and several other proposed methods of location privacy intended for use with mobile devices.  We have implemented CAP on a smartphone in its proposed method and evaluate its results, followed by modifications in order to gain more accurate POI results from publicly available LBSs.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>GPU PARALLEL COLLECTIONS FOR SCALA</strong><br /> Thursday, April 14, 2011<br /> Kishen Das Kondabagilu Rajanna<br /><p><a href="#" id="520-show" class="showLink" onclick="showHide(520);return false;">Read More</a></p></p><div id="520" class="more"><p><a href="#" id="520-hide" class="hideLink" onclick="showHide(520);return false;">Hide</a></p><p><strong>Abstract: </strong>Graphics processing units have, so far, been used specifically for high-speed graphics. Off late , they are becoming more popular as general purpose parallel processors. With the release of OpenCL, programmers can now split their program execution between CPU and GPU,  whenever appropriate,  resulting in huge performance gain. The cost of GPU is also reducing, making it more accessible to users. Although enormous performance gains can be achieved by parallelizing the code, identifying the right candidate for GPU execution  is very tricky. Coding in OpenCL has been a difficult task owing to the memory management complexities of the GPU.   We are developing  Firepile, a library and a compiler for GPU programming in Scala. Firepile makes it easier to port parallelizable code to GPU. As part of the library, I have been working on parallel collections that abstract the memory management and code complexities from the end-user.  Many of the basic collection frameworks like Array, Map, Matrix, etc have been provided as part of this library. Functions like map, reduce, scan, sort, find, filter, etc have been parallelized and implemented in the library that run on the GPU. Our experiments show that the library achieves performance similar to OpenCL implementation with much shorter, easy to understand code. Significant performance improvement has been observed when dealing with large data sets.  Also, there is no learning curve involved in understanding the inner-working of the GPU. Scala is a natural blend of functional paradigm and object orientation and hence it has been selected for the Firepile compiler.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>CONCURRENT POLYGLOT - AN EXTENSIBLE COMPILER FRAMEWORK</strong><br /> Wednesday, April 13, 2011<br /> Saurabh Kothari<br /><p><a href="#" id="519-show" class="showLink" onclick="showHide(519);return false;">Read More</a></p></p><div id="519" class="more"><p><a href="#" id="519-hide" class="hideLink" onclick="showHide(519);return false;">Hide</a></p><p><strong>Abstract: </strong>We have today crossed the threshold of increasing clock frequencies as the dominant solution to faster computing, and it is imperative for software developers to be able to think in the concurrent and parallel paradigm. Important still is to equip programmers with the right tools to develop concurrent and parallel software.  Software concurrency is a widely researched area, and many of today’s compilers concentrate on compiling for parallelism. With a lot of ongoing research on parallel software and although concurrent compilers have been explored since the 1970s, a very few of them exist today that justify multicore environments. gmake supports coarse grained parallelism through the –j option for building independent files in parallel. Our effort here brings into perspective a much finer grained approach: targeting the abstract syntax tree to extract opportunities for parallel and concurrent compilation. Polyglot is an extensible compiler framework. It allows the extension of Java in domain specific ways into languages that may define their own new constructs. Sequential compilation in Polyglot uses only one core, even if additional cores are present. In order to fully utilize the power of the underlying hardware, we present here our efforts in making Polyglot concurrent. Parallelization of the compilation process can yield faster compilation times. It also presents the challenges of maintaining order and correctness and scalability with hardware. All of these challenges make this a test bed for experimenting with various concurrency models. As part of the larger Polyglot project, we therefore demonstrate here that concurrency, parallelism, correctness and scalability are all achievable within the Polyglot compiler framework. We evaluate and present here the performance of Concurrent Polyglot against benchmarks and Sequential Polyglot.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Typifying Wikipedia Articles</strong><br /> Wednesday, November 24, 2010<br /> Quazi Hasan<br /><p><a href="#" id="512-show" class="showLink" onclick="showHide(512);return false;">Read More</a></p></p><div id="512" class="more"><p><a href="#" id="512-hide" class="hideLink" onclick="showHide(512);return false;">Hide</a></p><p><strong>Abstract: </strong>In Wikipedia, each article represents an entity. Entity can have different types like person, country, school, etc. Although Wikipedia encapsulates category information for each page, sometimes it is not sufficient to deduce the type of a page just from its categories. But, incorporating the clear type information in a Wikipedia page is very important for the users, as it will help them to explore the pages in more organized way. Hence, in my thesis, we explore different standard classification techniques and experiment how these techniques can be made more effective for typifying Wikipedia articles by using different feature selection methods.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Time Efficient Jamming Detection in Wireless Sensor Networks</strong><br /> Wednesday, November 24, 2010<br /> Kartik Siddhabathula<br /><p><a href="#" id="515-show" class="showLink" onclick="showHide(515);return false;">Read More</a></p></p><div id="515" class="more"><p><a href="#" id="515-hide" class="hideLink" onclick="showHide(515);return false;">Hide</a></p><p><strong>Abstract: </strong>Non-dedicated medium of communication for Wireless Sensor Networks (WSN) makes them vulnerable to jamming attack where an adversary injects strong signal or noises to interfere with the transmission and thus block the channel. In applications such as border security, people are particularly interested in the areas where the channel is currently jammed. It is therefore crucial to be able to detect the jamming attacks as fast as possible. Existing studies have shown that the most effective indicator of jamming attacks is the packet delivery ratio (PDR). However, current PDR-based schemes use end-to-end packet delivery ratio, which requires one to observe the communication for a long period of time before any good decision can be made. In this thesis, a collaborative detection scheme is introduced. The main idea is to collectively evaluate the packet delivery ratio in a given area instead of a pair of nodes since the attacker often jams an area of their interest (not just two specific nodes). In other words, we use observations from other nodes, which allows us to detect jamming attacks in a much faster way. We have evaluated the feasibility and performance of our protocol on Telosb motes. The results show that we can effectively detect jamming attacks within one round of communication. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Framework for Real-Time Fault Detection and  Response in Multi-Agent Teams</strong><br /> Monday, November 22, 2010<br /> Matthew Middleton<br /><p><a href="#" id="514-show" class="showLink" onclick="showHide(514);return false;">Read More</a></p></p><div id="514" class="more"><p><a href="#" id="514-hide" class="hideLink" onclick="showHide(514);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis details the creation of a fault detection and response framework that can be applied to mobile multi-agent teams.  This framework unites existing concepts of conditioned based maintenance and real-time machine diagnostics with a high-level matrix based discrete event command and control system that can respond to detected mechanical faults in agents in real-time. This allows for automatic corrective actions to be taken when an agent experiences failure if redundant resources are available to replace the failed unit or for the mission planner to be notified that the mission is no longer achievable given the operating conditions of the resources.   In addition to positing a theoretical framework, this thesis details the design and implementation of a device consisting of hardware, firmware, and software that allows for condition based maintenance methods to be used for diagnosis of mechanical faults in the context of a team of mobile, networked agents.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Measuring Named Entity Similarity Through Wikipedia Category Hierarchies</strong><br /> Friday, November 19, 2010<br /> Jared Ashman<br /><p><a href="#" id="507-show" class="showLink" onclick="showHide(507);return false;">Read More</a></p></p><div id="507" class="more"><p><a href="#" id="507-hide" class="hideLink" onclick="showHide(507);return false;">Hide</a></p><p><strong>Abstract: </strong>Identifying the semantic similarity between named entities has many applications in NLP, including information extraction and retrieval, word sense disambiguation, text summarization and type classification. Similarity between named entities or terms is commonly determined using a taxonomy based approach, but the limited scalability of existing taxonomies has led recent research to use Wikipedia’s encyclopedic knowledge base to find similarity or relatedness. These existing methods using Wikipedia have so far focused on relatedness, but are not as well suited to finding similarity. In this thesis, we evaluate methods for determining the semantic similarity between named entities by associating each named entity to a specific Wikipedia article, and then using the commonalities between Wikipedia category hierarchies as the similarity. To evaluate the effectiveness, we conducted a survey to get manually defined similarity scores for named entity pairs. The scores were then compared to both implemented methods and existing relatedness measures.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Recommender Systems: An Algorithm to predict “Who rated what”</strong><br /> Friday, November 19, 2010<br /> Rahul Singhal<br /><p><a href="#" id="508-show" class="showLink" onclick="showHide(508);return false;">Read More</a></p></p><div id="508" class="more"><p><a href="#" id="508-hide" class="hideLink" onclick="showHide(508);return false;">Hide</a></p><p><strong>Abstract: </strong>Recommender systems are systems that recommend content for us by looking at certain factors including what other people are doing as well as what we are doing. Examples of such systems present today are Amazon.com recommending books, CDs, and other products, Netflix recommending movies etc. These systems basically recommend items or movies to customers based on the interests of the present customer and other similar customers who purchased or liked the same item or movie. Despite all the advancements, recommender systems still face problems regarding sparseness of the known ratings within the input matrix. The ratings are given in the range of (1-5) and present systems predict “What are the ratings” but here we propose a new algorithm to predict “Who rated what” by finding contrast points in user-item input matrix. Contrast points are the points which are farthest from the known rated items and most unlikely to be rated in future. We experimentally validate that our algorithm is better than traditional Singular Value Decomposition (SVD) method in terms of effectiveness measured through precision/recall.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>TARGET TRACKING WITH LUCAS-KANADE OPTICAL FLOW AND PARTICLE FILTERS THROUGH AFFINE TRANSFORMS AND OCCLUSION</strong><br /> Friday, November 19, 2010<br /> Justin Graham<br /><p><a href="#" id="511-show" class="showLink" onclick="showHide(511);return false;">Read More</a></p></p><div id="511" class="more"><p><a href="#" id="511-hide" class="hideLink" onclick="showHide(511);return false;">Hide</a></p><p><strong>Abstract: </strong>This paper will investigate a hybrid approach derived from Lucas-Kanade optical flow tracking and particle filters that is capable of tracking objects through occlusion and affine transformations.  This approach was inspired by aircraft sensor pod infrared and electro-optical tracking applications.  For aircraft based sensors, it is important that a tracking system be able to track through rotations as the aircraft orbits a targeting area.  It is also of use to handle cases where the target may be momentarily occluded due to other vehicles or obstacles in the area.  The main focus of this investigation was to find a technique that would work in these scenarios for a single tracked target.  The implementation of this algorithm is written in Matlab and is not intended to run in realtime, but could be easily extended to do so with minor performance tweaks and native implementations of some of the more performance intensive functions.  This paper will also describe some additional algorithms that have desirable attributes for aircraft based tracking systems.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A HUMAN MOTION DATABASE: THE COGNITIVE AND PARAMETRIC SAMPLING OF HUMAN MOTION</strong><br /> Tuesday, November 16, 2010<br /> Arnab Biswas<br /><p><a href="#" id="509-show" class="showLink" onclick="showHide(509);return false;">Read More</a></p></p><div id="509" class="more"><p><a href="#" id="509-hide" class="hideLink" onclick="showHide(509);return false;">Hide</a></p><p><strong>Abstract: </strong>Motion databases have a strong potential to guide progress in the field of machine recognition and motion-based animation. Existing databases either have a very loose structure that do not sample the domain according to any controlled methodology or too few action samples which limits their potential to quantitatively evaluate the performance of motion-based techniques. The controlled sampling of the motor domain in the database may lead investigators to identify the fundamental difficulties of motion cognition problems and allow the addressing of these issues in a more objective way. In this thesis, we describe the construction of our Human Motion Database using controlled sampling methods (parametric and cognitive sampling) to obtain the structure necessary for the quantitative evaluation of several motion-based research problems. The Human Motion Database is organized into several components: the praxicon dataset, the cross-validation dataset, the generalization dataset, the compositionality dataset, and the interaction dataset. The main contributions of this thesis include (1) a survey of human motion databases describing data sources related to motion synthesis and analysis problems, (2) a sampling methodology that takes advantage of a systematic controlled capture, denoted as cognitive sampling and parametric sampling, (3) a novel structured motion database organized into several datasets addressing a number of aspects in the motion domain, (4) a study of the design decisions needed to build a custom skeleton to generate joint angle data from marker data, and (5) a study of the motion capture technologies and the general optical motion capture workflow including capturing and post processing data.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>ANDLING PERIODIC SIGNS IN AMERICAN SIGN LANGUAGE USING SYNTHETIC GENERATION OF PERIODS</strong><br /> Tuesday, November 16, 2010<br /> HIMANSHU PAHWA<br /><p><a href="#" id="510-show" class="showLink" onclick="showHide(510);return false;">Read More</a></p></p><div id="510" class="more"><p><a href="#" id="510-hide" class="hideLink" onclick="showHide(510);return false;">Hide</a></p><p><strong>Abstract: </strong>American Sign Language (or ASL) is the dominant sign language of deaf people in United States and parts of Canada. People using ASL as their primary language range from 500,000 to 2 million in the United States. ASL uses hands, head and body, with constantly changing movements and orientations. Since the language is based on gestures, not a printed alphabet, it gets difficult to know corresponding meaning given a video. There are multimedia tools and dictionaries available to view a sign video for a given word but there are no dictionaries available which, given a sign video will respond with corresponding word. This motivated the development of a video based lookup in ASL dictionary. The vision is to have system in which a user will be able to find the meaning of an ASL sign simply by performing it in front of a video camera synced to a computer. The computer will compare the unknown sign with a database of signs to identify the most likely matches. In existing ASL Lexicon Project a user submits a query sign video and the application finds the most similar signs from the system database. The existing system evaluates the similarity between the query video and every sign video in the  database, using Dynamic Time Warping (DTW) distance. DTW is an algorithm for measuring similarity between two sequences which may vary in time or speed. DTW is a method that allows a computer to find an optimal match between two given sequences (e.g. time series) with certain restrictions. The sequences are warped non- linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. The existing ASL Lexicon Project uses the similarity in hand locations and orientation to lookup a video sign in dictionary of ASL signs. The ability of DTW to cater to temporal misalignments helps us recognize signs differing in time or speed. ASL Periodic signs are signs which have repetition of an action. The number of times this action is repeated is signer’s discretion. DTW in such cases will still attempt to align input video with dictionary sign video. This will be incorrect if the number of times an action is repeated in input video differs from dictionary sign video. This makes the method of sign matching, for periodic signs, incorrect in an exemplar based database. DTW in such case result in non-meaningful sequence alignment, eventually resulting in poor similarity results. The paper attempts to correct the problem of incorrect period matching for periodic signs. The paper contributes by defining a protocol for annotating periodic signs and introduces a method for improving system accuracy on such signs. It builds an informative database for periodic video signs on the ASL Lexicon dataset of 1113 unique signs. It captures the temporal information(start and end of period) for all the periods executed in periodic sign video. This paper provides a mechanism to generate periods synthetically. We use the temporal information of the last period to create subsequent periods for the training video. We successfully corrected the incorrect period matching problem for periodic signs by synthetically generating periods, and still maintaining the exemplar status of the system database.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Mobility Enabled Video Conferencing for Next Generation Networks using Existing Telephony Tools</strong><br /> Monday, July 19, 2010<br /> Avinash Narayan<br /><p><a href="#" id="500-show" class="showLink" onclick="showHide(500);return false;">Read More</a></p></p><div id="500" class="more"><p><a href="#" id="500-hide" class="hideLink" onclick="showHide(500);return false;">Hide</a></p><p><strong>Abstract: </strong>The Heterogeneous nature of next generation networks requires architecture and technologies that have an intrinsic support for diversity. Providing seamless roaming and mobility wherever you go without intervention is not an added functionality but should be an in-built feature.  The plethora of existing wireless access technologies will soon be connected to Internet in the wave of “Internet of Things”. We have learnt from many endeavors that no single technology exists that is ubiquitous and connects everything. In Cellular service, along with the geographic coverage limitation, the major drawback is the lack of complete coverage inside buildings. Once you are inside many public buildings, cellular coverage is blocked by RF opaque walls, but may have strong Wi-Fi connectivity. If mobility solution depends on cellular services, that mobility functionality may be lost once you go inside. this is a clear problem as a weakness in their offerings. Addressing this problem means either bringing the “outside in” or the “inside out”, that is we need technologies that can bridge gaps between existing technologies. Though many multi-mode mobile devices and technologies with video capabilities exist for a more than a decade (with the recent release of “FaceTime” calling on Wi-Fi for popular iPhone), user needs a technology that enables seamless mobility not only in Wi-Fi or GSM but across heterogeneous networks and Internet. We propose a new architecture to enable seamless mobility in audio/video conferencing and show how this architecture achieves seamless mobility at two levels, first at core of the Internet and second at the edge of the Internet where different access networks are converged using technologies such as Fixed Mobile Convergence (FMC). A large number of mobility solutions and techniques exist in literature however most of them do not achieve seamless handoffs of video/audio between heterogeneous networks, for ex. cellular networks and IP networks. Our architecture achieves mobility in video/audio conferencing for heterogeneous networks. We will also discuss mobility management/vertical handoff techniques and load distribution mechanisms of control plane and media plane functionalities of a video/audio conference. H.323 technology is complex and expensive to implement. SIP through its simplicity has been envisioned as the signaling protocol of future but SIP standard does not describe the implementation for video conferencing in-order to keep it modular from implementation details. We do describe how SIP can be used to implement video conferencing systems with mobility. Though Mobile IP has been envisioned as provider of mobility for next generation networks, there are many issues such as persistence, looming crisis of Internet addressing, explosion of routing tables, security etc. that threaten its use for future. Instead our approach is to use flat self-certifying names for endpoints and a name based any-cast routing that exists on top of IP layer enabling IP to use path labels rather than globally routable addresses. Our approach also considers minimizing changes in the standard network components and the mobile devices carried by end user using existing telephony tools such as SIP Proxy Server, ISDN Gateway, SIP Conference servers, SIP client etc. that have been rightly positioned to be the future of telephony. We setup test bed to implement seamless mobility and measure performance characteristics (such as delay, jitter, handoff time, packet loss) for our new architecture.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Building Bayesian Network Based Expert Systems From Rules</strong><br /> Monday, July 19, 2010<br /> Saravanan Thirumuruganathan<br /><p><a href="#" id="504-show" class="showLink" onclick="showHide(504);return false;">Read More</a></p></p><div id="504" class="more"><p><a href="#" id="504-hide" class="hideLink" onclick="showHide(504);return false;">Hide</a></p><p><strong>Abstract: </strong>Combining expert knowledge and user explanation with automated reasoning in domains with uncertain information poses significant challenges in terms of representation and reasoning mechanisms. In particular, reasoning structures understandable and usable by humans are often different from the ones for automated reasoning and data mining systems.  Rules are a convenient and human understandable way to express domain knowledge and build expert systems. Adding certainty factors to these rules presents one way to deal with uncertainty in rule based expert systems. However such systems have limitations in accurately modeling the domain. Bayesian Network is , on the other hand , a probabilistic graphical model that allows accurate modeling of a domain and automated reasoning. But inference in Bayesian Network is harder for humans to comprehend.  In this thesis, we propose a method to combine these two frameworks to build Bayesian Networks from rules and derive user understandable explanations in terms of these rules. Expert specified rules are augmented with strength parameter for the antecedent and are used to derive probabilistic bounds for the Bayesian Network&#039s conditional probability table. The partial structure constructed from the rules is fully learned from the data. The thesis also discusses methods for identifying wrong rules, suggesting new rules and performing incremental learning.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Embedded Subspace Sparse Representation</strong><br /> Friday, July 16, 2010<br /> Jin Huang<br /><p><a href="#" id="499-show" class="showLink" onclick="showHide(499);return false;">Read More</a></p></p><div id="499" class="more"><p><a href="#" id="499-hide" class="hideLink" onclick="showHide(499);return false;">Hide</a></p><p><strong>Abstract: </strong>In this thesis, I would discuss a novel method which implemented the classification and dimension reduction simultaneously for face images.  Using the property of L1 norm recently discovered,  a sparse representation for any test image via training images could be possible, meanwhile, we could find a best subspace projection to optimize the proposed objective function. Doing these alternatively, we could find the best embedded subspace as scheduled.  At the end of the thesis, I would compare the experiment results with other classical unsupervised dimension reduction and classification methods combination, it demonstrates the effectiveness of our method.        </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Mathematical Modeling for Phagocyte Transmigration and Reverse Engineering</strong><br /> Thursday, July 01, 2010<br /> Mingon Kang<br /><p><a href="#" id="498-show" class="showLink" onclick="showHide(498);return false;">Read More</a></p></p><div id="498" class="more"><p><a href="#" id="498-hide" class="hideLink" onclick="showHide(498);return false;">Hide</a></p><p><strong>Abstract: </strong>Computational Modeling and simulation have been used as an important tool to analyze the behavior of a complex biology system by computation. Typically, the biology system is a complex non-linear system where a large number of components are involved. The major computational obstacle in computational modeling and simulation is to discover a large number of parameters in the mathematical equations, which represent biological properties of the system.  To tackle this problem, we have developed a method for global optimization solution. An algorithm, Discrete Selection Levenberg-Marquardt(DSLM), is developed for solving the non-linear least square problem which is the most popular approach to the approximate solution of over-determined systems. DSLM suggests a new approach using selection of optimal parameters on the discrete spaces for computational convergence, while other global optimization methods such as Genetic Algorithm and Simulated annealing use heuristic approach that don&#039t guarantee its convergence.  Phagocyte Transmigration is the process that fibroid materials are formed around a biomedical device when implantation. The goal of modeling for Phagocyte Transmigration is to construct the analyzer to understand the nature properties of the systems. Also, the exact simulation by computational modeling for Phagocyte Transmigration provides critical clues to recognize the current&#039s status of the system and predict the future degree. In this thesis, it is used to simulate the future degree after 16 hours and to predict its evolution for a variety cases such as blocking some components of the system.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Fast and Energy-efficient mapping of jammed regions in Wireless Sensor Networks</strong><br /> Thursday, June 17, 2010<br /> Nabila Rahman<br /><p><a href="#" id="496-show" class="showLink" onclick="showHide(496);return false;">Read More</a></p></p><div id="496" class="more"><p><a href="#" id="496-hide" class="hideLink" onclick="showHide(496);return false;">Hide</a></p><p><strong>Abstract: </strong>Wireless sensor networks (WSNs) are gaining much popularity by becoming more affordable now-a-days.  They have great practical importance in variety of deployments such as to perform monitoring  to detect any intrusion in a secured area. But, wireless communication is susceptible to jamming attacks. Jamming is very simple to launch in a WSN and can lead to DOS (denial of service) by disrupting communications among the sensor nodes in the network. Since it is difficult to prevent jamming attacks, detection and mapping out the jammed regions is critical to overcome this problem. In a security monitoring scenario, the network operator will be able to take proper measures against jamming once the jammed regions in the network are known to them. It is also desirable to keep the interactions in the network minimal, as they mostly consist of sensor nodes that are low powered commodity devices. A light-weight technique for fast mapping of the jammed regions is proposed in this thesis. The load on the sensors are minimized by removing the actual responsibility of mapping from the network to the central base station. After a few nodes report of jamming to the base station, it carries out the task of mapping of the jammed regions in the network. We measure the performance of our proposed system by our simulation results and experiments on the real sensor nodes and show that the jammed regions in a network can be mapped from few nodes reporting to the base station.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>DETECTING MEDICATION CONSUMPTION PATTERN IN ASSISTIVE ENVIRONMENTS</strong><br /> Wednesday, June 09, 2010<br /> Jyothi K Vinjumur<br /><p><a href="#" id="497-show" class="showLink" onclick="showHide(497);return false;">Read More</a></p></p><div id="497" class="more"><p><a href="#" id="497-hide" class="hideLink" onclick="showHide(497);return false;">Hide</a></p><p><strong>Abstract: </strong>One of the issues in healthcare systems or medical information systems is the reduction of medical errors to ensure patient safety. Our approach is to develop a cyber physical system which applies different RFID tags to monitor medicine consumption and its impact in an assistive environment. This approach talks about detecting the medication intake pattern in an assistive environment and implements an application oriented experimental research which tracks the drug intake pattern using RFID readers and tags, motion sensors, a wireless sensor mote and a weight sensor. In this approach, an energy efficient technique by using multiple sensor devices which aims in efficient information flow to achieve significant extension of the system lifetime is implemented. Here, we use wireless sensor network environment to gather a person&#039s behavior of daily pill usage in an apartment. Most people especially elderly are likely to have a sudden behavioral change due to their aging or existing health problems. Therefore, it is necessary to have an autonomous system that can monitor their activities in order to prevent emergent situation in advance. Our approach presents a sensor network environment that can recognize normal behavioral patterns of the patients who live in an apartment without assistance. We use a Web Based Caregiver Module to make the process of monitoring the medicine consumption simpler and easier.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Comparison and Evaluation of Motion Indexing Techniques</strong><br /> Monday, May 17, 2010<br /> Harnish Bhatia<br /><p><a href="#" id="494-show" class="showLink" onclick="showHide(494);return false;">Read More</a></p></p><div id="494" class="more"><p><a href="#" id="494-hide" class="hideLink" onclick="showHide(494);return false;">Hide</a></p><p><strong>Abstract: </strong>Motion capture (mocap) is a way to digitally represent the spatio-temporal  structure of human movement. Human motion is generally captured in long sequences  to record the natural and complex aspects of human actions, its sequential  transitions, and simultaneous combinations. As the amount of mocap data increases,  it becomes important to index the data in order to improve the performance of information  access. The motion indexing problem involves several challenging issues due  to the data being high dimensional, continuous, and time-variant. Indexing provides  the means to search for similar motion in a mocap database, to recognize a query  action as one among several motion classes, and fosters the reusability of motion data  to generate animation automatically.     The topic of my research is the design, implementation, and evaluation of several approaches  to the motion indexing problem. We consider three different existing types of  whole-body motion indexing algorithms: dimensionality reduction based techniques,  feature function based techniques, and dynamic time warping based techniques. The  advantages and disadvantages of these techniques are explored. We evaluate the performance of each technique using a subset of the CMU Motion Capture Database and its corresponding annotation. These experimental results will allow for an objective comparison between the different indexing techniques and for assessing the deficiencies of whole-body indexing techniques.   </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Performance Evaluation of a City Bus Network</strong><br /> Monday, April 19, 2010<br /> Samreen Tahir<br /><p><a href="#" id="487-show" class="showLink" onclick="showHide(487);return false;">Read More</a></p></p><div id="487" class="more"><p><a href="#" id="487-hide" class="hideLink" onclick="showHide(487);return false;">Hide</a></p><p><strong>Abstract: </strong>Opportunistic networks and mesh networks are one of the two most important evolutions of Mobile Ad hoc Networks.  In this thesis, we study opportunistic networks in which an end to end path between source and destination pair may never exist.  Opportunistic networks are characterized by intermittent connectivity and volatile topology due to which traditional MANET routing protocols cannot operate efficiently.  Particularly, we present the city bus network of Helsinki, Finland as a case study of ferry based message dissemination catering to communication needs of city residents.  City buses are a good candidate for message dissemination as they connect passengers distributed across the city within different regions and give rise to recurrent opportunities for travelers to be in close proximity.  This improves network connectivity message delivery probability.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Selective Cross Correlation in Passive Timing Analysis Attacks against Low-Latency Mixes</strong><br /> Friday, April 16, 2010<br /> Titus Abraham<br /><p><a href="#" id="484-show" class="showLink" onclick="showHide(484);return false;">Read More</a></p></p><div id="484" class="more"><p><a href="#" id="484-hide" class="hideLink" onclick="showHide(484);return false;">Hide</a></p><p><strong>Abstract: </strong>A mix is a communication proxy that hides the relationship between incoming and outgoing messages. Routing traffic through a path of mixes is a powerful tool for providing privacy. When mixes are used for interactive communication, such as VoIP and web browsing, attackers can undermine user privacy by observing timing information along the path. Mixes can prevent these attacks by inserting dummy packets (cover traffic) to obfuscate timing information in each stream. Two recently proposed defenses, defensive dropping and adaptive padding, enhance cover traffic by ensuring that timing information seen at the sender is very different from that seen at the receiver. In this work, we propose Selective Cross Correlation (SCC), an attack that an eavesdropper could employ to de-anonymize users despite the use of defensive dropping or adaptive padding. The main insight of our approach is that, with either defense, the timings at one end of the stream are a subset of the timings at the other end of the stream. By considering the network conditions and the defensive mechanism used, an appropriate correlation window can be found and used to effectively remove the cover traffic, thereby enabling us to correlate both ends of the stream. We have conducted real network experiments and have found that SCC greatly improves attacker effectiveness over prior techniques against both the defenses. With SCC, the attacker is nearly as successful as when neither defense is applied. This attack demonstrates the need for more robust defenses against statistical timing attacks. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Mimic: An active covert channel that evades regularity-based detection</strong><br /> Friday, April 16, 2010<br /> Kush Kothari<br /><p><a href="#" id="485-show" class="showLink" onclick="showHide(485);return false;">Read More</a></p></p><div id="485" class="more"><p><a href="#" id="485-hide" class="hideLink" onclick="showHide(485);return false;">Hide</a></p><p><strong>Abstract: </strong>A covert timing channel is a hidden communication channel based on network timing that an attacker can use to sneak secrets out of a secure system. Active covert channels, in which the attacker uses a program to automatically generate innocuous traffic to use as a medium for embedding the covert channel,are especially problematic, as they allow the attacker to output large amounts of secret data. A promising technique for detecting covert timing channels focuses on using entropy-based tests. This technique can reliably detect known covert timing channels by using a combination of entropy (EN) and conditional entropy (CE) to detect anomalies in shape and regularity, respectively. The CE test is particularly effective at detecting regularity in active covert channels.  In this work, we show that these detection techniques can be defeated by an active covert channel that generates traffic in a purposefully irregular manner. In particular, we propose Mimic, an active covert channel that mimics both the shape and regularity of legitimate traffic to disguise its presence. Mimic includes two modules, a shape modeler and a regularity modeler, for learning about the statistical properties of real traffic and generating traffic with the same properties. To measure the effectiveness of our mechanism, we ran experiments for both detection and throughput over a LAN and over the Internet. Our results show that Mimic is undetectable by any known detection technique while providing 100% channel capacity and low error rates. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Towards Modeling the Behavior of Physical Intruders in a Region Monitored by a Wireless Sensor Network</strong><br /> Wednesday, April 14, 2010<br /> Pranav Krishnamoorthy<br /><p><a href="#" id="486-show" class="showLink" onclick="showHide(486);return false;">Read More</a></p></p><div id="486" class="more"><p><a href="#" id="486-hide" class="hideLink" onclick="showHide(486);return false;">Hide</a></p><p><strong>Abstract: </strong>A priority task for homeland security is the coverage of large spans of open border that cannot be continuously physically monitored for intrusion. Low-cost monitoring solutions based on wireless sensor networks have been identified as  an effective  means to perform  perimeter monitoring.  A scattering of  an ad-hoc wireless sensor network   over  a  border   could  be  used  to  perform surveillance   over  a  large  area  with   relatively  little human  intervention.  Determining  the effectiveness of  such an autonomous network in detecting and thwarting  an intelligent intruder is a difficult task.    We propose a model for an intelligent attacker that  attempts to find a detection-free  path in a region with  sparse sensing coverage. In  particular, we apply  reinforcement learning (RL), a  machine learning algorithm,  for our model. RL algorithms are well  suited for  scenarios  in  which  specifying  and finding an optimal solution is  difficult. By using RL, our attacker can easily adapt to new  scenarios by translating constraints into rewards. We  compared our RL-based technique to a reasonable heuristic  in simulation. Our  results  suggest  that our RL-based attacker  model is significantly more effective, and therefore more realistic,  than the heuristic approach. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Real Time Hardware and Software Systems for Micro Air Vehicle Flight Control Testing</strong><br /> Tuesday, April 13, 2010<br /> Christopher Dale McMurrough<br /><p><a href="#" id="488-show" class="showLink" onclick="showHide(488);return false;">Read More</a></p></p><div id="488" class="more"><p><a href="#" id="488-hide" class="hideLink" onclick="showHide(488);return false;">Hide</a></p><p><strong>Abstract: </strong>Micro Aerial Vehicles (MAVs) are an emerging class of aircraft made possible by the continuing miniaturization of mechanical, electrical, and computing systems. While MAVs based on traditional fixed wing and rotorcraft designs have been successfully demonstrated, their capabilities are somewhat limited. MAVs with biologically inspired flapping wing flight will prove to be much more capable than traditional aircraft designs at the micro scale.   In this thesis, a Real-Time Testing Environment for Development of MAV Flight Controls is presented. The system makes it possible to test MAV flight controls without integrated avionics by using real-time computing hardware within a vision based motion capture environment. A Split-Cycle Wingbeat Modulated MAV Platform for Hardware-in-the-loop Control Analysis is also presented. This test platform was designed and fabricated to verify the motion capabilities of a proposed set of flapping wing MAV control laws on a realistically sized vehicle actuated by brushless DC (BLDC) motors. Finally, A Reinforcement Learning Approach to BLDC Motor Commutation is proposed in the context of actuating a flapping wing with finite timing constraints.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong></strong><br /> Monday, December 07, 2009<br /> GaurI Vakde<br /><p><a href="#" id="482-show" class="showLink" onclick="showHide(482);return false;">Read More</a></p></p><div id="482" class="more"><p><a href="#" id="482-hide" class="hideLink" onclick="showHide(482);return false;">Hide</a></p><p><strong>Abstract: </strong></p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Reactive control composition for mobile manipulators</strong><br /> Friday, December 04, 2009<br /> Binu Mathew<br /><p><a href="#" id="476-show" class="showLink" onclick="showHide(476);return false;">Read More</a></p></p><div id="476" class="more"><p><a href="#" id="476-hide" class="hideLink" onclick="showHide(476);return false;">Hide</a></p><p><strong>Abstract: </strong>A mobile manipulator is a manipulator mounted on a mobile platform. Due to this combination it has increased mobility compared to a fixed manipulator and increased dexterity compared to a mobile platform. At the same time it has a significantly higher number of degrees of freedom than fixed manipulators or mobile platforms and an increased task and workspace. In particular, the size of the workspace of the manipulator is restricted only by the workspace limitations of the mobile platform and the obstacles around the goal location. In addition, the extra degrees of freedom increase the number of ways in which a particular task can be performed. As a consequence of these properties, the task domain of a mobile manipulator is significantly larger than for other mobile platforms, allowing for a large variety of applications to be addressed. However, the operation of such a system becomes also more challenging because of the increased complexity of the task domain and the unstructured and uncertain environments that the robot generally operates in. Problems like kinematic singularities and non-holonomic constraints arise with increasing frequency and manipulation here critically depends on feedback during its interactions with the environment. As a consequence, a control technique applied to the mobile manipulator must be able to deal with the unstructured, uncertain environment and multiple potentially interacting goals, and be able to address a broad range of operating contexts. It also must be flexible enough to allow the derivation of task-specific control instances without the need for complex reengineering control structures while ensuring some level of completeness, correctness and robustness. Traditional control methods are challenged when applied to this domain, especially in terms of range of operating contexts. The control basis approach is used in this research to address this. In the control basis approach, behaviour is composed online from a set of base controllers which represent generic control objectives. By varying the composition functions of the base controllers, different tasks can be achieved. In this thesis, control basis is designed and implemented on a mobile manipulator. Based upon the task, the control composition is derived from the control basis and applied to the robot. To demonstrate its operation, the results of multiple task-specific composition are observed on a dynamic robot simulator.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Web Application Integration Using Mashups</strong><br /> Monday, November 23, 2009<br /> Ronda Hilton<br /><p><a href="#" id="477-show" class="showLink" onclick="showHide(477);return false;">Read More</a></p></p><div id="477" class="more"><p><a href="#" id="477-hide" class="hideLink" onclick="showHide(477);return false;">Hide</a></p><p><strong>Abstract: </strong>The HTML DOM is the W3C standard data model for HTML documents.  A web page may be viewed as a tree structure with data nodes at each level, according to the HTML DOM,  to enable web applications to access it dynamically.  If a web page mashes up more than one web application, the data from one web application may serve as input to another.  The user may manually transfer such data piecemeal by using the mouse to cut and paste the displayed text.  Instead of this tedious and error-prone repetitious method of data transfer, the mashup may contain a GUI to a software agent which can dynamically integrate web applications so that data flows from one to another automatically.  The user indicates the source web application, e.g. by clicking one text item in a list, and similarly indicates the target web application, e.g. a form input field.  The agent traverses the source web application’s internal HTML DOM structure, finding all data nodes at the same level as the user-indicated data node, in an attempt to discern the selection intentions of the user.  The GUI displays the agent’s selected nodes in an array structure, which is part of the mashup.  The user may indicate from the array structure which array element to transfer into the target web application.  The GUI also provides a control for the user to enact the next iteration.  As proof of concept, we have built a mashup of two iframes that uses an agent which selects text and transfers it from one iframe into the form input fields of the other iframe.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A REPUTATION SYSTEM FOR ROBUST, STRUCTURED P2P SYSTEMS</strong><br /> Monday, November 23, 2009<br /> Apurv Ashok Dhadphale<br /><p><a href="#" id="478-show" class="showLink" onclick="showHide(478);return false;">Read More</a></p></p><div id="478" class="more"><p><a href="#" id="478-hide" class="hideLink" onclick="showHide(478);return false;">Hide</a></p><p><strong>Abstract: </strong>Structured peer-to-peer systems are distributed communication systems that typically use Distributed Hash Table (DHT) indexing to efficiently locate resources. These networks are highly scalable and can route messages correctly even in an extremely dynamic environment. Thus they form a substrate for large-scale, decentralized applications, including distributed data stores, group communication, content distribution or even DNS. However, these networks are vulnerable; even a small fraction of malicious nodes can bias the lookup results when they are present on a lookup path. In our thesis, we address this problem of reliably searching structured p2p networks. We propose a reputation system to identify good and bad lookup paths and thereby reduce the number of failed lookups, making the systems more robust. For our study, we apply this concept to the Salsa peer-to-peer communication system. Since Salsa is a DHT-based structured p2p system, it is possible to approximate the actual overlay network and lookup path, which is not possible with the other unstructured p2p networks like Bit Torrent. Each node builds its own reputation tree using the look-up results; due to the properties of the DHT, the closest results are considered to be good. These reputations are then used to select the nodes for lookup. Since we use the redundancy that is inherent in Salsa, there is no separate communication overhead to collect reputation. Also since the working of reputation system does not depend on peers, it is not subject to the attacks like bad-mouthing. We first study the effectiveness of the reputation system for a static Salsa environment in which the peers are fixed and then adapt the approach to dynamic environment where the peers join and leave randomly. We modified the existing continuous-time Salsa simulator to include a reputation system module. Our simulation results show that the number of failed lookups reduces by up to 90% in typical configurations. The results also demonstrate how the different system parameters can be changed to control reputation score and help decrease the number of failed lookups further.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Design and Analysis of Application Architecture for Opportunistic networks using Ad Hoc Wi-Fi</strong><br /> Monday, November 23, 2009<br /> Sankalp Shere<br /><p><a href="#" id="479-show" class="showLink" onclick="showHide(479);return false;">Read More</a></p></p><div id="479" class="more"><p><a href="#" id="479-hide" class="hideLink" onclick="showHide(479);return false;">Hide</a></p><p><strong>Abstract: </strong>In recent years, the number of smartphone users has increased by many folds. In fact, it is estimated to reach 100 million by 2013. Current generation of smartphones has better storage, battery, computing capabilities and they come equipped with short range communication technologies like Bluetooth and Wi-Fi. These improved capabilities coupled with staggering rise in number of users, has prompted growing interest in smartphone applications that help users to communicate with each other directly and over the internet. In particular as a result, when a large number of mobile users can communicate directly with each other during opportunistic contacts when they are physically proximate to each other, an opportunistic network is formed.  Consider scenarios where researchers are gathered at a research conference or shoppers are roaming in mall. It will benefit them significantly if they can automatically exchange information of their interest with neighboring users. For example, researchers can receive information about nearby researchers with similar research interests, and shoppers can get to know about discounts on products or brands they like. All these scenarios make perfect case for opportunistic networking applications.   In this thesis, we design and implement an application architecture that addresses the needs of the opportunistic networking applications. We provide innovative information publishing model that embeds application and device information in MAC layer frames and uses faster MAC layer device discovery mechanisms to gather application level information. Based on recent signal strength values and history of time of contact between nodes, we design different neighbor selection strategies and analyze their performance by conducting series of experiments in different environments. Finally we enhance an existing multicasting communication framework to implement connection establishment and one-hop message exchange in opportunistic networks.  Instead of relying on Bluetooth communication technology where many implementations exist for opportunistic networks, we focus on wireless LAN (Wi-Fi) technology owing to its longer communication range, hence longer opportunistic contact period, more contact opportunities and faster device discovery technique. As we will reveal, the difficulty of designing and implementing over Wi-Fi is significantly higher as compared those based on Bluetooth.  The first component of our architecture is neighbor discovery. In highly dynamic and ever changing opportunistic networks, efficient and timely neighbor discovery provides the foundation for any following data communications. In our design, we employ current signal strength values and history of time of contact information to facilitate neighbor discovery and selection. Using innovative beacon stuffing mechanism, we publish application and device information using MAC layer beacon frames, thus speeding up device discovery process.   The next component in our architecture addresses the need of communication among devices. We design connection establishment mechanism for a device to initiate connection with its best neighbor on unique Wi-Fi SSID network. Our architecture handles one-hop message exchange over UDP/IP protocol, also maintains communication message synchronicity. We keep track of all successful and unsuccessful communication sessions and assign credibility to neighbors for future encounters. We realize the intermittent nature of opportunistic communication and provide re-establishment mechanism for the broken communication links between neighboring devices.  To demonstrate and verify the performance of our architecture, we have implemented the prisonerís dilemma game as an example application. In this game, a mobile node finds suitable devices in the surroundings and plays autonomous games with its neighbors. Also, we have developed a device discovery application that lets user configure neighbor discovery parameters.  We have conducted series of functional and performance tests to analyze device discovery scheme, different neighbor selection mechanisms and the efficiency of communication between neighboring devices. Our results show that using our neighbor discovery and selection mechanisms, speed and performance of the opportunistic communication, especially neighbor discovery process can be significantly improved. Using signal strength and time of contact information, our neighbor selection strategies give great insight into still unexplored aspect of opportunistic networks ñ neighbor selection. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Sequential, Interleaved, and Concurrent Activity Recognition in Smart Environments</strong><br /> Thursday, November 19, 2009<br /> Roman Arora<br /><p><a href="#" id="475-show" class="showLink" onclick="showHide(475);return false;">Read More</a></p></p><div id="475" class="more"><p><a href="#" id="475-hide" class="hideLink" onclick="showHide(475);return false;">Hide</a></p><p><strong>Abstract: </strong>Recognizing human activities is an important feature for the development of context-aware applications that are so fundamental to enabling intelligent home environments. In order to do this, it is necessary to build models that can capture the human capability to perform sequential, interwoven, and concurrent activities; and to accurately recognize the observed patterns. Artificial intelligence algorithms based on Markov and Hidden Markov Models (HMM) have been explored in the past and have already proven valuable for activity recognition. In this thesis, we explore how to recognize sequential, interleaved, and concurrent activities in complex scenarios by using HMMs. We validate our algorithm on real sensor data collected at the Heracleia AtHome apartment testbed and the CASAS smart apartment testbed. Furthermore, we introduce a new performance metric to allow comparison amongst  the competing activity recognition algorithms. Finally, we develop an application for the visual design of smart environments and the analysis of different activity recognition algorithms.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A PRIORITY BASED PEER-TO-PEER BLUETOOTH FILE SHARING SYSTEM IN AN OPPORTUNISTIC NETWORK WITH PUSH-PULL MECHANISM AND FILE COMPRESSION</strong><br /> Monday, July 20, 2009<br /> Gautam Chavan<br /><p><a href="#" id="471-show" class="showLink" onclick="showHide(471);return false;">Read More</a></p></p><div id="471" class="more"><p><a href="#" id="471-hide" class="hideLink" onclick="showHide(471);return false;">Hide</a></p><p><strong>Abstract: </strong>As quoted by Heinz V. Bergen, Information is the seed for an idea, and only grows when it’s watered. Information is important for any individual for a school kid, a college teen, a working adult or an elderly person. Information is extremely important in today’s fast paced world, as everyone needs to keep up with the things that are happening around them before they become outdated. Information can mean different things for different people, for a school kid it might be the homework that he or she has to submit, for a college teen it could be information on the project due or it could be a new song from the latest album from Michael Jackson, for a working adult it could be, the important architecture or design documents of the product that he or she is currently working on, the share prices of companies, interest rates on loans, the weather in the next few hours, etc. We propose a priority based Bluetooth peer-to-peer file sharing system application for an opportunistic network, with pull and push mechanism and file compression. The connections between the devices are established on the fly and file transfer takes place within a short span of time. Also since its peer-to-peer if the file is present with more than one peer, we should simultaneously download the file from all the other peers who have the file, thereby increasing the overall data rate and the time taken to download a file. We introduce the concept of priority, where the user of a system can set the priority of the files for e.g. the user may mark the audio files as high priority, large video files as low priority, and wallpapers or images as medium priority. The files will be downloaded according to their priorities. We also introduce file compression mechanism where in the device compresses files in order to minimize bandwidth and also minimize the time taken to download thereby increasing the battery life of the mobile device. We also discuss the energy efficiency of the system, as the application runs on a battery constrained mobile device. We discuss theory and research leading to the proposed method and describe the software system built for this purpose and provide results of real-time experiments.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Grid Scheduling and Monitoring in the Context of ATLAS Experiment</strong><br /> Friday, July 10, 2009<br /> Tengkok Aaron Thor<br /><p><a href="#" id="468-show" class="showLink" onclick="showHide(468);return false;">Read More</a></p></p><div id="468" class="more"><p><a href="#" id="468-hide" class="hideLink" onclick="showHide(468);return false;">Hide</a></p><p><strong>Abstract: </strong>The technological advancements in the areas of computing and networking over the recent years have led to an emerging infrastructure known as Computational Grids, which provides users with the flexibility of pervasive access to enormous computational resources hosted at remote locations. Effective resource management and job scheduling poses a challenge when constraints such as resource utilization, response time, global and local policies need to be taken into account, while dealing with potentially independent sources of jobs, computational, and storage resources. It must be ensured that scheduling decisions made are still valid by the time a job is to be executed, with all the necessary resources remaining available.  In order to provide a more accurate scheduling and to obtain a better balance between micro and macro goals some status information about the resources needs to be obtained. However, this brings up another controversial issue which has plagued all dynamic scheduling communities: at what resolution monitoring should be performed. Since jobs are constantly submitted throughout the grid and resources are used for processing such jobs, acquired monitoring information should be updated frequently. On the other hand, monitoring too frequently takes up valuable resources and bandwidth which could otherwise be used for job execution. Thus, another objective is to reach a balance between the risk of having outdated resource status information (which leads to incorrect scheduling decisions) and performing too much monitoring (wasting limited resources). In a conventional grid environment, such as the ATLAS project [?40], system administrators are often required to monitor the activities of a selected group of preferred sites and submit jobs to those sites if deemed capable of processing such tasks. The sites chosen, however, may not necessarily be the best sites for processing the jobs. This results in underutilized resources and stagnant efficiency of sites as there is no global incentive for improving efficiency as well as remaining competitive. One of the main reasons for sub-optimal resource allocation is that when taking factors such as system resource utilization, response time, global and local policies into account while dealing with potentially independent sources of jobs, computational and storage resources, the job of managing resources and job scheduling becomes too tedious for a centralized entity to perform. On top of that, with the implementation of varying local policies, a centralized scheduling entity may not have access or control over such policies, hence it could only perform scheduling based on a best effort basis. It is often up to the job-receiving host to perform the final leg of scheduling, based on its locally defined policies. As such, scheduling within a grid is often a multi-tiered process where the job-submitting host performs its job scheduling to the best of its knowledge of the current state of the grid environment, while the receiving host takes over the final phase of the scheduling process. Dividing the task of scheduling amongst several sites would add the advantage of easing the load and complexity of performing scheduling at a single location. However, in order to motivate individual local domains in competing to become more efficient, in addition to being more aggressive in competing for accepting more jobs, some form of incentive mechanism could be applied.   In this work, we explore a decentralized combinatorial exchange scheme, as well as pull-based grid scheduling methodology which adopts the use of brokers with job advertisement and propagation within a grid environment. The main motivation for this scheme is to create an automated two tiered scheduling methodology to perform the tedious task of performing service discovery, and task scheduling at the global level, while performing resource monitoring, utilization and efficiency control at the local level. To achieve the best attainable optimization at any point in time, participating sites are to remain motivated to offer their best services based on the job submitting host?s preferred optimization settings. Global scheduling of jobs will be done at the broker level via a bidding process. The submitting host will have the privilege to choose the best available offer to suit its requirements. A pricing scheme is implemented as a trading mechanism in exchange for the services provided. This pricing mechanism will hence serve as a motivation for participating sites to compete for jobs so as to increase its overall wealth. As such, competing sites will be required to constantly monitor and improve their own resources, its utilization and efficiency so as to remain competitive.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>-</strong><br /> Wednesday, July 08, 2009<br /> Robert Walls<br /><p><a href="#" id="467-show" class="showLink" onclick="showHide(467);return false;">Read More</a></p></p><div id="467" class="more"><p><a href="#" id="467-hide" class="hideLink" onclick="showHide(467);return false;">Hide</a></p><p><strong>Abstract: </strong>Covert timing channels provide a way to surreptitiously leak information from an entity in a higher-security level to an entity in a lower level. The difficulty of detecting or eliminating such channels makes them a desirable choice for adversaries that value stealth over throughput. When one considers the possibility of such channels transmitting information across network boundaries, the threat becomes even more acute. A promising technique for detecting covert timing channels focuses on using entropy-based tests. This method is able to reliably detect known covert timing channels by using a combination of entropy and conditional entropy to detect anomalies in shape and regularity, respectively. This dual approach is intended to make entropy-based detection robust against both current and future channels. In this work, we show that entropy-based detection can be defeated by a channel that intelligently manipulates the metrics used for detection. Specifically, we propose a new covert channel that uses a portion of the inter-packet delays in a compromised stream to help &quot;smooth out&quot; the symbol probability distortions detected by the entropy test. Our experimental results suggest that this channel can successfully evade entropy-based detection and other known tests while maintaining reasonable throughput.   Furthermore, we investigate how to set the channel parameters in order to achieve a desired level of entropy.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Query Auditing Against Partial Disclosure</strong><br /> Tuesday, April 14, 2009<br /> Mayur Motgi<br /><p><a href="#" id="464-show" class="showLink" onclick="showHide(464);return false;">Read More</a></p></p><div id="464" class="more"><p><a href="#" id="464-hide" class="hideLink" onclick="showHide(464);return false;">Hide</a></p><p><strong>Abstract: </strong>Many government agencies, businesses, and nonprofit organizations need to collect, analyze, and report data about individuals in order to support their short-term and long-term planning activities. Statistical Databases therefore contain confidential information such as income, credit ratings, type of disease, or test scores of individuals. Such data are typically stored online and analyzed using sophisticated database management systems (DBMS) and software packages. On the one hand, such database systems are expected to satisfy user requests of aggregate statistics related to non-confidential and confidential attributes. On the other hand, the system should be secure enough to guard against a user’s ability to infer any confidential information related to a specific individual represented in the database.. A major privacy threat is the adversarial inference of individual (private) tuples from aggregate query answers. Most existing work focuses on the exact disclosure problem, which is inadequate in practice. We propose a novel auditing algorithm for defending against partial disclosure.             We introduce ENTROPY-AUDITING, an efficient query auditing algorithm for partial disclosure which supports a mixture of common aggregate functions. In particular, we classify aggregate functions into two categories: MIN-like (e.g., MIN and MAX) and SUM-like (e.g., SUM and MEDIAN), and support a combination of them. Our proposed scheme utilizes an exact-auditing algorithm as a primitive function, and supports a combination of queries with various aggregate functions (e.g., SUM, MIN, MAX). We also present a detailed experimental evaluation of our PARTIAL-AUDITING approach.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Pseudo-Hierarchical Ant-Based Clustering Using a Heterogeneous Agent Hierarchy and Automatic Boundary Formation</strong><br /> Tuesday, April 14, 2009<br /> Jeremy Bernard Brown<br /><p><a href="#" id="465-show" class="showLink" onclick="showHide(465);return false;">Read More</a></p></p><div id="465" class="more"><p><a href="#" id="465-hide" class="hideLink" onclick="showHide(465);return false;">Hide</a></p><p><strong>Abstract: </strong>The behavior and self-organization of ant colonies has been widely studied and served as the inspiration and source of many swarm intelligence models and related clustering algorithms. Unfortunately, most models that directly mimic ants produce too many clusters and converge too slowly. A wide range of research has attempted to address this issue through various means, but a number of problems remain: 1) Ants must still physically move from one cluster to another through intermediate locations, 2) current methods for remote relocation of an item only consider one movement at time to a particular location and do not consider patterns in movement to that location, and 3) while current methods have included effective bulk item movement, they do not provide efficient movement while still maintaining the self-organizing nature of ant-based clustering which is essential for filtering out outliers and allowing effective splitting of clusters.  This thesis addresses these problems by proposing a new algorithm for ant-based clustering. In this algorithm ants maintain a movement zone around each cluster, keeping ants from spending time in locations where there is nothing to do. These movement zones around individual clusters are used to elect representatives that are responsible for all long distance movement. Each elected representative can, probabilistically, pass an object it has to any other elected representative. Since each cluster has approximately one representative at any given time, the search space for placing items over a long distance is reduce to the number of clusters. So instead of having all the ants use up time wandering around the map, one ant can be responsible for sampling all local clusters. This provides an infrastructure by which clusters can efficiently merge over long distances and better clusters for items in the wrong clusters can be found without having to travel to them. While this model does require a considerable overhead as compared with contemporary algorithms, a better convergence rate can be achieved because the efficient bulk movement of items and sampling at the cluster level.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>TEMPORAL POTENTIAL FUNCTION APPROACH FOR PATH PLANNING IN DYNAMIC ENVIRONMENTS</strong><br /> Tuesday, December 30, 2008<br /> Vamsikrishna Gopikrishna<br /><p><a href="#" id="463-show" class="showLink" onclick="showHide(463);return false;">Read More</a></p></p><div id="463" class="more"><p><a href="#" id="463-hide" class="hideLink" onclick="showHide(463);return false;">Hide</a></p><p><strong>Abstract: </strong>A Dynamic environment is one in which either the obstacles or the goal or both are in motion. In most of the current research, robots attempting to navigate in dynamic environments use reactive systems. Although reactive systems have the advantage of fast execution and low overheads, the tradeoff is in performance in terms of the path optimality.  Often, the robot ends up tracking the goal, thus following the path taken by the goal, and deviates from this strategy only to avoid a collision with an obstacle it may encounter. In a path planner, the path from the start to the goal is calculated before the robot sets off. This path has to be recalculated if the goal or the obstacles change positions. In the case of a dynamic environment this happens often. One method to compensate for this is to take the velocity of the goal and obstacles into account when planning the path. So instead of following the goal, the robot can estimate where the best position to reach the goal is and plan a path to that location. In this thesis, we propose such a method for path planning in dynamic environments. The proposed method uses a potential function approach that considers time as a variable when calculating the potential value. This potential value of a particular location and time value indicates the probability that a robot will collide with an obstacle, assuming that the robot executes a random walk from that location and that time onwards. Thus the robot plans a path by extrapolating the object&#039s motion using current velocities and by calculating the potential values up to a look-ahead limit that is determined by calculating the minimum path length using connectivity evaluation and then determining the utility of expanding the look-ahead limit beyond the minimum path length. The method is fast, so the path can be re-planned with very little overhead if the initial conditions change at execution time. The thesis will discuss how the potential values are calculated and how a suitable look-ahead limit is decided. Finally the performance of the proposed method is analyzed in a simulated environment.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Adapting Harmonic function path planning: To reflect user motion preferences</strong><br /> Tuesday, November 25, 2008<br /> Giles Da Silva<br /><p><a href="#" id="457-show" class="showLink" onclick="showHide(457);return false;">Read More</a></p></p><div id="457" class="more"><p><a href="#" id="457-hide" class="hideLink" onclick="showHide(457);return false;">Hide</a></p><p><strong>Abstract: </strong>Every human in the real world has a unique motion preference while moving to achieve a given task. These preferences could be expressed by moving in a straight line, following the wall along a corridor, avoiding sharp turns, preferring hard flat surfaces over damp uneven surfaces, choosing the shortest path to the goal or by giving a high priority towards safety by maintaining a definite distance from obstacles. To automatically incorporate user preferences into motion planning we could extract the trajectories taken by the user to achieve a task in a given local environment and attempt to use them in potentially similar environments. However these trajectories can not be easily generalized and the intermediate paths can generally not be inferred by interpolation between similar paths since the  path may not always lead to the goal or could sometimes even collide with an obstacle. As a result a motion planner using sample trajectories and path interpolation would lose the characteristic of a good path planner and may also fail to correctly represent user&#039s motion preferences while transferring such trajectories into more dissimilar environments. Motivated by this, the goal of this research is to design a path planner that is able to transfer user motion preferences  in a parametric form to new similar local environments and to generate paths that are smooth, complete and have no local minima or maxima.   In this research we modify a harmonic function path planner to model the preferences of a user’s motion as parameters and then generate new paths based on these preferences for potentially similar environment configurations. To model such preferences the representing parameters have to be learned using a machine learning algorithm. The algorithm extracts the data for the user’s trajectory whose preference we are trying to capture and computes the desired path(the harmonic function gradient) direction from every trajectory point. Next it initializes the parameters to default values that generate a generic path to the goal. We then modify and update these parameters until a path is generated that matches the desired direction followed by the user. These parameters now represent the user’s motion preference in that local environment and can be transferred into new, similar environments. We then input these parameters into our path planner to generate a path for this new environment. Since the path is generated by a harmonic function path planner it is complete and has no local minima or maxima and the user is assured of reaching the goal if a path exist.   The customization of the path planner to learn user motion preferences could have potential applications in autonomous vehicles, semi autonomous wheel chairs, remote control of robotic systems or the creation of custom game characters. In the context of this research we simulate the motion of a robot by learning from its trajectories to generate a gradient that matches the desired path. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Evolutionary Approach to Optimal Path Planning Problem in Sensor Networks</strong><br /> Tuesday, November 25, 2008<br /> Awais Iqbal<br /><p><a href="#" id="461-show" class="showLink" onclick="showHide(461);return false;">Read More</a></p></p><div id="461" class="more"><p><a href="#" id="461-hide" class="hideLink" onclick="showHide(461);return false;">Hide</a></p><p><strong>Abstract: </strong>Path planning is an important task in sensor networks and robotics and has received considerable attention in the past. This paper describes an evolutionary approach to find optimal path for a mobile node in a grid of stationary sensors. The aim of the work is to study the behavior of an evolutionary algorithm that will discover the best possible path within the required constraints. We assume that sensors are uniformly deployed in a predefined deployment area. A mobile node walks through the area, communicating with the sensors in the range at regular intervals while keeping a bound on maximum distance traveled. We then compare our result using Cramer Rao Bound (CRB) for unbiased evaluation. Multiple genetic operators including mutation, splicing, selection and cross-over are used to create new paths which are evaluated for optimality. The paper also introduces some modified operators including removing an area from path, adding a new area to path, greedy shortes t path and others.  Extensive simulations are performed in order to evaluate the methodology. If the new generation contains a better path, it is saved and used in subsequent generations while discarding the least optimal path. However, if no better solution is found, the entire generation goes through the process again. The process repeats until the improvements with each generation become insignificant for a considerable period of time thus resulting in an optimal path.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A PARTICLE FILTER BASED FRAMEWORK FOR INDOOR WIRELESS LOCALIZATION USING CUSTOM IEEE 802.15.4 NODES</strong><br /> Tuesday, November 25, 2008<br /> Vijay Dixit<br /><p><a href="#" id="462-show" class="showLink" onclick="showHide(462);return false;">Read More</a></p></p><div id="462" class="more"><p><a href="#" id="462-hide" class="hideLink" onclick="showHide(462);return false;">Hide</a></p><p><strong>Abstract: </strong>Locating people close to real-time and with acceptable precision has always been an important part of any organization or industry, especially in law enforcement, manufacturing, healthcare, and logistics. Technologies that have the ability to locate objects or people are called Real Time Location Systems (RTLS). They typically use small low-power transmitters called tags attached to assets (or worn by people) as well as sets of readers that map the location of these tags. Systems that map the longitude and attitude of an object are geo-location systems and generally use the Global Positioning System (GPS) for location mapping. GPS could be used as the location determination portion of an RTLS system (relaying that information would have tor to rely on another system); unfortunately, GPS signals do not penetrate buildings well and thus GPS will in general not work inside buildings and in dense urban areas. Thus, there is a need for RTLS systems that work in GPS-denied environments.             Several technologies have been proposed to create Real Time Location Systems. Some use dedicated tags and readers while others use existing WLAN networks and add RTLS ability to those networks. We propose a probabilistic approach to localization, based upon Received Signal Strength (RSSI) and inertial information coming from tags (e.g., accelerometer and rotational sensor readings). Global localization is a flavor of localization in which the device is unaware of its initial position and has to determine the same from scratch.              The first step to localize tags in this work involves building a wireless measurement model of the tag with respect to some anchor nodes (access points). The model is built by measuring the RSSI readings of the mobile node relative to the access point at various distances and orientations (rotation away from the access point). These readings form a sample set for sequential Monte-Carlo sampling. Next, a posterior probability distribution for the location of the wireless device is computed over the entire area using Monte-Carlo sampling based Bayesian filtering, also known as Particle filters. Location estimates may then be determined from this distribution using the maximum density point or other parameters depending on the estimate needed.              We discuss theory and research leading to the proposed method and describe the experimental hardware/firmware/software system built for the purposes of this work and provide results of real-life experiments. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Use of Wireless Signal Characteristics for Mobile Robotics Localization and Mapping ("SLAM")</strong><br /> Monday, November 24, 2008<br /> Joshua Davies<br /><p><a href="#" id="458-show" class="showLink" onclick="showHide(458);return false;">Read More</a></p></p><div id="458" class="more"><p><a href="#" id="458-hide" class="hideLink" onclick="showHide(458);return false;">Hide</a></p><p><strong>Abstract: </strong>Mobile robotics is an interesting and challenging emerging field in computer science.  To properly achieve true autonomy, a robot must be able to determine where it is in relationship to a global frame of reference and it must be able to do so with a minimum of interaction from human operators and impose as few constraints on its surroundings as possible.  The uncertain nature of mobility and perception requires that advanced probabilistic inference techniques be applied to minimize error. This work examines the applicability of radio-frequency signals to the mobile robot localization problem to localize quickly over a wide area.  Particle filtering techniques are employed to adjust for anticipated errors in both the motion model and the perception model.  Radio hardware designed to capture time of flight and received signal-strength indicators (RSSI) is used to infer relative distances and triangulate the most likely position of a mobile node, taking into account a priori knowledge about past poses. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>DynaCet: A Minimum-Effort Driven Dynamic Faceted Search System Over Structured Databases</strong><br /> Friday, November 14, 2008<br /> Haidong Wang<br /><p><a href="#" id="454-show" class="showLink" onclick="showHide(454);return false;">Read More</a></p></p><div id="454" class="more"><p><a href="#" id="454-hide" class="hideLink" onclick="showHide(454);return false;">Hide</a></p><p><strong>Abstract: </strong>In this thesis, we propose minimum-effort driven navigational techniques for enterprise database systems based on the faceted search paradigm. Our proposed techniques dynamically suggest facets for drilling down into the database such that the cost of navigation is minimized. At every step, the system asks the user a question or a set of  questions on different facets and depending on the user response, dynamically fetches the next most promising set of facets, and the process repeats. Facets are selected based on their ability to rapidly drill down to the most promising tuples, as well as on the ability of the user to provide desired values for them. Our facet selection algorithms also work in conjunction  with any ranked retrieval model where a ranking function imposes a bias over the user preferences for the selected tuples.  Our methods are principled as well as efficient, and our experimental study validates their effectiveness on several application scenarios.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient Processing of Set Queries Using Bitmap Index</strong><br /> Monday, July 14, 2008<br /> Muhammad Assad Safiullah<br /><p><a href="#" id="448-show" class="showLink" onclick="showHide(448);return false;">Read More</a></p></p><div id="448" class="more"><p><a href="#" id="448-hide" class="hideLink" onclick="showHide(448);return false;">Hide</a></p><p><strong>Abstract: </strong>Growing complexity of enterprise-wide data and business processes necessitates the efficiency of complex decision support set queries. However, contemporary DBMS remain unsuccessful in handling set queries efficiently. In this thesis we propose efficient set query processing methods using bitmap index. The methods use bitmap vectors to represent attributes values in binary format. The methods test groups within a schema in a hierarchical fashion. Satisfying groups are bisected further and checked recursively while non-satisfying groups are pruned resulting in significant reduction in response times. In addition, our iterative implementation avoids the inefficiency that can be introduced by recursive implementation by reading the same bitmap vector for intersection many times. We also introduce pre-processing methods to reduce the complexity of the bitmap vectors, thus to improve the efficiency. Our implementation is based on FastBit, an open-source efficient compressed bitmap index framework. Experimental results on large datasets and comparison with results from PostgreSQL prove that our approach is superior owing to the fact that we are able to discard non-satisfying groups and capably optimize complex queries.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>CAP: A Context-Aware Privacy Protection System for Location-Based Services</strong><br /> Thursday, July 10, 2008<br /> Aniket Pingley<br /><p><a href="#" id="445-show" class="showLink" onclick="showHide(445);return false;">Read More</a></p></p><div id="445" class="more"><p><a href="#" id="445-hide" class="hideLink" onclick="showHide(445);return false;">Hide</a></p><p><strong>Abstract: </strong>Location Based Services (LBS) are information services that provide users with customized contents, such as the nearest restaurants/hotels/clinics, retrieved from a dedicated spatial database. They make use of technologies such as Global Positioning System (GPS), triangulation/triliteration etc. to get the geographical position of the user. Since the queries on spatial database include the user&#039s current location, LBS may raise serious concerns on the user&#039s location privacy. If disclosed, a user&#039s location information may be misused in many ways by a malicious adversary who has access to the LBS server or even by the LBS provider. Therefore, our aim in this thesis is to provide a user with a system to protect her location privacy, without impeding the LBS.   <br /><br /> In this thesis, we address issues related to privacy protection for location-based services (LBS) without trusted-third parties (e.g., anonymizers). There are two critical challenges to such a system. First, the degree of privacy protection and LBS accuracy depends on the context, such as population and road density, around a user&#039s location e.g. for an user in rural area, we use more perturbation than for an user in downtown to achieve the same level of privacy protection and LBS accuracy. Second, location privacy may be breached through not only an LBS query, but also via the network traffic that carries the query payload, leading to a dual requirement on data privacy and communication anonymity. In order to address these challenges, we introduce CAP, a Context-Aware Privacy-preserving LBS system with integrated protection for data privacy and communication anonymity. For data privacy, we propose a projection-based location data perturbation algorithm, called Various-grid-length Hilbert Curve (VHC) - mapping, which provides universal guarantees on privacy protection and LBS accuracy for all locations with diverse context. VHC-mapping is designed to require minimal storage and computational cost. For communication anonymity, CAP uses a revised version of Tor. In this revised version, we address the issue of QoS degradation due to Tor&#039s random routing protocols. By exploiting the dual requirement with data privacy, we propose a set of new routing algorithms with significantly enhanced QoS. We have implemented a prototype of CAP, which can be readily integrated with existing LBS. Our theoretical analysis and experimental results validate CAP&#039s effectiveness on privacy protection, LBS accuracy, and communication QoS.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>High-Level Constraint Support for Combinatorial Testing</strong><br /> Thursday, July 03, 2008<br /> Anthony Opara<br /><p><a href="#" id="446-show" class="showLink" onclick="showHide(446);return false;">Read More</a></p></p><div id="446" class="more"><p><a href="#" id="446-hide" class="hideLink" onclick="showHide(446);return false;">Hide</a></p><p><strong>Abstract: </strong>Combinatorial testing constructs test cases by combining different input parameter values based on some effective combinatorial strategy. This software testing approach  has displayed very promising attributes and is rapidly  gaining popularity in recent years. However, existing  work does not provide adequate support for constraint  handling. Constraints are often specified as part of an  input parameter model and they may be due to several  reasons such as incompatibility between certain hardware  and software components. A test generation algorithm  needs to take these constraints into account during the test generation process to exclude combinations that  are invalid from the domain semantics. <br /><br /> In this thesis, we describe a general approach to handling constraints for combinatorial testing. Our approach  includes a formal notation that allows the user to specify  constraints at a higher level of abstraction. We discuss  how to deal with the problem of &quot;future conflicts&quot;, which  arises when a selected value satisfies all the constraints  at one point in the test generation process but fails to  satisfy some constraints in the future. Our approach can  be combined with different combinatorial test generation  algorithms, and we demonstrate this by showing how to  extend an existing combinatorial test generation algorithm,  called In-Parameter-Order-General (IPOG), to handle  constraints. We describe a Java based combinatorial testing tool called FireEye, which implements an extended version  of IPOG that supports constraint handling, and report  some experimental results that demonstrate the  effectiveness of our approach.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Automated Integration of Biomedical Information for the Support of Genome-wide Association Studies</strong><br /> Thursday, June 26, 2008<br /> Abhijit R. Tendulkar<br /><p><a href="#" id="447-show" class="showLink" onclick="showHide(447);return false;">Read More</a></p></p><div id="447" class="more"><p><a href="#" id="447-hide" class="hideLink" onclick="showHide(447);return false;">Hide</a></p><p><strong>Abstract: </strong>Whole genome association studies of the genetic underpinnings of complex phenotypes, and human diseases in particular, have been steadily gaining momentum over the past several years. Yet, the number of polymorphic sites in the human genome, including, but not limited to, Single Nucleotide Polymorphisms (SNPs) is so large that identifying the combination of these few that have a significant effect on the condition of interest remains an overwhelming task. In this thesis, we present a new networked solution, and a program GeneNAB implementing it, to the computational identification and ranking of SNPs likely to be relevant for the phenotype of interest, genome-wide. We expect that the output of this program will be useful to guide further laboratory and clinical studies of these SNPs.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>The Dynamics of Salsa: A Structured Approach to Large-Scale Anonymity</strong><br /> Monday, June 23, 2008<br /> Safwan Mahmud Khan<br /><p><a href="#" id="440-show" class="showLink" onclick="showHide(440);return false;">Read More</a></p></p><div id="440" class="more"><p><a href="#" id="440-hide" class="hideLink" onclick="showHide(440);return false;">Hide</a></p><p><strong>Abstract: </strong>Anonymity provides a technical solution for protecting one’s online privacy. Highly distributed peer-to-peer (P2P) anonymous systems should have better distribution of trust and more scalability than centralized approaches, but existing systems are vulnerable to attacks as they require nodes to have global knowledge of the system. To overcome these problems and to provide more secure, distributed organization for P2P anonymity systems, prior work proposed the Salsa system. Salsa is designed to select nodes to be used in anonymous circuits randomly from the full set of nodes, even though each node has knowledge of only a small subset of the network. It uses randomness, redundancy and bounds checking while performing lookups to prevent malicious nodes from returning false information without detection.   In this thesis, we further investigate the dynamics of the Salsa system and propose to handle important system-level functionalities without giving advantages to attackers. We propose algorithms for joining and leaving of a node, splitting a group when its size reaches a maximum threshold, merging of two groups when a group’s size reaches a minimum threshold and updating global contacts of nodes locally. We have introduced more randomness in the lookup procedure and made bounds checking more flexible. Finally, we implemented these dynamic events and developed a complete continuous time simulator for Salsa. Using this simulator, we present simulation results that show that still Salsa continues to have good lookup success in a dynamic environment with modest overheads in the system. These results also demonstrate the stability of Salsa in presence of many peers joining and leaving.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Stepping stone NEtwork Attack Kit (SNEAK) for evading timing-based detection methods under the cloak of constant rate multimedia streams</strong><br /> Thursday, June 05, 2008<br /> Jaideep Padhye<br /><p><a href="#" id="439-show" class="showLink" onclick="showHide(439);return false;">Read More</a></p></p><div id="439" class="more"><p><a href="#" id="439-hide" class="hideLink" onclick="showHide(439);return false;">Hide</a></p><p><strong>Abstract: </strong>With the advent of Internet, network based security threats  have been constantly on the rise. Till now, the source of an attack could be traced by studying the system logs and the source IP address of the attack can be used to identify and prosecute the attacker. To avoid getting traced and to mislead the forensic investigators, the attackers usually compromise weaker nodes on less secure networks and use them as stepping stones to attack the victim. The owner of the compromised node may not even have the knowledge about its presence in the attack path. But still the owner might end up being prosecuted for taking part in malicious activity.  Hence, it is of paramount importance to research the stepping stone attack attribution techniques so that the real attackers can be apprehended. An interesting approach towards detecting stepping stone attack is to try and correlate incoming and outgoing streams from the stepping stone. The timing-based watermarking of packets streams has been effective even against the low latency anonymizing systems. But this approach places some unrealistic restrictions on the ability of the attacker. In prior work efforts were made to loosen these assumptions and an algorithm was proposed which used buffering and chaff along with selective dropping of packets to severely degrade detection.  In this thesis, we propose the Stepping stone NEtwork Attack Kit (SNEAK) for evading timing-based detection methods under the cloak of constant rate multimedia streams. It primarily consists of two algorithms which the attacker can use to evade detection. The first algorithm which is a modified version of the original algorithm is better in terms of robustness and the second algorithm provides better usability. Both the algorithms are suitable for practical use depending on the needs of the attacker. We defined metrics for robustness and usability and also studied trade off between them. We implemented a prototype of the SNEAK system and tested it on the on the PlanetLab network. Our prototype provides reliable transmission and reasonable performance for shell commands over at least two stepping stones and the traffic has the characteristics of a constant multimedia stream. We tested the effectiveness of SNEAK against centroid interval based watermarking technique which is currently the best available timing-based detection technique. The experimental results indicate that timing information embedded in the incoming stream is completely eliminated in the outgoing stream. The results also demonstrate that SNEAK is suitable for practical use without affecting overall usability of the system. The also indicate that SNEAK can be effective against all timing based detection techniques. The intent of this work is to demonstrate the need to consider the true potential of the attacker and develop better detection methods to defeat such attacks. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Protecting Location Privacy In Sensor Networks Against A Global Eavesdropper</strong><br /> Monday, June 02, 2008<br /> Kirankumar Mehta<br /><p><a href="#" id="438-show" class="showLink" onclick="showHide(438);return false;">Read More</a></p></p><div id="438" class="more"><p><a href="#" id="438-hide" class="hideLink" onclick="showHide(438);return false;">Hide</a></p><p><strong>Abstract: </strong>While many protocols for sensor network security provide confidentiality for the content of messages, contextual information usually remains exposed. Such contextual information can be exploited by adversaries to derive sensitive information such as the location of destinations and locations of monitored objects in critical sensor network applications. Attacks on these components can affect working of a network and cause monetary losses and endanger lives.The existing techniques defend the leakage of location based information only from a weak adversary, who sees only local network traffic. All the recent studies on providing location privacy have focused separately on destination and source location privacy. We first argue that the existence of a strong adversary, the global eavesdropper, is realistic in practice and can defeat existing techniques. We then formalize the location privacy issues for destination and monitored objects under this strong adversary model and prove a lower bound on the amount of communication overhead needed for achieving a certain level of location privacy.  We present techniques which provide location privacy to monitored objects and destinations. Our techniques provide trade-offs between privacy, communication cost and latency.  Through analysis and simulation, we demonstrate that the proposed techniques are efficient and effective in protecting the locations of destinations and monitored objects from the adversary.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>HIERARCHICAL REINFORCEMENT LEARNING USING AUTOMATIC TASK DECOMPOSITION AND EXPLORATION SHAPING</strong><br /> Thursday, April 17, 2008<br /> Predrag Djurdjevic<br /><p><a href="#" id="433-show" class="showLink" onclick="showHide(433);return false;">Read More</a></p></p><div id="433" class="more"><p><a href="#" id="433-hide" class="hideLink" onclick="showHide(433);return false;">Hide</a></p><p><strong>Abstract: </strong>Reinforcement learning agents situated in real world environments have to be able to address a number of challenges in order to succeed at accomplishing a wide range of tasks over their lifetime. Among these, such systems have to be able to extract control knowledge from already learned tasks and apply them to subsequent ones in order to allow the agent to accomplish the new task faster and to accelerate the learning of an optimal policy. To address skill reuse and skill transfer, a number of approaches using hierarchical state and action spaces have been introduced recently which build on the idea of transferring the previously learned policies and representations to model and control the new task. However, while such transfer of skills can significantly improve learning times, it also poses the risk of “behavior proliferation” where the increasing set of available reusable actions makes it incrementally more difficult to determine a strategy for a new task. To address this issue, it is important for the agent to have the capability to analyze new tasks and to have a means of predicting the utility of an action or skill in a new context prior to learning a policy for the task. The former here implies an ability to decompose the new task into known subtasks while the latter implies the availability of an informed exploration policy used to find the new goal and to more efficiently learn a corresponding policy. This thesis presents a novel approach for learning task decomposition by learning to predict the utility of subgoals and subgoal types in the context of the new task, as well as for exploration shaping by predicting the likelihood with which each available action is useful in the given task context. To achieve this, the approach presented here uses past learning experiences to acquire set of utility functions that encode relevant knowledge about useful subgoals and skills and applies them to shape the search for the optimal policy for the new task. Acceleration is achieved by focusing the search on contextually identifiable subgoals and actions/skills that have been learned to be valuable in the context of optimal policies in the previously encountered worlds. Performance increase is achieved here both in terms of the time required to reach the task’s goal the first time and time required to learn an optimal policy, which is demonstrated in the context of navigation and manipulation tasks in a grid world domain.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Assistive Navigation Paradigm for Semi-Autonomous Wheelchairs Using Force-Feedback and Goal Prediction</strong><br /> Thursday, April 17, 2008<br /> John Staton<br /><p><a href="#" id="435-show" class="showLink" onclick="showHide(435);return false;">Read More</a></p></p><div id="435" class="more"><p><a href="#" id="435-hide" class="hideLink" onclick="showHide(435);return false;">Hide</a></p><p><strong>Abstract: </strong>As computer technology advances and facilitates an increased amount of autonomous sensing and control, the interface between the human and autonomous computer systems becomes increasingly important. This applies in particular in the realm of service robotics where robots with increasing levels of autonomy are constantly interacting with human users and where it is paramount to have an efficient way to convey user intentions and commands to the robot system, integrate them into the robot’s action and control, and to indicate robot control choices to the user in a “natural”, intuitive way. One particular example where semi-autonomous operation of a system has been shown to be of major benefit is in the case of advanced wheelchairs which benefit from local autonomy and fine control capabilities of sensor-driven computer control systems but have to be under the higher-level control of the disabled user. This thesis investigates technologies aimed at facilitating the integration of such autonomous path planning capabilities with intuitive human control using a force-feedback interface. While force-feedback has been applied to artificially intelligent wheelchairs, only the surface of the capabilities of this interface mechanism has been scratched. This thesis seeks to expand upon the earlier work in this field by developing the idea of force-feedback not just as a tool for obstacle avoidance, but also as a tool for guiding the wheelchair user to their goal.  To this point, harmonic function path planning has been integrated to create a new, more robust force-feedback paradigm. A force-feedback joystick has been used to communicate information from the user to the robot control system which uses this to infer and interpret the user’s navigation intentions as well as from the harmonic function-based autonomous control system to the user to indicate the system’s suggestions. To demonstrate and evaluate its capabilities his new paradigm has been implemented within the Microsoft Robotics Studio framework and tested in a simulated mobile system, with positive results.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>FluSim: An Influenza Virus Molecular Infection Model and  Discrete Event, Stochastic Simulation</strong><br /> Friday, April 11, 2008<br /> Richard Burke Squires<br /><p><a href="#" id="434-show" class="showLink" onclick="showHide(434);return false;">Read More</a></p></p><div id="434" class="more"><p><a href="#" id="434-hide" class="hideLink" onclick="showHide(434);return false;">Hide</a></p><p><strong>Abstract: </strong>Influenza virus is responsible for the greatest pandemic  in human history causing 20 - 40 million deaths worldwide  during the 1918 flu season. In 1997 fears of a future  pandemic arose with the discovery of a new strain of H5N1  avian influenza. In an effort to better understand the  dynamics of influenza infection a new model of influenza  virus infection at the molecular level has been developed.  Comprising nineteen stages and five molecular types (mRNA,  cRNA, vRNA, vRNP and proteins) the FluSim model consists  of the major contributing pathogen factors to influenza infection.  A discrete event, stochastic simulation based upon this  model provides the dynamic implementation of the model  utilizing the SimJava framework. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>VISUALIZATION OF MONITOR-BASED EXECUTIONS</strong><br /> Friday, March 07, 2008<br /> Keerthika Koteeswaran<br /><p><a href="#" id="428-show" class="showLink" onclick="showHide(428);return false;">Read More</a></p></p><div id="428" class="more"><p><a href="#" id="428-hide" class="hideLink" onclick="showHide(428);return false;">Hide</a></p><p><strong>Abstract: </strong>Concurrent programs play a vital role in the world of software engineering. Concurrent programs are used in various fields such as safety critical applications, applications involving advanced graphical user interface, operating system implementation etc. Concurrent programs due to their inherent multi-threaded nature pose a challenge to software engineers not only in coding but also in ensuring data consistency and accuracy. One of the important software constructs used by concurrent programmers to handle critical sections and data synchronization is monitors. A monitor is an encapsulation of shared data, operations on the data and any synchronization required to access the data~cite{CarTai06:ModMulti}. Monitors are widely used by concurrent programmers since it guarantees mutual exclusion and can be implemented to ensure synchronization between all the threads in a multi-threaded application.  All concurrent programs are unpredictable in nature since the output directly depends on the sequence of thread execution. This unpredictability poses a great challenge to concurrent programmers. This thesis mainly proposes and implements a method to visualize the working of monitors in concurrent programs. This would facilitate concurrent programmers to view, understand, analyze and test the monitor implemented in their program. The GUI application is incorporated with JSWAT which is a open-source graphical debugger front end based on Java Platform Debugger Architecture. The motive behind integrating the visualization application and JSWAT is to provide useful features like sophisticated breakpoints, colorized code display, panels displaying call stacks, visible variables apart from visualization of monitor execution. The visualization application analyzes each statement in the code to capture data pertinent to visualization. This data is communicated to the GUI which maps it to the appropriate visualization rule. The visualization rules determine where each thread is to be positioned in the GUI. When the concurrent program is executed in the debugger, the user would be able to view the different threads in different positions in the GUI. The positions would directly correspond to the states of the threads with respect to the monitor.  Visualizing the interaction of the different threads with each other and with the monitor would facilitate a concurrent programmer to analyze the program and ensure its accuracy and correctness. This tool can also be used as a teaching aid to instruct software engineering students and novice concurrent programs by helping them to visualize, understand and appreciate the working of concurrent programs in general and monitors in particular. This tool aims to contribute towards easing the burden of concurrent programmers by enhancing their understanding of monitor based executions. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong> ENHANCING JSWAT FOR MONITOR-BASED EXECUTIONS</strong><br /> Tuesday, March 04, 2008<br /> Arun Ramani<br /><p><a href="#" id="422-show" class="showLink" onclick="showHide(422);return false;">Read More</a></p></p><div id="422" class="more"><p><a href="#" id="422-hide" class="hideLink" onclick="showHide(422);return false;">Hide</a></p><p><strong>Abstract: </strong>Concurrent programs contain more than one thread that execute concurrently to accomplish a particular task. Since the threads in a concurrent program work together to accomplish a common goal, they share the data, code, resources and address space of their process. This reduces the overhead involved in creating and managing the threads but leads to side-effects like race conditions, critical section problem and deadlocks. There are several operating system constructs to solve these issues such as locks, semphores, monitors etc. Locks are associated with each object to control access of shared resources. Semaphores can be described as counters used to control access to shared resources. A monitor by definition encapsulates shared data, all the operations on the data and any synchronization required for accessing the data.  JSwat is a stand-alone graphical Java debugger front-end that uses the Java Platform Debugger Architecture. It has features including sophisticated breakpoints; colorized source code display with code navigator; byte code viewer; movable display panels showing threads, call stack, visible variables, and loaded classes; command interface for more advanced features; and Java-like expression evaluation, including method invocation. These advanced features make JSWAT an ideal debugger for a Java concurrent programmer. The main disadvantage of concurrent programs is that they are extremely difficult to test and debug. This is because multiple executions of a concurrent program with the same input may produce different results. This nondeterministic execution behavior creates several problems during the testing and debugging cycle of a concurrent program.  To alleviate this problem to a certain extent this thesis proposes integrating a visualization tool with JSWAT for viewing the status of the monitor and the different threads in a concurrent program during runtime. The visualization will be viewed in a panel in the graphical user interface of JSWAT. When a concurrent programmer can view the status of the threads with respect to the monitor at runtime, he or she can understand the working of the program better. Hence can easily identify, analyze and rectify bugs in the program.  The tool parses the program to identify data pertinent to visualization such as number of condition variables involved in a concurrent program and their names, sections where threads interact with the monitor and the condition queues. These sections are then mapped to their corresponding visual interpretations or rules. The visualization rules decide how each action of the thread is depicted in the visualization panel. This depiction also depends on the signaling discipline of the monitor. The signaling disciplines are Signal-and-Continue, Signal-and-Urgent-wait and Signal-and-exit. Based on the method&#039s executed by the thread and signaling discipline, the visualization panel decides if a particular thread has to be placed at the entry queue, reentry queue, a condition queue or the monitor. Therefore, the programmer can view the interaction among the threads and the monitor in a particular execution and identify any bugs in the program easily. The programmer can also set breakpoints in JSWAT and view the current status of the threads at that point in the visualization panel.  This user-friendly visualization tool along with the feature-rich JSWAT debugger aims to reduce the time and effort spent by Java programmers in testing and debugging concurrent programs and hence increases their productivity. This tool also aims to function as an aid to understand the nuances of concurrent programming. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>LOCALIZATION BANDWIDTH LIMITATIONS IN WIRELESS MESH NETWORKS USING THE IEEE 802.15.4 STANDARD</strong><br /> Monday, November 26, 2007<br /> William Wallace II<br /><p><a href="#" id="420-show" class="showLink" onclick="showHide(420);return false;">Read More</a></p></p><div id="420" class="more"><p><a href="#" id="420-hide" class="hideLink" onclick="showHide(420);return false;">Hide</a></p><p><strong>Abstract: </strong>The need to create mesh networks is highly possible in certain scenarios.  Consider the case of a large storage warehouse.  Workers are constantly walking the area and often need to send data to other locations in the warehouse.  This can be accomplished with the 802.15.4 standard, but a mesh network must be created.  There would need to be stationary 802.15.4 nodes that join together to form a backbone infrastructure.  This backbone infrastructure would allow the workers, using mobile 802.15.4 nodes, to localize their data communication to nodes in their immediate area.  The backbone would then be responsible for relaying the data received from mobile nodes to destination nodes in areas outside their operating space. This research work aims to shed more light on possible solutions and performance data of the previously described scenario.  A model was created to show the behavior of a wireless mesh network built on the technology described in the IEEE 802.15.4 standard.  Furthermore, models of mesh network routing and the application scenario have been devised in order to evaluate a proposed solution.  We use our models to simulate a network using the 802.15.4 standard in addition to existing wireless sensor data routing techniques to send data from mobile nodes to various data sinks.  The findings will be presented and described to show that the mesh networks can be created and are fairly reliable when routing data between various nodes in the network.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Panda Monitoring – A System To Monitor High Performance Computing For The ATLAS Experiment Design, Development, Implementation And Deployment Of A Prototype</strong><br /> Monday, November 26, 2007<br /> Prem Anand Thilagar<br /><p><a href="#" id="423-show" class="showLink" onclick="showHide(423);return false;">Read More</a></p></p><div id="423" class="more"><p><a href="#" id="423-hide" class="hideLink" onclick="showHide(423);return false;">Hide</a></p><p><strong>Abstract: </strong>The thesis is an analysis of the existing monitoring system of the Panda which is a grid middleware for the ATLAS experiment running in CERN. The thesis aims at identifying the key bottlenecks of the current monitor and speaks about the implementation of a new monitor for the same .The new monitor being designed with scalability and maintainability in mind shows how it  will perfectly fit in for the changing needs of the panda middleware ,as panda is moving to the next generation it is being used as a generic Grid middleware for a lot of other experiments running in the OSG sites in the United states. The new monitor is designed in Ruby on Rails and has numerous advantages over the existing one. The thesis deals with the design, development and implementation of this new monitor.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Comparing XML Loosely Structured Search Engines</strong><br /> Monday, November 19, 2007<br /> Indhu Krishna Sivaramakrishnan<br /><p><a href="#" id="419-show" class="showLink" onclick="showHide(419);return false;">Read More</a></p></p><div id="419" class="more"><p><a href="#" id="419-hide" class="hideLink" onclick="showHide(419);return false;">Hide</a></p><p><strong>Abstract: </strong>XML keyword search has been a widely researched area. [1] has proposed an XML semantic search engine called OOXSearch, which answers loosely structured queries. The framework of OOXSearch takes into account the semantic relationship between nodes based on their contexts. The context of a node is determined by its parent node. The label “name”, for example, could mean the name of a book and could also mean the name of the book’s author. If we try to consider the relationship between these two nodes without considering their parent nodes, we would be drawing the incorrect conclusion that these two nodes represent the characteristics of two entities belonging to the same type.  Thus, the OOXSearch framework treats a parent and its leaf nodes as a single unified entity. The OOXSearch Engine works well for all types of XML trees, except for one scenario where a parent and its child node belong to the same type and are both having leaf children nodes. In this thesis, we propose an extension to the OOXSearch Engine that handles the above mentioned case. We experimentally evaluated the extended search engine and compared the results with other systems.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Visualization of Monitor-based Executions</strong><br /> Monday, November 19, 2007<br /> Keerthika Koteeswaran<br /><p><a href="#" id="421-show" class="showLink" onclick="showHide(421);return false;">Read More</a></p></p><div id="421" class="more"><p><a href="#" id="421-hide" class="hideLink" onclick="showHide(421);return false;">Hide</a></p><p><strong>Abstract: </strong>Concurrent programs play a vital role in the world of software engineering. Concurrent programs are used in various fields such as safety critical applications, applications involving advanced graphical user interface, operating system implementation etc. Concurrent programs due to their inherent multi-threaded nature pose a challenge to software engineers not only in coding but also in ensuring data consistency and accuracy. One of the important software constructs used by concurrent programmers to handle critical sections and synchronization is monitors. A monitor is an encapsulation of shared data, operations on the data and any synchronization required to access the data. Monitors are widely used by concurrent programmers since it guarantees mutual exclusion and can be implemented to ensure synchronization between all the threads in a multi-threaded application.  All concurrent programs are unpredictable in nature since the output directly depends on the sequence of thread execution. This non-determinism in execution poses a great challenge to concurrent programmers. This thesis proposes and implements a tool to visualize the working of monitors in concurrent programs. This would facilitate concurrent programmers to view, understand, analyze and test the monitor implemented in their program. This method is implemented in a GUI visualization tool using Java Swing. The GUI application is incorporated with JSWAT which is an open-source graphical debugger front end based on Java Platform Debugger Architecture. The motive behind integrating the visualization application and JSWAT is to provide useful features like sophisticated breakpoints, colorized code display, panels displaying call stacks, visible variables apart from visualization of monitor execution. The visualization application analyzes each statement in the code to capture data pertinent to visualization. This data is communicated to the GUI that maps it to the appropriate visualization rule. The visualization rules determine where each thread is to be positioned in the GUI. When the concurrent program is executed in the debugger, the user would be able to view the different threads change positions in the GUI. The positions would directly correspond to the states of the threads with respect to the monitor.   Visualizing the interaction of the different threads with each other and with the monitor would facilitate a concurrent programmer to analyze the program and ensure its accuracy and correctness. This tool can also be used as a teaching aid to instruct software engineering students and budding concurrent programs by helping them to visualize, understand and appreciate the working of concurrent programs in general and monitors in particular. This tool aims to contribute towards easing the burden of concurrent programmers by enhancing their understanding of monitor-based executions. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Distinct Value Estimation by Sampling on Peer to Peer Networks</strong><br /> Thursday, November 08, 2007<br /> Zubin Joseph<br /><p><a href="#" id="414-show" class="showLink" onclick="showHide(414);return false;">Read More</a></p></p><div id="414" class="more"><p><a href="#" id="414-hide" class="hideLink" onclick="showHide(414);return false;">Hide</a></p><p><strong>Abstract: </strong>Peer-to-Peer networks have become very popular on the Internet, with millions of peers all over the world sharing large volumes of data. The sheer scale of these networks has made it difficult to gather statistics that could be used for building new features. This thesis presents a technique of obtaining estimations of the number of distinct values matching a query on the network. The method is then analyzed by considering simulation results that demonstrate its effectiveness and flexibility in supporting a variety of queries and applications.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Distributed Predictive Health Monitoring</strong><br /> Thursday, November 08, 2007<br /> Jorge David Herrera<br /><p><a href="#" id="416-show" class="showLink" onclick="showHide(416);return false;">Read More</a></p></p><div id="416" class="more"><p><a href="#" id="416-hide" class="hideLink" onclick="showHide(416);return false;">Hide</a></p><p><strong>Abstract: </strong>According to the National Center for Health Statistics the major causes of death-in the US are (per year) heart diseases with 654 thousand cases, followed by cancer with 550 thousand cases and stroke with 150 thousand cases. In this project we propose, design, implement and test a prototype of a system that improves the level of detection and prediction of several diseases and improves the quality of life of patients by utilizing non-invasive wireless medical sensors.   This system is composed of a set of wireless medical sensors, a nearby (close-proximity) PDA (personal digital accessory) or cell phone used for data gathering and short-term analysis, and a large server used as a data repository and longer-term analysis and visualization center. Medical sensors are used to monitor cardiac function (ECG), as well as other medical conditions, while the 'local' device (for example a cell phone) collects sensor information, does a preliminary analysis (looking for simple anomalies) and collects and sends information to a larger server. The server looks for larger (days or months) trends and detects unusual conditions as well as creating visual displays of the patient's data.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Top K query processing in Distributed Database</strong><br /> Monday, July 23, 2007<br /> Amrita Tamrakar<br /><p><a href="#" id="411-show" class="showLink" onclick="showHide(411);return false;">Read More</a></p></p><div id="411" class="more"><p><a href="#" id="411-hide" class="hideLink" onclick="showHide(411);return false;">Hide</a></p><p><strong>Abstract: </strong>Today’s data is rarely stored in centralized location due to the enormous amount of information that needs to be stored and also to increase the reliability, availability and performance. Same data is stored in different format into different company’s database. We consider various scenarios of distributed database such as horizontal, vertical fragmentation and attribute overlapping. Allowing access to integrated information from these multiple datasets can provide accurate and wholesome information to the end-user. We research on efficient querying to these distributed databases to get top k elements matching the ranking order provided by the user. We also discuss hierarchical way of using the top k algorithm and their limitations to our problem. We propose four different algorithms based on NRA algorithm to solve this problem efficiently and compare and contrast these methods. Once the combination of data sources has been identified, we use our algorithms to get the top elements from these data source combination, process them to get the top k elements according to the user’s ranking function.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Performance Analysis of Caching Effect on Real Time Packet Processing in a  Multi-threaded Processor</strong><br /> Friday, July 20, 2007<br /> Miao Ju<br /><p><a href="#" id="402-show" class="showLink" onclick="showHide(402);return false;">Read More</a></p></p><div id="402" class="more"><p><a href="#" id="402-hide" class="hideLink" onclick="showHide(402);return false;">Hide</a></p><p><strong>Abstract: </strong>Caching has been time proven to be a very effective technique to improve  memory access speed and average performance for general processors. Based  on the real-world trace simulation, earlier research also showed that cache  can help improve the route lookup and packet classification performance in  a Network Processor (NP). However, how effective the caching technique is,  in dealing with traffic under stringent delay/loss constraints (as is the  case for router interface using an NP for packet processing)?is still an  open issue.  In this thesis, we aim at addressing the above issue through simulation  studies using a well-designed, lightweight simulator. We first demonstrate  how such a simulator can be developed to allow effective performance  analysis of a multi-threaded, single core NP. Then we apply this simulator  to the study of the caching effect on the packet throughput performance  under stringent delay/loss constraints. Our simulation studies indicate  that the effectiveness of caching is sensitive to the actual delay/loss  constraints. Moreover, the use of a large number of threads can effectively  hide the memory latency, making caching less effective. In conclusion,  caching can be effective in the parameter range where the delay/loss  constraints are loose and the configurable number of threads is small. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Graph Based Approach for Classification of Documents</strong><br /> Friday, July 20, 2007<br /> Aravind Venkatachalam<br /><p><a href="#" id="403-show" class="showLink" onclick="showHide(403);return false;">Read More</a></p></p><div id="403" class="more"><p><a href="#" id="403-hide" class="hideLink" onclick="showHide(403);return false;">Hide</a></p><p><strong>Abstract: </strong>The area of document classification has been examined, explored and experimented as a technique for organizing and managing vast repositories of electronic documents such as emails, text and web pages. Over the past decade, several approaches such as machine learning, data mining, information retrieval and others have been proposed for addressing this problem of classifying electronic documents. While a majority of these techniques rely on extracting high-frequency keywords, they ignore the aspect of extracting groups of related keywords. Additionally, they fail to capture the salient relationships between a number of keywords and their inherent structure, which can prove to be a decisive element in classifying specific types of documents (e.g., web-pages). To this effect, the design of InfoSift was proposed which incorporates graph mining techniques for document classification by using a supervised learning model. This framework focused on examining whether the incoming unknown document can be classified into a single category/folder using a ranking mechanism.  However, in the real world, documents are categorized into multiple folders based on varied characteristics (such as multiple folders for different emails). Existing approaches have suggested rigid techniques to accomplish multi-folder classification. Adopting these approaches within the InfoSift framework do not lead to a feasible solution due to the consideration of group of keywords and their relationships with other words. In order to bridge this gap between the strength of InfoSift and issues of Multi-folder classification, a different technique needs to be investigated.  Hence, in this thesis, we introduce a new approach to extend the abilities of InfoSift to support multiple categories (folders). A ranking technique to order the representative -common and recurring - structures generated from pre-classified documents to categorize new incoming documents has been presented. This approach is based on a global ranking model that incorporates several factors regarding document classification and overcomes numerous problems while using existing approaches for multiple folder classification in the InfoSift system. A number of parameters which influence the generation of representative substructures in single folder classification are analyzed and adapted to multiple folders. Additional graph representations have been analyzed and its use has been validated experimentally to represent the input data. Exhaustive experiments substantiating the selection of parameters for classification of unknown documents into multiple folders have been conducted. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Rate Control Algorithms for IEEE 802.11 Wireless Networks</strong><br /> Friday, July 20, 2007<br /> Bodhisatwa Chakravarty<br /><p><a href="#" id="410-show" class="showLink" onclick="showHide(410);return false;">Read More</a></p></p><div id="410" class="more"><p><a href="#" id="410-hide" class="hideLink" onclick="showHide(410);return false;">Hide</a></p><p><strong>Abstract: </strong>Over the past few years we have seen a high demand for wireless network access. Wi-Fi or IEEE 802.11 standard has become the most popular one in the local area network domain. These networks have the capability of transmitting at multiple rates depending upon the signal quality measured at the node. Depending upon the signal quality the node adapts the best modulation technique and the bit rate that would optimize the throughput.   This thesis evaluates software driven rate selection algorithms used in IEEE 802.11 wireless network interface cards. The motive of the bit-rate selection techniques is to optimize the throughput over the wireless network out of the many rates that are supported by the link. The decision to switch from one rate to another with the changing link conditions to optimize the throughput is the primary focus of the bit-rate selection algorithms. This thesis also presents a novel learning bit-rate selection algorithm called the LeziRate. LeziRate uses the quality of the signal received at the device to learn and predict the quality of signal. Based on the prediction it also predicts the best bit-rate that it predicts would achieve the maximum throughput. This thesis provides the simulation study of the LeziRate algorithm. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>RESOURCE ADAPTIVE AGENT BASED FAULT TOLERANT RESOURCE ADAPTIVE AGENT BASED FAULT TOLERANT ARCHITECTURE</strong><br /> Friday, July 20, 2007<br /> Shreyas K Shetty<br /><p><a href="#" id="412-show" class="showLink" onclick="showHide(412);return false;">Read More</a></p></p><div id="412" class="more"><p><a href="#" id="412-hide" class="hideLink" onclick="showHide(412);return false;">Hide</a></p><p><strong>Abstract: </strong>Pervasive environment consists of increasing number of mobile, tiny and heterogeneous devices communicating through interconnected network. As ubiquitous computing has seeped into various aspects of our everyday life, there has been an increasing demand for dependable systems. But providing reliability will require introduction of fault tolerance which would mean devoting substantial time and resources. The dynamic nature and the unreliability associated with pervasive systems coupled with the energy constraints of the devices involved makes fault tolerance a challenging task.  In general, the techniques used to provide fault-tolerance are based on having redundancy and duplication of the user tasks. However the additional cost and the low resource availability will prohibit implementation of such fault tolerance methodologies for a pervasive environment.  The traditional fault detection and recovery techniques need to be modified to make it applicable in a pervasive environment.  	Pervasive Information Community Organization (PICO) is a framework consisting of software agents, called delegents, which performs services on behalf of users and devices. PICO framework introduces the concept of community computing where delegents work on behalf of devices and users and collaborate with each other to carry out application-specific services. PerSON (Service Overlay Network for Pervasive Environments) provides the service overlay network for the implementation of the community computing concept introduced in PICO. PerSON uses the device model proposed in PICO and provides an overlay network which abstracts the details of service creation, discovery and utilization in a pervasive environment.  In this thesis we have developed a Resource Adaptive Agent based Component (RAAC) which is integrated with PerSON to enhance and facilitate the services provided by PerSON. To deal with the dynamic nature and make best use of resources available in a pervasive environemt, RAAC adds features like fault tolerance, provisions for checkpointing and resource aware distribution of user requests to PerSON. RAAC not only provides reactive measures to failures, but also proactively deals with the probable future failures and if required performs reassignment of user task from the recently saved checkpoint.  Demonstration applications that perform data intensive tasks have been tested on RAAC. For a set of tasks, energy savings of about 40% was achieved by adding the resource adaptiveness feature to PerSON. The energy savings achieved is proportional to the size of tasks and is subject to the devices available in the environment.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Incremental Retrieval and Ranking of Complex Patterns from Text Repositories</strong><br /> Thursday, July 19, 2007<br /> Jayakrishna ThathiReddy<br /><p><a href="#" id="404-show" class="showLink" onclick="showHide(404);return false;">Read More</a></p></p><div id="404" class="more"><p><a href="#" id="404-hide" class="hideLink" onclick="showHide(404);return false;">Hide</a></p><p><strong>Abstract: </strong>As the volume of information accessible via the electronic medium (such as the Internet) is staggeringly large and growing rapidly, users have to sift through vast reservoirs of information to retrieve relevant data of their choice. It has been estimated that, the Internet consists of 2.5 billion unique, publicly accessible web-pages and this figure is growing at an alarming rate of 7.3 million pages per day. Currently, the only way to wade through such colossal information is by using search engines (such as Google, Live, Yahoo, etc.). Although the popularity of search engines has increased manifold due to -- simplicity of usage, speed of retrieval and amount of results generated, their ability to intelligently retrieve relevant information is significantly hampered due to over-reliance on Boolean operators for data retrieval. Consider searching for complex patterns involving pattern frequency, e.g., (at least 5 occurrences of the phrase &#039research experiences&#039), proximity e.g., (&#039metal&#039 near &#039traders&#039, in any order, within 10 words of each other), sequence of sub-patterns e.g., (&#039soya&#039 followed by &#039plantings&#039, within 5 words of each other), all occurrences of the word (&#039contract&#039 and its synonyms) or structural patterns like (&#039France&#039 within the occurrence of &#039sunflower plantings&#039 and &#039harvest&#039) etc.. Such queries are not supported by currently available search engines. Researchers looking for information in domains such as security, biology, legal research etc.,  need focused, objective and precise semantics to specify such patterns. In order to deal with such complex requirements of specific domain users, the design of a document retrieval system that consists of -- a pattern specification language and an efficient detection engine -- that allows specification of such expressive patterns is needed. To address these issues, we have designed a framework (made up of two interdependent yet distinct systems -- InfoFilter and InfoSearch) based on an expressive pattern specification language and a set of novel detection algorithms that handles streaming as well as static data. InfoFilter handles pattern detection for streaming data (news feeds, IP packets, etc.) where freshness of search results is paramount. However, in the case of data that resides in the form of large yet static repositories, and when the freshness of data is not critical, the InfoSearch system handles pattern detection using a pre-computed index. The initial design of InfoSearch, for complex pattern detection, focused on fetching all matching occurrences of the pattern in the data repositories. It further processed all the tuples of the operands that constituted the pattern. However, this approach proves to be inefficient in terms of processing time, memory utilization and number of computations. Moreover, the results are generated in the order in which they are detected, thus ignoring the relevancy of results with respect to user preferences. In order to address these problems, an incremental approach for complex pattern detection is needed. Moreover, in order to generate results based on the relevancy rather than the order of detection, ranking mechanisms for appropriately filtering the results also needs to be addressed. In this thesis, we investigate several approaches for incremental detection, retrieval and ranking of results based on user specified criteria. We also investigate the need for novel data structures as well as the types of structural meta-data to be associated with the data stored in the index for ranking the fetched results. We propose a novel ranking algorithm that utilizes the structural boundaries of the data to rank results based on the location and occurrence of a complex pattern in a document. We also present algorithms for each operator encountered in a pattern, that are based on ensuring optimal utilization of computational and memory resources in the least possible time. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Sequence alignment editor: a new tool to assist with inferring phylogenetic and functional relationships in biological data</strong><br /> Thursday, July 19, 2007<br /> Ramya Raghukumar<br /><p><a href="#" id="408-show" class="showLink" onclick="showHide(408);return false;">Read More</a></p></p><div id="408" class="more"><p><a href="#" id="408-hide" class="hideLink" onclick="showHide(408);return false;">Hide</a></p><p><strong>Abstract: </strong>Sequence alignment is one of the oldest and most common tasks in bioinformatics.   Biologists study evolution, discover functional and structural information in genomic or protein data through the extensive use of sequence alignments.  For long sequences, or a large number of sequences, alignment construction is complex, given the nature of their often substantial differences. It is very difficult to manually align long regions, so development of methods for this task continues to be an active area of research.   Alignment construction algorithms based on dynamic programming, such as Needleman-Wunsch and Smith-Waterman, approach the problem from a mathematical perspective. The best possible, or optimal, alignment is computed relative to a scoring scheme. Points are awarded or deducted based on match/mismatch/gap (indels) of characters among the sequences. The resulting layout is guaranteed to be mathematically optimal, though not necessarily biologically meaningful. In addition, dynamic programming methods can become time consuming with the increase in length and number of sequences. They usually account only for single letter substitutions and relatively short indels, representing the latter as gaps. Other possible evolutionary scenarios like rearrangements and inversions are generally not considered.   In a multiple sequence alignment, the most popular method for reigning in complexity is by using a progressive approach. Progressive alignment techniques can incorporate limited evolutionary information in the form of a phylogenetic tree while building alignments, although they can also reconstruct the tree based on pair wise sequence similarities (if the tree is not known, or has not been provided). Otherwise, they generally do not make use of knowledge that shed light on the sequences in a biological context, although there are some notable exceptions. Progressive techniques may be implemented in iterative refining steps; however they still remain both computationally and biologically approximate.   There is a constant need for achieving biologically meaningful and accurate alignments, so work on this goal remains important. At present, biologists are often forced to manually adjust the alignments after the software has finished its part. These adjustments include the placement of sites of experimentally confirmed homology or characteristic structural features in conformation, in cases when such similarities are not well reflected in the sequences, thus misguiding the automated (mathematical) optimization process. Unfortunately, life science researchers often lack technical expertise to manipulate the files underlying the alignment, which imposes a need for providing them with user-friendly interfaces. So far, this need has been largely neglected. 	 Many tools for alignment visualization provide extensive annotation facilities (such as the tools from Vista browser family), but in most cases they are passive, i.e. they would allow the user to display known (or postulated) biological features, but not to change the alignment itself. Some feature editors are available; however these are prominently sequence-only editors. Their capabilities include insertion, deletion, copying and pasting of residues or gaps in sequences, or deletion and replication of sequences. Tools of this category include JALVIEW, SEAVIEW, CINEMA and others. Since they would permit sequences to be removed, new bases introduced and/or existing ones deleted, the entire concept is somewhat at odds with the idea of alignment editing, in which the sequences and order of residues in individual sequences should not be disturbed.  With these issues in mind, we have undertaken the design of an editor which would facilitate post-processing of sequence alignments. This editor is a user-friendly stand-alone utility, receiving sequence alignments generated by core alignment tools as its input. Alignments are available in various formats, and this editor supports many popular ones, including FASTA and PHYLIP. Similarly, the final result can be saved in several different formats. Information about the areas of biological interest in the involved sequences can be supplied through annotation files. The annotations may include the source of the sequence and details about specific biological features such as gene exons, regulatory sites, repeats, and many more. Every sequence may have its own set of annotations which may or may not be provided, depending on how much is known about their nature, from experiments or computational analysis.  Given these details, the editor can highlight the different regions in different hues, and thus assist the user in determining which elements may not have been placed in proper conformation. The color-to-genomic region mapping is supplied as a legend.  The users can decide whether they would like to view the alignment with or without the annotations marked.   The core editor features provide for drag-and-drop movement of </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A stochastic simulation of influenza Life Cycle Dynamics.</strong><br /> Thursday, July 19, 2007<br /> Richard Burke Squires<br /><p><a href="#" id="409-show" class="showLink" onclick="showHide(409);return false;">Read More</a></p></p><div id="409" class="more"><p><a href="#" id="409-hide" class="hideLink" onclick="showHide(409);return false;">Hide</a></p><p><strong>Abstract: </strong>The influenza virus, which is classified by the National Institutes of Health (NIH) as a re-emerging infectious disease, causes 36,000 deaths each year and was known to cause the most severe pandemic in human history in the form of the “Spanish Flu” of 1918, which claimed 20 – 40 million lives worldwide. The discovery of a new subtype of influenza, H5N1, known as “Avian Influenza” in 1997 raised the fear of a new pandemic. In an effort to understand the dynamics of the influenza virus for the purpose of designing better vaccine or therapeutics a stochastic model of influenza infection is presented. Building upon work contributed to the Reactome database in the form of biological pathway of the influenza life cycle and accompanying host-pathogen interactions, a stochastic model and simulation of influenza infection has been implemented utilizing the SimJava framework. In the stochastic model the 14 stages of influenza infection as well as the various genetic molecules of influenza are models and put into motion. What comes out is a better understanding of the dynamics of influenza infection.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>BUILDING ENERGY-EFFICIENT BROADCAST TREES USING A GENETIC ALGORITHM IN WIRELESS SENSOR NETWORKS</strong><br /> Wednesday, July 18, 2007<br /> Min Kyung An<br /><p><a href="#" id="401-show" class="showLink" onclick="showHide(401);return false;">Read More</a></p></p><div id="401" class="more"><p><a href="#" id="401-hide" class="hideLink" onclick="showHide(401);return false;">Hide</a></p><p><strong>Abstract: </strong>In wireless broadcast environment, data can be transmitted to several nodes by a single transmission. The nodes in that environment have limited energy resources; therefore we need to reduce the energy consumption when they broadcast data to all nodes to prolong the lifetime of the networks. We call this problem the MPB (minimum power broadcast) problem, and solving the problem has been shown as NP-Completeness problem [2], [11]. That’s why we are focus on finding ‘near-optimal’ solution for the problem.  An algorithm of constructing the minimum power broadcast trees, named BIP (broadcast incremental power) algorithm, was first proposed by Wieselthier et al. in [1], and several other algorithms also have been proposed by researchers so far. They use the property of the broadcast nature which can be exploited to optimize energy consumption. We propose an alternate search based paradigm wherein the minimum broadcast tree is found using a genetic algorithm which is used to solve the NP-Completeness problems. We borrow the BIP algorithm and the MST algorithm to create a search space in our genetic algorithm and we evolve the initial broadcast trees in the space to get an energy-efficient broadcast tree. Finally, through the simulations, the genetic algorithm achieved up to 20.60% improvement over the other broadcasting algorithms including the traditional BIP algorithm.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Dynamic Push and Pull Strategy Based On Data Interpolation For Web Sensor Networks</strong><br /> Wednesday, July 18, 2007<br /> Rohita Mohan<br /><p><a href="#" id="407-show" class="showLink" onclick="showHide(407);return false;">Read More</a></p></p><div id="407" class="more"><p><a href="#" id="407-hide" class="hideLink" onclick="showHide(407);return false;">Hide</a></p><p><strong>Abstract: </strong>Sensor Web, a new trend has been developed recently to effectively network large scale deployed heterogeneous sensors with Internet to make sensor data discoverable, accessible and controllable via World Wide Web. Since sensor nodes are highly resource constrained, continuous back hauling of a large amount of data is not feasible. In this paper, we propose a dynamic push/pull service architecture where sensory quality is dynamically monitored and controlled. The architecture is implemented in a web proxy residing at the gateway of the network. As data are being pushed continuously from the network for effective sketching of the sensing field, interpolation techniques are employed to further render the details. Due to varying user requirements and dynamics of the sensing field, pushed data only may lead to low quality of the interpolated data. In this case, the system will dynamically pull data from the sensing field based on the requirements of the interpolation algorithms for desired confidence. Also, continuous demands of data pulling will eventually be converted to pushing services in order to reduce system overhead. The overall result is an architecture where push and pull services are dynamically monitored in order to reduce the overall system energy while satisfying user’s demand for sensory data quality.  Simulation of a web proxy has been conducted to continuously regulate the push and pull rates of temperature data based on Spatial Point to Point Interpolation algorithm, which has been proved appropriate for GIS applications. From our simulation results, the optimal degree of push and pull rates is determined to maintain high quality of sensor data delivered to the users while reducing overall system energy but it can also be seen that with an increase in the accuracy of the interpolated data, there is a tradeoff in the energy consumption level of sensor nodes. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Flash crowd mitigation system</strong><br /> Monday, May 07, 2007<br /> Dongchul Kim<br /><p><a href="#" id="398-show" class="showLink" onclick="showHide(398);return false;">Read More</a></p></p><div id="398" class="more"><p><a href="#" id="398-hide" class="hideLink" onclick="showHide(398);return false;">Hide</a></p><p><strong>Abstract: </strong>Flash crowd means a phenomenon that a web server suffers a sudden surge of traffic since a large number of Internet users access the web server simultaneously. Once flash crowd occurs in a web server, the response rate for the HTTP requests decreases rapidly, or the web server may crash down even. To protect the web server from such flash crowd, in this paper, we propose Flash Crowd Mitigation System (FCMS) which is based on the cooperation of the web servers and the redirection of the HTTP request. In FCMS, when flash crowd is predicted by the traffic monitoring module in the router which is connected to the web server, other web server replicates the hot server's every objects, and then the router redirects the HTTP request to other web server in order to alleviate the effect of flash crowd. The simulation with real-world flash crowd traces shows that the monitoring module can predict the flash crowd and release the system from flash crowd mode effectively. Also, our experiment in testbed demonstrates that implemented FCMS and our mechanism can work correctly and mitigate the effect of flash crowd in a web server.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>HIEARCHICAL OVERLAY FOR SERVICE COMPOSITION HIEARCHICAL OVERLAY FOR SERVICE COMPOSITION IN PERVASIVE ENVIRONMENTS</strong><br /> Wednesday, May 02, 2007<br /> Aparna Kailas<br /><p><a href="#" id="399-show" class="showLink" onclick="showHide(399);return false;">Read More</a></p></p><div id="399" class="more"><p><a href="#" id="399-hide" class="hideLink" onclick="showHide(399);return false;">Hide</a></p><p><strong>Abstract: </strong>The objective of pervasive computing is to allow users to perform their tasks in a transparent way, regardless of device features. Resources on devices should be exploited to provide services in order to perform user tasks. When there is no exact match for the user task in the environment, the capabilities of available devices should be combined to perform the user task. Seamless Service Composition (SeSCo) abstracts device capabilities as services and leverages existing work on graph algorithms to perform service composition. A lightweight framework, PerSON (Service Overlay Network for Pervasive Environments) was developed to provide a service overlay network for the Pervasive Information Community Organization (PICO) middleware. In this thesis SeSCo service composition mechanism is implemented on top of PerSON. The ability of PerSON to construct an ad hoc service overlay network is exploited to create a hierarchical service overlay using message exchanges that constitute the latch protocol. This thesis develops a mechanism for propagation of services in the hierarchy. Services at each level are aggregated with services at lower levels of the hierarchy and requests are resolved by constructing a path in the service aggregation. Demonstration applications that perform string encryption and decryption are implemented using the PICO framework.  The application uses matrices and their multiplication to encrypt and decrypt data.  SeSCo is extended to include device resources in the service path, so that service composition will find the set of services hosted by the least resource constrained devices among all feasible candidates. A combination of device resources is used to define a suitable length for all services hosted by the device in the service aggregation. A proof of concept example scenario that uses battery energy has been implemented to demonstrate resource aware service composition.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Dynamic Framework for Testing the Synchronization Behavior of Java Monitors</strong><br /> Tuesday, April 17, 2007<br /> Andres Yanes<br /><p><a href="#" id="397-show" class="showLink" onclick="showHide(397);return false;">Read More</a></p></p><div id="397" class="more"><p><a href="#" id="397-hide" class="hideLink" onclick="showHide(397);return false;">Hide</a></p><p><strong>Abstract: </strong>A Java monitor is a specialized class that is used to synchronize the behavior of threads in a Java program. The monitors in a Java program must be adequately tested to ensure the correctness of the program. In this thesis we propose a dynamic framework in which a Java monitor is tested by exploring the state space of a Java monitor in a depth-first manner. The state exploration procedure consists of dynamically creating method sequences to exercise the possible synchronization behavior of a Java monitor. During exploration, new threads will be created on the fly to simulate different scenarios that result from threads reaching the monitor at different times. Each state reached is represented by a collection of data members that have been identified as having an affect the synchronization behavior of the monitor as well as an abstraction of the thread states.  A prototype tool that implements our framework has been built and has been used to evaluate the effectiveness of our approach in five case studies. In each case study, mutations to the original source code of a Java monitor are introduced to create variants that represent common mistakes made by programmers. The experimental results show that our framework is effective in detecting the synchronization failures in the case studies.   </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SEES - AN ADAPTIVE  MULTIMODAL USER  INTERFACE FOR THE VISUALLY IMPAIRED</strong><br /> Friday, April 13, 2007<br /> Aparajit Saigal<br /><p><a href="#" id="387-show" class="showLink" onclick="showHide(387);return false;">Read More</a></p></p><div id="387" class="more"><p><a href="#" id="387-hide" class="hideLink" onclick="showHide(387);return false;">Hide</a></p><p><strong>Abstract: </strong>The enormous amount of electronic data present today can be a daunting task to access and process for a regular person, let alone someone with a disability.  The World Wide Web and Electronic Mail have transformed the way we live. We are constantly become more dependent on this information, communication and commerce medium. The Speech Enabled Email System (SEES) is an alternate user interface that allows for the retrieval of emails and RSS feeds using speech. SEES can be accessed via a desktop application or through a telephony interface such as a regular phone-line or Voice over IP.  This thesis explores the different types of interfaces, describes SEES in length and compares SEES with interfaces that attempt similar functionality. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A PATIENT-CENTERED, GOAL-DIRECTED ELECTRONIC MEDICAL RECORD SYSTEM FOR THE HOMELESS</strong><br /> Friday, April 13, 2007<br /> Kallol Mahata<br /><p><a href="#" id="391-show" class="showLink" onclick="showHide(391);return false;">Read More</a></p></p><div id="391" class="more"><p><a href="#" id="391-hide" class="hideLink" onclick="showHide(391);return false;">Hide</a></p><p><strong>Abstract: </strong>Homelessness is a growing problem in the United States. Numerous barriers keep the street homeless from obtaining healthcare. Medical Street Outreach (MSO) programs are designed to reach out to the homeless. Gathering relevant clinical information on the streets is difficult, and carrying paper records in MSO is cumbersome and inefficient. Several complex healthcare record systems have been developed for hospitals and outpatient, but no such system exists for collecting health data on the streets. In this thesis, we describe a light weight Electronic Medical Record (EMR), called HEROS (Homeless Electronic RecOrd System) that we have built to address the process of healthcare on the streets. The HEROS system has been designed for use on a Tablet Personal Computer (TPC) to collect, organize, and share clinical data between clinicians and provide quality healthcare to the homeless. The workflow is based on a novel model of healthcare known as Goal Negotiated Care (GNC), which stresses the needs of a patient on the street, and stresses on small success and the self-efficacy theory. Some major features of HEROS include:  (1) Advanced usability features like handwriting recognition, collapsible user interface modules; (2) Communication features like email client and Internet Telephony; (3) Health Layer 7 (HL7) compliant Message Passing mechanism for communication with other HL7 compliant EMR's; and (4) Security measures that comply with Health Insurance Portability and Accountability Act (HIPAA) regulations. We will report a field test that has been conducted to evaluate the effectiveness of this system. The design and workflow of HEROS can be easily expanded as a disaster relief EMR or an EMR for emergency situation.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A RANDOM WALK APPROACH TO SAMPLING HIDDEN DATABASES</strong><br /> Thursday, April 05, 2007<br /> Arjun Dasgupta<br /><p><a href="#" id="394-show" class="showLink" onclick="showHide(394);return false;">Read More</a></p></p><div id="394" class="more"><p><a href="#" id="394-hide" class="hideLink" onclick="showHide(394);return false;">Hide</a></p><p><strong>Abstract: </strong>A large part of the data on the World Wide Web is hidden behind  form-like interfaces. These interfaces interact with a hidden back-end  database to provide answers to user queries. Generating a uniform  random sample of this hidden database by using only the publicly  available interface gives us access to the underlying data  distribution. In this thesis, we propose a random walk scheme over the  query space provided by the interface to sample such databases. We  discuss variants where the query space is visualized as a fixed and  random ordering of attributes. We also propose techniques to further  improve the sample quality by using a probabilistic rejection based  approach and conduct extensive experiments to illustrate the accuracy  and efficiency of our techniques</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>COMPUTING BEST COVERAGE PATH IN THE PRESENCE OF OBSTACLES IN WIRELESS  SENSOR NETWORKS</strong><br /> Monday, April 02, 2007<br /> Senjuti Roy<br /><p><a href="#" id="393-show" class="showLink" onclick="showHide(393);return false;">Read More</a></p></p><div id="393" class="more"><p><a href="#" id="393-hide" class="hideLink" onclick="showHide(393);return false;">Hide</a></p><p><strong>Abstract: </strong><br /><br /><a href="http://www.cse.uta.edu/news/seminars/Documents/absspr.pdf">Click here to view Abstract</a><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient Visualization of Streaming Sensor Network Data Using Approximation Technique</strong><br /> Friday, February 23, 2007<br /> Sunil Pai<br /><p><a href="#" id="385-show" class="showLink" onclick="showHide(385);return false;">Read More</a></p></p><div id="385" class="more"><p><a href="#" id="385-hide" class="hideLink" onclick="showHide(385);return false;">Hide</a></p><p><strong>Abstract: </strong>Recent advances in low power wireless sensor networks are enabling new applications for wireless devices. At the same time the power and flexibility of the web services is expanding the power of the internet.   Deployed sensor networks provide us with a large amount of  data which arrives, from its deployed habitant, in a continuous stream rather than static relations. Not only are the size of these applications unbound, but the data arrives in a bursty mode which makes it very difficult to process all the data in a timely manner so that it can be inputed to systems that carryout the visualization of the habitant being monitored.  This work in an attempt to build a system that can use approximation technique like random sampling to process the incoming data as quickly as posible and with very small reduction in efficiency so that the data can be visualized on web based systems in a very timely manner. This system changes dynamically as new data arrives from the habitant being monitored thus providing us with real time visualization of the habitant as it changes.  Techniques like approximation are necessary because the data arriving from the sensor network is so overwhelming that if a traditional approach is used it will result in a  long delay times and thus the system is unable to visualize the habitant as it changes dynamically.Also, given the necessicity to insert huge amount of data and run queries on them in a very quick manner, traditional DBMS system are not good enough. So we have to build more dynamic systems to enable processing and visualization of data in a dynamic manner.  We will have a look at the results obtained and analyse the benefits and trade offs of using this approach. Further, we will also have a look at how this system can be extended to incorporate data from different applications and the future scope of work.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Analyzing Differences in Color Variation Sensitivity between Experts and Novices to Find a Differentiating Process in Pigmented Skin Lesion Diagnosis</strong><br /> Monday, December 18, 2006<br /> Yu-Chin Chai<br /><p><a href="#" id="384-show" class="showLink" onclick="showHide(384);return false;">Read More</a></p></p><div id="384" class="more"><p><a href="#" id="384-hide" class="hideLink" onclick="showHide(384);return false;">Hide</a></p><p><strong>Abstract: </strong>Fatal skin lesions such as melanomas appear as having high color variation to experts but often as having low variation to novices. This thesis investigates if it is possible to find a function representing an expert’s color variation sensitivity to skin lesions without explicitly requiring them to provide rules or even a diagnosis. Various pigmented skin patch images with either normal pigmentation or with diseased lesions were evaluated by the participants, including experts and novices. Initial comparisons in various color spaces showed that only the RGB model confirmed the expectation that the expert’s judgment represented the actual variation of the color whereas the judgment of novices seemed to be based on more than color variation itself. To determine the best color space for lesion differentiation, an optimization process based on parametric color dimensions in a RGB color space was used to find the parameters of a color transformation function that best differentiates the color variation sensitivity between experts and novices. The analysis of results showed that this process was able to maximize the differences between the experts’ and novices’ color variation sensitivity and to provide a criterion to help discriminate between images of non-invasion and melanoma type skin lesion images.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Adaptive Load Balancing and Change Visualization for WebVigiL</strong><br /> Monday, November 27, 2006<br /> Chelladurai Hari Hara Subramanian<br /><p><a href="#" id="380-show" class="showLink" onclick="showHide(380);return false;">Read More</a></p></p><div id="380" class="more"><p><a href="#" id="380-hide" class="hideLink" onclick="showHide(380);return false;">Hide</a></p><p><strong>Abstract: </strong>There is a need for selective monitoring, the contents of web pages. Periodical visit for understanding changes to a web page is both inefficient and time consuming.  WebVigiL is a system developed for automating the change detection and timely notification of HTML/XML pages based on user specified changes of interest. User interest, specified as a sentinel/profile, is automatically monitored by the system using a combination of learning-based and event-driven techniques.  The first prototype concentrated on the functionality of the WebVigiL system. This thesis extends the WebVigiL system in a number of ways. The primary focus of this thesis is the improvements to the performance and scalability aspects of the  prototype.  A load balancer is  proposed based on the analysis and estimation of the load factors imposed by a sentinel on the system. An adaptive load balancer has been developed for the WebVigiL, that uses various sentinel properties to distribute sentinels among servers using a number of strategies. a web-based DASHBOARD has been developed to enable users to manage and visualize changes to their sentinels. As part of the DASHBOARD, a Dual-Frame presentation for displaying the detected changes is provided (both for HTML and XML). Other contributions of the thesis include integration of the above modules into the current system, making the WebVigiL system stable and robust (by fixing various bugs) and testing the system for various test cases. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Data Mining Approach to Change Detection in Skin Lesions</strong><br /> Monday, November 27, 2006<br /> Mehmet Zahid Boyukozer<br /><p><a href="#" id="382-show" class="showLink" onclick="showHide(382);return false;">Read More</a></p></p><div id="382" class="more"><p><a href="#" id="382-hide" class="hideLink" onclick="showHide(382);return false;">Hide</a></p><p><strong>Abstract: </strong>The goal of this work is to develop a system for automatic monitoring of changes in skin lesions. Our hypothesis is that by computer imaging, segmentation and feature extraction techniques, we can identify quantitatively a set of indicators for significant clinical change. We  address the problem of change detection with a data-mining approach. We use a computer vision module to segment the border of a skin lesion. We then extract visual features of three types: Color, shape and texture assuming that multiple pictures from the same subject are available with a time difference. Based on the extracted features in each shot, we produce a delta matrix which encapsulates the differences in feature values for each lesion on the subject’s body. We then cluster these changes. Many lesions are expected to show no change. We hypothesize that the significant changes will not be part of any cluster, instead they will become outliers. We will apply outlier detection techniques to identify significant changes.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Computing Best Coverage Path in the Presence of Obstacles in Wireless Sensor Networks.</strong><br /> Monday, November 27, 2006<br /> Senjuti Basu Roy<br /><p><a href="#" id="383-show" class="showLink" onclick="showHide(383);return false;">Read More</a></p></p><div id="383" class="more"><p><a href="#" id="383-hide" class="hideLink" onclick="showHide(383);return false;">Hide</a></p><p><strong>Abstract: </strong>Given a set S = {S1, . . . Sn} of n homogeneous wireless sensors deployed in a two dimensional area, a source point s and a destination point t, the least protected point p along a path P(s, t) is that point such that the Euclidean distance between p and its closest sensor node Si is maximum. This dis- tance between p and Si is the Cover value of the path P(s, t). The Best Coverage Path between s and t, BCP(s, t), is the path that has the min- imum cover value. Although there exists efficient algorithms to compute BCP in O(n log n) time, the presence of obstacles inside the two dimensional area has not been considered in the literature. In this work, we consider the problem of computing BCP(s, t) in the presence of m line segment obstacles. We observe that such obstacles pose significant challenges, and propose three algorithms to compute two different variations of the problem. When the ob- stacles obstruct both the sensing as well the computed path, we develop an algorithm that computes BCP(s, t) in O((m2n2+n4) log(mn+n2))) time, by leveraging the concept of a quartic time Constrained and Weighted Voronoi diagram among obstacles and creating its dual. In the restricted case when obstacles cannot obstruct sensing but the computed path has to avoids ob- stacles, we develop an algorithm that computes BCP(s, t) in O(nm2 + n3) time, by using the visibility graph data structure. Finally, we develop a more efficient O(nm+n2) time approximation algorithm to compute BCT(s, t) for this restricted problem, using spanners of the visibility graph. The approxi- mation factor of the cover value is O(nk) where k is the stretch factor of the spanner graph. The proofs of correctness of the three proposed algorithms are also presented in this work.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Camouflaging watermarks in stepping stone detection to evade detection by an attacker</strong><br /> Wednesday, November 22, 2006<br /> Madhu Venkateshaiah<br /><p><a href="#" id="369-show" class="showLink" onclick="showHide(369);return false;">Read More</a></p></p><div id="369" class="more"><p><a href="#" id="369-hide" class="hideLink" onclick="showHide(369);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis describes a robust watermarking technique to detect stepping stones. The watermarking technique is based on an earlier technique by Wang et al. for correlation of VOIP streams. We have modified the above technique and adapted it for stepping stone detection. The watermarking technique has been made more robust to withstand attacks against stepping stone detection. We also try to defeat the attack published by Peng et al. which tries to detect the presence of watermarks in traffic streams and to infer the parameters used for watermarking. We demonstrate that the above attack works with the technique proposed by Wang et al. but not with our new watermarking technique. To demonstrate the effectiveness of the proposed technique, we conducted experiments by simulating the correlation of streams using our watermarking algorithm. Experiments were conducted by varying different network parameters like average delay, drop rate, packet rate etc. and comparative results have been provided.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Compensation of photobleached images for a cell dynamics system</strong><br /> Wednesday, November 22, 2006<br /> Sowmya Gopinath<br /><p><a href="#" id="370-show" class="showLink" onclick="showHide(370);return false;">Read More</a></p></p><div id="370" class="more"><p><a href="#" id="370-hide" class="hideLink" onclick="showHide(370);return false;">Hide</a></p><p><strong>Abstract: </strong>Today we have a better understanding of diseases and viral infections. This progress in the field of medicine attributes to the discoveries in biology that required an extensive knowledge of cell dynamics. Intracellular Cell Dynamics Analysis System (ICellDAS) is the first web based tool aimed at automating sub cellular particle motion estimation, tracking and mobility analysis. The goal of this tool is to help develop a better understanding of specific biology problems and better visualization of biological data. For reliable analysis as well as visualization of cell dynamics, it is essential that the acquired images reflect the exact information of the specimen. The objective of the proposed method is to help regain the information lost by the photobleaching of fluorescent images. Photobleaching is the irreversible destruction of fluorescence within the stained specimen caused due to scattering, absorption of excitation and fluorescence light in a confocal laser scanning microscope .Current approaches to solve this problem are computationally complex, time consuming, restricted to exponential decay model and highly sensitive to noise. The proposed method provides a simple yet effective statistical approach to solve this problem. It aims to overcome the disadvantages of current methods and at the same time increase the visual value of photobleached images. The main idea is to filter the foreground information from a given image by modeling it as a mixture of Gaussians and use this information to compensate the intensity loss of the photobleached images. The experimental results show that the restored images have better contrast and visual value compared to the original stack of photobleached images. In addition to this work, an interface for the proposed ICellDAS has been developed in Java Server Page (JSP) technology. Details of this method together with experimental results and design principles of ICellDAS are presented in the thesis.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A new indicator of syntactic complexity in the prediction of text readability</strong><br /> Tuesday, November 21, 2006<br /> Jagadeesh Kondru<br /><p><a href="#" id="374-show" class="showLink" onclick="showHide(374);return false;">Read More</a></p></p><div id="374" class="more"><p><a href="#" id="374-hide" class="hideLink" onclick="showHide(374);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis presents a novel method of predicting readability of text. We introduce Syntax Grade as a measure of syntactic complexity of text.  Syntax Grade of a text is the school grade level, sentence structure typically associated with which best models the structure of the text.  We define a new readability estimator, Readability Index (RI). RI is a linear function of the parameters Syntax Grade and Average Word Length.  RI outputs a grade level that indicates the readability of the input text.  Experimental results show that grade levels predicted by RI have a better correlation with the original grade levels than do those predicted by the standard Dale-Chall formula.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Runtime Optimization and Load Shedding in MavStream: Design and Implementation</strong><br /> Monday, November 20, 2006<br /> Balakumar K. Kendai<br /><p><a href="#" id="381-show" class="showLink" onclick="showHide(381);return false;">Read More</a></p></p><div id="381" class="more"><p><a href="#" id="381-hide" class="hideLink" onclick="showHide(381);return false;">Hide</a></p><p><strong>Abstract: </strong>In data stream processing systems Quality of Service (or QoS) is extremely important. The system should try its best to meet the QoS requirements specified by a user. On account of this difference, unlike in a database management system, a query cannot be optimized once and executed. It has been shown that different scheduling strategies are useful in trading tuple latency requirements with memory and throughput requirements. In addition, data stream processing systems may experience significant fluctuations in input rates.  In order to meet the QoS requirements of data stream processing, a runtime optimizer equipped with several scheduling and load shedding strategies is critical. This entails monitoring of QoS measures at run-time to effect the processing of the queries to meet expected QoS requirements.  This thesis addresses runtime optimization issues for MavStream, a data stream management system (DSMS) being developed at UT Arlington. In this thesis we develop a runtime optimizer for meeting the output (latency, memory, and throughput) of a continuous query (CQ) with its QoS requirements. The calibration is done by monitoring the output and comparing it with the expected output characteristics. Alternative scheduling strategies are chosen as needed based on the runtime feedback. A decision table is used to choose a scheduling strategy based on the priorities of QoS requirements and their violation. The decision table approach allows us to add new scheduling strategies as well as compute the strategy to be used in an extensible manner. A master scheduler has been implemented to enable changing scheduling strategies in the middle of continuous query processing and optimize each query individually (that is, different queries can be executed using different schedulers).   In addition, to cope with situations where the arrival rates of input streams exceed the processing capacity of the system, we have incorporated load shedding component into the runtime optimizer as well. We have implemented shedders as part of the buffers to minimize the overhead for load shedding. We also choose load shedders that minimize the error introduced into the result as a result of dropping some tuples. Finally, load shedders are activated and deactivated by the runtime optimizer. Both random and semantic shedding of tuples is supported.   A large number of experiments have been conducted to test the runtime optimizer and observe the effect of different scheduling strategies and load shedding on the output of continuous queries. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>CA-OLE: An Adaptive and Collaborative Online Learning Environment</strong><br /> Friday, November 17, 2006<br /> Paola Gomez Santamaria<br /><p><a href="#" id="375-show" class="showLink" onclick="showHide(375);return false;">Read More</a></p></p><div id="375" class="more"><p><a href="#" id="375-hide" class="hideLink" onclick="showHide(375);return false;">Hide</a></p><p><strong>Abstract: </strong>Demand for online learning environments has grown in the past few years, and schools have been offering more distance courses to their students.  Designing adequate online learning environments is considerably more challenging than conventional Face-To-Face courses; therefore traditional methods can not be used.  This thesis proposes the creation of an Adaptive and Collaborative Online Learning Environment (CA-OLE) that provides a structure where instructors can integrate their lesson materials with an adaptive system and collaborative tools.  This is achieved by integrating AHA!, an adaptive framework (developed at the Eindhoven University of Technology) with collaborative tools.  AHA! allows instructors to create a domain model for their lessons, interconnecting concepts they would like their students to acquire. At the same time, professors set up their learning materials as lessons on CA-OLE.  Students interact with CA-OLE by accessing different lessons the instructors provided, an user model is created on AHA!, that is used by CA-OLE to evaluate how well the students are doing and what the next steps are.  Collaborative tools allow students to interact with each other, where group formation is determined by how well they did on the pre-test and following evaluations done during the learning experience.  We show how the Adaptation of Presentation and Content, as well as the collaboration between the different actors improves their learning skills and methods, as well as their knowledge on the concepts presented and evaluated.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Distributed Sensor Data Management Infrastructure Based On 802.15.4/ZIGBEE Networks</strong><br /> Wednesday, November 15, 2006<br /> Tianqiang Li<br /><p><a href="#" id="373-show" class="showLink" onclick="showHide(373);return false;">Read More</a></p></p><div id="373" class="more"><p><a href="#" id="373-hide" class="hideLink" onclick="showHide(373);return false;">Hide</a></p><p><strong>Abstract: </strong>By commanding a large number of nodes capable of sensing the physical environment and communicate through wireless interfaces, wireless sensor networks have revealed its vast potential in a plethora of applications. In this thesis, we design and develop a prototype of a wireless sensor node. Leveraging existing low power radio solutions such as 802.15.4 networks in an SOC, our focus is given to the embedded software systems, including the data management, media access control, and data querying support. Specifically, the system consists of the sensor data collecting subsystem, data storing subsystem, data query algorithm and subsystem, data intercommunication subsystem and browser based user accessing interface. Sensor data collection and storing system will periodically save the detected value from sensor to the memory system and later to the permanent flash memory. Data query subsystem will provide an efficient approach to locate the data to be queried among large number of stored data. Data intercommunication subsystem is in charge of the radio frequency communication between devices, the delivery back and forth for the query request and response is the one of the major duty in this subsystem. The user accessing interface support the user to submit a query request in a browser with the standard http link address in user&#039s personal computer. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Automated Software Testing Using Covering Arrays</strong><br /> Wednesday, November 15, 2006<br /> Chinmay Jayaswal<br /><p><a href="#" id="376-show" class="showLink" onclick="showHide(376);return false;">Read More</a></p></p><div id="376" class="more"><p><a href="#" id="376-hide" class="hideLink" onclick="showHide(376);return false;">Hide</a></p><p><strong>Abstract: </strong>Modern society is increasingly dependent on the quality of software systems. Software testing is a widely used approach to ensure software quality. Since exhaustive testing is impractical due to resource constraints, it is necessary to strike a balance between test effort and quality assurance. One approach, called interaction testing, characterizes the system under test by a set of parameters and the respective test values (domain size) for each parameter. Instead of testing all possible combinations of values of all the parameters, interaction testing constructs a covering array as a test set to cover all the t-way combinations, i.e., combinations involving t parameters, where t is referred to as the degree of interaction and is usually small. Each combination of values of a set of parameters represent a possible interaction among those parameters. The rationale of interaction testing is that not every interaction contributes to every fault, and many faults can be exposed by the interactions among a small number of parameters. Empirical studies have shown that interaction testing can significantly reduce the number of tests while still detecting faults effectively. This thesis describes the design, implementation, and evaluation of an interaction testing tool called FireEye. FireEye constructs covering arrays that provide multi-way coverage up to 6-way testing. We describe several ways to improve the performance of FireEye in terms of time and space required for test generation. We also compare the performance of FireEye with other publicly available tools.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>End-to-End Optimal Algorithms for Traffic Engineering, Failure Recovery and Detection in Connectionless Networks</strong><br /> Wednesday, November 15, 2006<br /> Sukruth Srikantha<br /><p><a href="#" id="377-show" class="showLink" onclick="showHide(377);return false;">Read More</a></p></p><div id="377" class="more"><p><a href="#" id="377-hide" class="hideLink" onclick="showHide(377);return false;">Hide</a></p><p><strong>Abstract: </strong>In this thesis, we address the above issues with a scheme that does not require any modifications to the underlying routing protocols. We focus on the application of a set of distributed control laws on the edge routers in a connectionless network. The control laws provide optimal data rate adaptation and load balancing where multiple disjoint paths are available between an ingress-egress pair. The contribution of this thesis also includes the implementation of a source inferred congestion detection scheme, to find congested paths and node/link failures. The information inferred from this mechanism serves as input for the operation of the control laws to perform optimal rate adaptation and load balancing. The proposed approach endows the network with the important property of stability and robustness with respect to node/link failures. The congestion detection scheme also serves as a failure detection mechanism and thus helps to overcome the IGP convergence problem in OSPF. On the occurrence of a failure, the congestion detection mechanism is capable of detecting it within a few milliseconds. This information is provided to the control laws, which reroute traffic away from the inoperative node/link and converge to the optimal allocation for the &#039reduced&#039 network. In comparison, OSPF takes several tens of seconds for the failure detection and convergence process. Highly scalable TE and FR features are implemented on edge routers based on these control laws and the congestion detection mechanism without the involvement of the routers in the network core. Simulation results are presented to demonstrate the advantages of the proposed approach under a variety of network scenarios. Results show that the convergence time reduces from few tens of seconds to the order of milliseconds. The average throughput is improved considerably due to dynamic rate adaptation and load balancing provided by the control laws.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Personalizing (Re-Ranking) web search results using information present on a social network</strong><br /> Wednesday, November 08, 2006<br /> Sushruth Puttaswamy<br /><p><a href="#" id="372-show" class="showLink" onclick="showHide(372);return false;">Read More</a></p></p><div id="372" class="more"><p><a href="#" id="372-hide" class="hideLink" onclick="showHide(372);return false;">Hide</a></p><p><strong>Abstract: </strong>We describe a social search engine paradigm which can be built on top of a classic search engine (e.g. Google, Yahoo, etc.) and a social information network (such as FriendSter). In this thesis, the objective was to design algorithms and develop methods to efficiently combine information available in the underlying systems (Search Engine &amp; Social Information Network) to better satisfy the search needs of a user. We are interested on how to efficiently employ social information to re-order a list of URLs retrieved by querying a search engine. The objective was to re-order the list of URLs in a way that favors URLs that are more relevant to user&#039s interest towards a personalized search engine. We show through rigorous experimental results that the probability of obtaining superior search results using this method is higher compared to the results of the regular search engines. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient Main Memory Algorithms for Significant Interval and Frequent Episode Discovery</strong><br /> Wednesday, November 01, 2006<br /> Sagar Savla<br /><p><a href="#" id="371-show" class="showLink" onclick="showHide(371);return false;">Read More</a></p></p><div id="371" class="more"><p><a href="#" id="371-hide" class="hideLink" onclick="showHide(371);return false;">Hide</a></p><p><strong>Abstract: </strong>There is a considerable research on sequential mining of time-series data. Sensor-based applications such as Smart Home require prediction of events for automating the environment using time-series data collected over a period of time. In these applications, it is important to predict tight and accurate intervals of interest to effectively automate the application. Also, detection of frequent patterns is needed for the automation of sequence of happenings. Although, there is a considerable body of work on sequential mining of transactional data, most of them deal with time point data and make several iterations over the entire data set for discovering frequently occurring patterns.  An alternative approach consisting of three phases has been proposed for detecting significant intervals and frequent episodes. In the first phase, time-series data is folded over a period (day, week, etc.) using which intervals are formed. The advantage of this approach is that the data set is compressed substantially thereby reducing the size of input used and hence the computation. Also, each event/device can be processed individually allowing for parallel computation of individual events. Significant intervals that satisfy the criteria of minimum confidence and maximum interval-length specified by the user are discovered from this compressed interval data.  In this thesis, we present a new single pass main memory algorithm (OnePass algorithm) for detecting significant intervals. Unlike its counterparts, OnePass algorithm does not follow the classic Apriori style to discover significant intervals. While analyzing the OnePass algorithm, we shall discuss its characteristics, complexity, scalability issues and its advantages over other algorithms. We shall also compare the performance of our algorithm with previously developed SID and SQL-based algorithms.  For the second phase, we propose a frequent episode discovery (FED) algorithm (OnePassFED algorithm). The OnePassFED algorithm proposed in this thesis, is a main memory algorithm. The OnePassFED algorithm works on the significant intervals discovered in the first phase to discover interesting episodes in a single pass as compared to the Apriori class of algorithms. This approach is significantly more efficient and scales well as compared to traditional mining algorithms. Extensive experimental analysis establishes its efficiency and scalability. We also compare the performance of this algorithm with our previously developed SQL based Hybrid Apriori algorithm.  The third and final phase is frequent episode validation phase. This phase validates the frequent episodes generated by the second phase if they are to be interpreted on a finer granularity (example, week using daily periodicity). For the third phase, we present a BatchValidation algorithm. The steps carried out for frequent episode validation are redesigned in the BatchValidation algorithm to give a much improved performance. Experiments carried out to compare the performance of the BatchValidation algorithm with previously developed Naive algorithm showed that, the BatchValidation algorithm is significantly faster. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Evaluating Indexing and Routing schemes for Spatial Queries in Sensor Networks</strong><br /> Friday, July 21, 2006<br /> Raja Rajeshwari Anugula<br /><p><a href="#" id="366-show" class="showLink" onclick="showHide(366);return false;">Read More</a></p></p><div id="366" class="more"><p><a href="#" id="366-hide" class="hideLink" onclick="showHide(366);return false;">Hide</a></p><p><strong>Abstract: </strong>Recent advances in low-power sensing devices coupled with widespread availability of wireless ad-hoc networks, has fueled the development of sensor networks. These senor networks have various applications such as to monitor conditions at different locations (temperature, pressure, rainfall, vibrations etc.,), tracking of objects and so on. Each device is equipped with an energy source (usually with a battery), memory, CPU and communication bandwidth, which is severely constrained. Hence each sensor network is comprised of hardware for sensing, software for communication and computational algorithms. Spatial queries are commonly applied to sensor network, for example: “Find the highest temperature sensed in a particular region”. Spatial query processing is considered extensively in the context of centralized databases. In this thesis, we examine spatial queries in distributed sensor networks. We use a binary tree, which is constructed by a MULT routing protocol and the meta-data that has been collected in the base station to construct an index structure, thus resolving the spatial queries, we evaluate and compare the behavior of our approach with other kind of indexing structure and show that our mechanism is efficient in terms of increased in-network computation and lowered communication.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An application of parallel and distributed computing methods to approximate pattern matching of genetic regulatory motifs</strong><br /> Wednesday, July 19, 2006<br /> Tushar K Jayantilal<br /><p><a href="#" id="367-show" class="showLink" onclick="showHide(367);return false;">Read More</a></p></p><div id="367" class="more"><p><a href="#" id="367-hide" class="hideLink" onclick="showHide(367);return false;">Hide</a></p><p><strong>Abstract: </strong>Bioinformatics is a relatively new scientific field concerned with providing computational means and support to research in molecular biology and genetics. It draws from many different areas of computer science, including database theory, algorithm design and analysis, and artificial intelligence, to name just a few. In many applications, such as one described in this thesis, a biologist is interested in locating a particular pattern, or sequence motif, in a given string or set of strings over the four-letter DNA alphabet. In this thesis we present an efficient approach to locating promoter and other regulatory sequences in entire genomes or in specific target areas. Promoters are short conserved sequences located upstream of the genes they regulate, and they have an essential role in driving the expression of the genes. However, in higher organisms promoter sequences are very diverse, and their motifs can feature substantial sequence variation, including character (chemical base) substitutions, insertions and deletions. We were thus interested in designing methods for the efficient search for approximate matches of the target sequences to putative promoter consensus. To achieve this goal we have used a combination of classic pattern-matching algorithms and high-performance computing, parallelizing the search over a number of processors. We have used these methods in the genome-wide search for a 14-base promoter element in the genome of the fruit fly Drosophila melanogaster, postulated to have a role in the testis-specific expression of the genes it controls. The list of genes of interest has been provided by our collaborator from the UTA Department of Biology, and we have obtained the raw sequence and other genetic data from FlyBase, a public database maintained by a consortium of Drosophila researchers. Although the number of genes we were interested in was moderate, 791, the number of exact patterns approximately matching the 14 base consensus was very large, approximately 38,000. This problem has thus guided the design of the methods we describe in this thesis. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Relation Database Approach for Frequent Subgraph Mining</strong><br /> Monday, June 26, 2006<br /> Subhesh Kumar Pradhan<br /><p><a href="#" id="364-show" class="showLink" onclick="showHide(364);return false;">Read More</a></p></p><div id="364" class="more"><p><a href="#" id="364-hide" class="hideLink" onclick="showHide(364);return false;">Hide</a></p><p><strong>Abstract: </strong>Data mining aims at discovering interesting and previously unknown patterns from data sets. Further more, graph-based data mining represents a collection of techniques for mining the relational aspects of data represented as a graph. Complex relationships in data can be represented using graphs and hence graph mining is appropriate for analyzing data that is rich in structural relationships. Database mining of graphs, on the other hand, aims at directly mining graphs stored in a database using SQL queries. Several SQL based mining algorithms have been developed successfully and their efficiency and scalability have been established.  SQL-based algorithms have also been developed for mining best substructures in a graph. Determining frequent subgraphs is another type of graph mining for which only main memory algorithm exist. There are many applications in social networks, biology, computer networks, chemistry and World Wide Web that require mining of frequent subgraphs. Also, data in most applications are directly stored in a database. Hence, there is a need for developing for developing as SQL-based approach for frequent subgraph mining that scales to very large data sizes. The focus of this thesis is to apply relational database techniques to support frequent subgraph mining over a set of graphs. Our primary goal is to address scalability of graph mining to very large data sets, not currently addressed by main memory approaches. This thesis addresses the most general graph representation including multiple edges between any two vertices, and includes cycles. In the process of developing frequent subgraph mining over a set of graphs, the substructure representation of previous work (HDB-Subdue) has been leveraged and extended. An algorithm is presented for frequent subgraph mining over a set of graphs. We also present an algorithm for pseudo duplicate elimination that is more efficient than the one used in the previous approach (HDB-Subdue). This thesis also presents an efficient approach to infer structural relationships from relational data to facilitate graph mining. The approach developed for the task infers the entity-relationship model for the database using the table descriptions along with primary and foreign key constraints and generates the instances of the graphs from the populated instances of the relations. The primary focus of this approach is the portability of the system across various relational database platforms and to minimize memory/space requirement. As part of this task the following issues were addressed: i) representation of an individual relations as a graph (template), ii) representation of multiple relations as a graph (based on foreign key constraints), iii) alternative representations for ii), and iv) an algorithm which uses the most appropriate sequence in which the relations are processed to generate graph instances to minimize memory/space requirements.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Context Aware Energy Conservation in a Pervasive Computing Environment</strong><br /> Tuesday, June 20, 2006<br /> Pratibha Joseph<br /><p><a href="#" id="365-show" class="showLink" onclick="showHide(365);return false;">Read More</a></p></p><div id="365" class="more"><p><a href="#" id="365-hide" class="hideLink" onclick="showHide(365);return false;">Hide</a></p><p><strong>Abstract: </strong>Extending battery lifetime is one of the most critical and challenging problems in mobile systems. When the mobile device sounds a low battery alarm, the user is given an ultimatum to quickly find a power source or suspend work. The greatest utility of mobile devices is their ability to be used anywhere, and at anytime. But power limitations of these devices seem to hinder this goal. The ever-growing needs of mobile users for increased lifetime of wireless devices imply that emerging wireless systems must be more energy-efficient than ever before.  Cyber foraging or remote resource exploitation may be an efficient way to deal with this problem in a pervasive computing environment. Mobile devices can save battery power by migrating tasks to a nearby wired infrastructure or to other wireless devices in the environment with higher battery capacity and processing power. However, this process requires considerable automation to minimize energy consumption and user distraction. Also, a pervasive computing environment is highly dynamic and the contexts in the environment change rapidly as devices enter and leave the network. Thus, it is important that the devices are aware of the changing context and adapt to these changes accordingly. This leads to the challenge of how the devices would detect these changes and secondly how they would adapt to these changes after they are detected.  A middleware service framework has been developed and deployed over a network of machines that exploit remote resources. The framework adopts a context-aware approach to make intelligent decisions on task migration. It uses the achievable throughput in an end-to-end path as the context. The throughput achievable to each of the remote devices is measured and stored, and this is used to examine the trade-off between communication power expenditures and the power cost of local processing.  For a set of tasks, energy savings of up to 43% are achieved through this process of context aware energy conservation. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>CROSS-LAYER CAPACITY ANALYSIS  OF DMT (DISCRETE MULTITONE) BASED ADSL SYSTEM</strong><br /> Monday, May 15, 2006<br /> Indradip Ghosh<br /><p><a href="#" id="361-show" class="showLink" onclick="showHide(361);return false;">Read More</a></p></p><div id="361" class="more"><p><a href="#" id="361-hide" class="hideLink" onclick="showHide(361);return false;">Hide</a></p><p><strong>Abstract: </strong>The DSL technology is the main access side networking for the Telecom Network Providers offering broadband networking solutions for the Internet users in most of the developed countries. The wide scale deployment of this technology is possible because of the existing subscriber base of POTS (Plain Old Telephone System), whose customers are connected through the same physical media of twisted pair copper wire to the internet as used by DSL. Additionally, the ability of DSL to naturally inherit the copper wire based access network of POTS, which is old and hence highly stable, also proves to be a big economical factor endorsing DSL's business model.  DSL technology essentially uses the available spectrum capacity of the twisted pair copper medium that exists in the current outside plant by using advanced modulation and error correcting codes. Therefore POTS has bequeathed to it two advantages, firstly minimal resource planning for deployment and secondly a subscriber base that out numbers any other contemporary broadband technology by a very large margin, reckoning with the fact that there is hardly any place on this earth which does not have a POTS presence. Another primary motivation for the development of this technology was to solve the famous last mile problem that made DSL a highly desirable mode of connecting to the backhaul data network at broadband speeds for almost the same costs that one would pay for a dial-up connection. However, DSL also has its limitations when it comes to actual delivery of bandwidth over longer distances from the Central Office (CO) as DSL performance is a distance sensitive. It is observed that as the length of the local loop increases, the signal quality decreases and the connection speed goes down. The occurrence of this phenomenon is due to the presence of a number of interfering sources in the cable bundle, irrespective of whether the noise signal is of a similar nature or dissimilar one. Nevertheless its been proven that the major impairment is contributed by noise signals of similar nature commonly referred to a Cross-Talk noise signals. These noise signals are of two types, namely the "Near End Cross Talk (NEXT)" and "Far End Cross Talk (FEXT)". Among the two sources of noise, NEXT has a stronger bearing than FEXT as the later gets attenuated by traveling the entire loop length from the CO to the customer premises. In this thesis our aim is to develop the analytical model of the cross layer characteristics and determine the effective channel capacity of a NEXT limited ADSL channel. To achieve this end we propose a model for capturing the Cross-Layer dynamics of an ADSL system by quantifying the impact of NEXT interference on the effective channel capacity of the ADSL line. We do this by modeling the channel state with respect to the number of active interferers as a (Finite State Markov Chain) FSMC.. Then we consider the source of traffic in the host machine as an "ON-OFF" source that is continuously drained off its contents from the system's buffer by the underlying ADSL channel that has the above said interference sources. Hence the channel state is directly affecting the channel capacity and consequently the contents of the system buffer. Thus we consider two level of uncertainty in our model, at the source level because of multiple on-off durations of traffic sources, and at the channel level due to multiple on-off type interference sources affecting the channel. We solve this complex random traffic handling of the channel by using the fluid queue modeling method. We obtained the effective channel capacity and the amount of buffer we need to provision for a given probability of buffer overflow (packet loss) as quality of service.  The proposed model is generic in nature and can be extended by incorporating other types of noise sources to improve the results.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Multiple Object Tracking Using Particle Filters</strong><br /> Monday, May 08, 2006<br /> Hwangryol Ryu<br /><p><a href="#" id="353-show" class="showLink" onclick="showHide(353);return false;">Read More</a></p></p><div id="353" class="more"><p><a href="#" id="353-hide" class="hideLink" onclick="showHide(353);return false;">Hide</a></p><p><strong>Abstract: </strong>We describe a novel extension to the Particle Filter algorithm for tracking multiple objects. The recently proposed algorithms and the variants for multiple object tacking algorithms estimate multi-modal posterior distributions that potentially represent the multiple peaks (i.e., multiple tracked objects). However, the specific state representation does not demonstrate birth, death and more importantly partial/complete occlusion of the objects. Furthermore, the weakness of the Particle Filter such that the representation may increasingly bias the posterior density estimates toward objects with dominant likelihood makes the multiple object tracking algorithms more difficult. To circumvent a sample depletion problem and maintain the computational complexity as good as the joint Particle filters under certain assumptions: (1) targets move independently, (2) targets are not transparent, (3) each pixel of image only come from one of the targets, we proposed a new approach dealing with partial and complete occlusions of a fixed number of objects in an efficient manner that provides a robust means of tracking each object by projecting particles into the image space and back to a particle space. The projection procedure between the particle space and the image space represents an important probability density function not only to give more trust to a target being visible, but also to explain an occluded target. In addition, the joint Particle filters suffer from the curse of dimensionality in the number of the targets to being tracked, whereas the proposed algorithm only adds a constant factor to the computational cost of the standard Particle filters. To present qualitative results, the experiments were performed over the sequence of the real images which is composed of the rectangular boxes in the different colors. The experiments demonstrated that the Particle filters implemented using the proposed method effectively and precisely track multiple targets, whereas the standard Particle filters failed to track the multiple targets.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>to Relate the Observation to the Internal State for Robot Imitation</strong><br /> Friday, May 05, 2006<br /> Heng Kou<br /><p><a href="#" id="357-show" class="showLink" onclick="showHide(357);return false;">Read More</a></p></p><div id="357" class="more"><p><a href="#" id="357-hide" class="hideLink" onclick="showHide(357);return false;">Hide</a></p><p><strong>Abstract: </strong>Imitation is a powerful form of learning widely used by humans and animals. Inspired by biological systems, learning from observation has been studied for years in robotics systems, where the imitator, a robot, learns a new task by observing how the demonstrator, either a human or artificial agent, executes the task. Most of the approaches taken are assuming that the demonstrator and imitator have similar bodies and capabilities, which is not necessarily true in the real world. In this thesis we mainly deal with the case where the demonstration and imitator have different bodies and capabilities. We introduce a distance function to represent the similarity between the observed state and internal state. The robot tries to learn the distance function such that it is able to find a policy that approximately achieves the demonstration even if the demonstrator and imitator are substantially different. The learning process involves two steps. First the robot learns the correspondence between the observed state and the internal state. Second the robot learns which aspect of the demonstration is more important such that the robot is able to finish the task even if the environment is slightly different. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>SIMULATION OF SENSOR RESPONSES OF ADVANCED SECURITY SYSTEMS</strong><br /> Friday, April 21, 2006<br /> Janakiram Natarajan<br /><p><a href="#" id="354-show" class="showLink" onclick="showHide(354);return false;">Read More</a></p></p><div id="354" class="more"><p><a href="#" id="354-hide" class="hideLink" onclick="showHide(354);return false;">Hide</a></p><p><strong>Abstract: </strong>Security systems are becoming an increasingly important area of research. Advanced security detection and surveillance systems that integrates a variety of detection mechanisms, like signals from different kinds of sensors, is expected to yield more accurate assessment than any one sensor analyzed individually. Designing and investigating these systems, to date, has relied primarily on physical deployments and experimentation. While the quality of the results from such efforts is excellent, the need to work with the physical systems directly imposes a substantial research impediment. One obvious possibility for widening the scope of what can be investigated is to employ simulation as an alternative to experimentation with deployed systems.  Our goal is to develop a simulator for simulating infrared, millimeter wave and metal detector sensor systems. The simulator was developed using the Java 2D API. The data obtained from the simulator and the real systems were processed using the WEKA library of machine learning tools to produce threat classifiers which in turn were to be compared in order to establish the accuracies of the simulator with respect to the real system. We used t-test to compare the classification accuracies obtained using the real and simulation data. This would provide us with the P-value, that corresponds to the probability that the two data sets are not different, in other words, the probability that the difference between data sets that is observed is due to chance alone. We obtained a P value greater than 0.05 which showed that the data sets are not significantly different. We also found that the simulated data helps in increasing the classification accuracy of the threat classifiers when it is combined with the real data. Also the agreement between threat classifiers obtained using simulated and combined data validated the accuracy of our simulator. We believe that the simulator can serve as a cost effective alternate tool for studying the characteristics of the security systems and help in constructing better threat classifiers. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>TOWARDS OPTIMUM PLAY-OUT BUFFERING DELAY IN VoIP</strong><br /> Wednesday, April 19, 2006<br /> Ruchir Shende<br /><p><a href="#" id="352-show" class="showLink" onclick="showHide(352);return false;">Read More</a></p></p><div id="352" class="more"><p><a href="#" id="352-hide" class="hideLink" onclick="showHide(352);return false;">Hide</a></p><p><strong>Abstract: </strong>Voice over Internet Protocol (VoIP) or transmission of real-time voice packets over the Internet is slowly emerging as a cost-effective alternative to the traditional Public Switched Telephone Network (PSTN) and this trend is expected to continue in the future. However, varying end-to-end delay and packet loss, which are inherent in a packet-switched network like the Internet, lead to relatively lower quality of VoIP calls. The call quality can be improved by adaptively adjusting the play-out buffer at the receiver to reduce the impact of the delay and jitter during the transmission. A standard play-out strategy uses a weighted moving average of the mean and variance of network delay to adaptively set the play-out deadline. Other techniques used include adaptive adjustment of talk-spurt and silence periods. Adjustments can also be made within talk-spurts by scaling individual voice packets using time-scale modification.   In this thesis, we simulate an adaptive play-out algorithm that uses smoothed average of network delays and analyze its performance using a discrete event-based simulator that has been developed in Java. For an observed packet loss rate, we determine the optimum buffering delay and compare it to the buffering delay reported by the algorithm. We then tweak the algorithm so that the average buffering delay reported by the modified algorithm is nearer to the optimum value. As a result, the end-to-end delay is reduced improving the call quality perceived by the end-user. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Anytime top-k queries on Exact and Fuzzy Data</strong><br /> Wednesday, April 19, 2006<br /> Bhushan Chaudhari<br /><p><a href="#" id="356-show" class="showLink" onclick="showHide(356);return false;">Read More</a></p></p><div id="356" class="more"><p><a href="#" id="356-hide" class="hideLink" onclick="showHide(356);return false;">Hide</a></p><p><strong>Abstract: </strong> Top-k queries on large multi-attribute data sets are fundamental operations in information retrieval and ranking applications. In this paper, we initiate  research on the anytime behavior of top-k algorithms on exact and fuzzy data.  In particular given specific top-k algorithms we are interested to study their  progress towards identification of the correct result at any point of the  algorithms's execution. We adopt a probabilistic approach where we seek to  report at any point the scores of the top-k results the algorithm has identified,  as well as associate a confidence with this prediction. Such a functionality can  be a valuable asset when one is interested to reduce the runtime cost of top-k  computations. We show analytically that such probability and confidence are  monotone in expectation. We present a thorough experimental evaluation to  validate our techniques using both synthetic and real data sets.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Relative Clock Drift Rate Based Secure Time Synchronization for Wireless Sensor Networks</strong><br /> Friday, April 14, 2006<br /> Jae Sung Choi<br /><p><a href="#" id="342-show" class="showLink" onclick="showHide(342);return false;">Read More</a></p></p><div id="342" class="more"><p><a href="#" id="342-hide" class="hideLink" onclick="showHide(342);return false;">Hide</a></p><p><strong>Abstract: </strong>Time synchronization is critical issue to many wireless sensor network applications such that target tracking, ?TESLA for authenticated broadcasting, TDMA radio scheduling, and secure localization. However, the most of existing time synchronization algorithms in wireless sensor networks did not consider malicious attacks in hostile environments. In this thesis, we propose a Relative Clock Drift Rate Based Secure Time Synchronization(RSTS) schemes to address security problems. RSTS alleviates delay attacks and incorrect time stamp transmissions caused by external or internal malicious attackers. We discuss a simple estimation technique to calculate a relative clock drift rate between sender-receiver clocks without aid of third party, because the estimated relative clock drift rate is important to detect malicious attacks, when we employ a threshold-based algorithm to calculate bounds of acceptable the offset and the delay. We show that RSTS incurs minimal computational overhead. Further, it does not consume any extra bandwidth when it is compared to any other sender-receiver synchronization algorithms.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>UBCA:  A UTILITY BASED CLUSTERING ARCHITECTURE</strong><br /> Friday, April 14, 2006<br /> Brent Lagesse<br /><p><a href="#" id="349-show" class="showLink" onclick="showHide(349);return false;">Read More</a></p></p><div id="349" class="more"><p><a href="#" id="349-hide" class="hideLink" onclick="showHide(349);return false;">Hide</a></p><p><strong>Abstract: </strong>Systems based on the Peer to Peer (P2P) architecture have recently spread in popularity.  File Sharing and ad hoc networks have fueled the architecture&#039s rise to prominence.  This architecture generates new challenges in scalability, fairness, quality of service, and distributing load.  Current solutions to these challenges tend to fall into two main areas:  incentives and system design.  Incentive-based approaches seek to appeal to the self-interested nature of agents by requiring that they accrue a certain amount of service to the system in order to access resources (or to gain priority for resource access).  System design includes many efforts such as distributed hash tables (DHTs) and graph-theoretical based designs.  The results of these approaches have seen some success, but often introduce new problems in place of those they solve.  For instance, incentives raise issues regarding trust and security while DHTs require constraints to be placed on the P2P system which makes it less decentralized. We introduce a Utility-Based Clustering Architecture, UBCA, designed to address scalability, fairness, quality of service, and load distribution through the use of implicit incentives.  The UBCA runs on peer agents and dynamically clusters them into groups during the execution based on mutual utility gained as a result of the formation of the group. Simulation results of the UBCA implemented on top of Gnutella show improved results in terms of bandwidth and latency per access, overhead costs.  Furthermore, the UBCA has shown potential in simulations as a means of classifying peers.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>MONITOREXPLORER: A STATE-BASED EXLORATION BASED TOOL TO TEST JAVA</strong><br /> Thursday, April 13, 2006<br /> Vidur Gupta<br /><p><a href="#" id="344-show" class="showLink" onclick="showHide(344);return false;">Read More</a></p></p><div id="344" class="more"><p><a href="#" id="344-hide" class="hideLink" onclick="showHide(344);return false;">Hide</a></p><p><strong>Abstract: </strong>A monitor is a concurrency construct that encapsulates data and functionality for allocating and releasing shared resources. Java associates a lock with every object. When a block of code is guarded by the 'synchronized' keyword then the thread has to get lock on objects inside the block. This block is a Java Monitor which guarantees mutual exclusion and the thread needs to get the locks before it can enter the monitor. The locks are released when the thread exits the guarded block. If the thread is not able to get all the locks it waits for the conditions to be right.  There are many application classes which are written using the Java Monitors and these are difficult to test due to the inherent complexities of the concurrent programs. The key challenge is to be able to trace all possible execution paths and then able to reproduce them for regression testing. Our work explores the state-space of the monitor application. The state space is explored in the depth first fashion. At each state the possible transitions are calculated pushed on a stack. The transition on the top of the stack is executed. This process is repeated till a duplicate or invalid state is detected.  The key features of the approach are data abstraction which helps reduce the number of states to be explored and introduction of threads on the fly to execute the selected transition that simulate the race conditions. We have developed 'MonitorExplorer', a tool that can used to test monitor applications. This tool has been used to test various applications and their mutants. The experimental results show that the tool is effective in detecting synchronization bugs.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A PERFORMANCE EVALUATION OF ALTERNATE NUMBERING BASED XML INDEXING TECHNIQUES</strong><br /> Wednesday, April 12, 2006<br /> Chul Ho Ahn<br /><p><a href="#" id="346-show" class="showLink" onclick="showHide(346);return false;">Read More</a></p></p><div id="346" class="more"><p><a href="#" id="346-hide" class="hideLink" onclick="showHide(346);return false;">Hide</a></p><p><strong>Abstract: </strong>Since XML became a standard of representing semi-structured data and  exchanging format over the web, the sheer volume of XML data has  become larger. While relational database represents data as a  structured format, XML represents data as a self-describing way of a  hierarchical tree structure. For expediting query process over XML  many different types of indexing techniques having each different  characteristic were emerged.  This thesis proposes a refined categorization of XML indexing  techniques namely structure-based, numbering-based, sequence-based,  keyword-based index after in-depth surveying of such techniques. We  will focus on numbering-based schemes. We will show performance  comparison according to each different XPath query among three  distinct numbering based XML indexing named GENE (Generic numbering  based), XISS (Range based numbering), XACC (Dimension based  numbering) over shallow/deep tree structures generated by ToXgene.  By doing our experiment, XACC showed relatively better performance  in most of the cases. An analysis goes to three dimensions: varying  size of the XML data, distinguished XPath queries having different  features, two different structures of XML data.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>CONCEPT-BASED SEARCH USING PARALLEL QUERY EXPANSION</strong><br /> Wednesday, April 12, 2006<br /> Rahul Joshi<br /><p><a href="#" id="347-show" class="showLink" onclick="showHide(347);return false;">Read More</a></p></p><div id="347" class="more"><p><a href="#" id="347-hide" class="hideLink" onclick="showHide(347);return false;">Hide</a></p><p><strong>Abstract: </strong>Search engines are the preferred way to look-up information over the Web. They are also the starting points for any explorative search tasks that are undertaken. Popular search engines use keyword matching to return the results without considering the various meanings or possible concepts that a word represents. Moreover, most queries are short and hence prone to being ambiguous. The user has to identify the relevant search results and read many irrelevant results in this process. A solution to this problem is to supplement the query in a way to make it less ambiguous. Such an automatic expansion of the query can help in a better matching of the concept that the user intended to explore. In this research, in order to represent the meaning of the query in a fuller, contextual sense, we expand the query using associated words from domain-specific models in the word vector space. We examine the effectiveness of this approach to increase the precision in Web search and the conditions when it is most beneficial.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>IMPROVING SECURITY DETECTION USING MULTIPLE SENSOR DATA</strong><br /> Wednesday, April 12, 2006<br /> Amar Singh<br /><p><a href="#" id="348-show" class="showLink" onclick="showHide(348);return false;">Read More</a></p></p><div id="348" class="more"><p><a href="#" id="348-hide" class="hideLink" onclick="showHide(348);return false;">Hide</a></p><p><strong>Abstract: </strong>Public security has always been a key issue for science. Metal detectors are primarily used for this purpose in the areas of aviation security, jail security, and security of various other social activities against terrorism and smuggling. The most advanced technology today makes use of metal detectors, gas trap sensors, and imaging techniques. The current models make use of these technologies separately but not in combination to determine a possible threat.   We propose a model based on fusion of data from multiple sensors to determine a threat. Our model makes use of previous knowledge learned to determine the threat level of a person. We show that our model can more accurately determine the presence of a threat as compared to any single sensor. The presence or absence of a threat is independent of the threat being metal, non-metal or a combination of these.  </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>The Advantages of Implementing Software Engineering Process Models</strong><br /> Monday, April 10, 2006<br /> Ricky Don Preuninger<br /><p><a href="#" id="339-show" class="showLink" onclick="showHide(339);return false;">Read More</a></p></p><div id="339" class="more"><p><a href="#" id="339-hide" class="hideLink" onclick="showHide(339);return false;">Hide</a></p><p><strong>Abstract: </strong>The North Atlantic Treaty Organization Science Committee  had discussions on the topic concerned the state of  Computer Science.  There were worldwide issues with the  development of software, the crisis being that software  projects did not seem ever to complete.  The study group  coined the term &quot;software engineering&quot; to be provocative  and implying need for software manufacturing to be similar  to traditional branches of engineering.  In the beginning,  individual programmers used whatever means worked to build  software.  Formal methods of design or programming did not  exist.  Programmers were never able to give a definitive  estimate as to how long a project would take.  Software  projects often were behind schedule, over cost, poorly  documented, poorly designed, and contained items other than  the requirements.  These projects were costing corporations  thousands of dollars.  The software industry was quite  undisciplined and it was obvious. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Alternative Architectures for improving document readability</strong><br /> Monday, April 10, 2006<br /> Ankur Bora<br /><p><a href="#" id="343-show" class="showLink" onclick="showHide(343);return false;">Read More</a></p></p><div id="343" class="more"><p><a href="#" id="343-hide" class="hideLink" onclick="showHide(343);return false;">Hide</a></p><p><strong>Abstract: </strong>A number of tools have been developed to improve the readability of documents. These tools assist users to make changes which result in an easy to read document. Most of these tools were developed for users who work independently with little interaction with other users. However, in recent years, because of the proliferation of Internet, there has been increased collaboration between users. A user may need to share his document repositories with others so that both can benefit. Another set of users may work with information from a specific domain. These domains may be located in different geographical areas.  Some users may like to import the result of the readability tool to other applications.   With improved computer processing power, storage capacity and transmission speed it is possible to develop systems to meet such needs. Current development in Graphical User Interfaces has also made it possible to develop a better editing tool. With these tools, it is possible to offer users more alternatives to restructure and simplify the document. This thesis describes a number of techniques to develop tools for improving document readability both in local and distributed environments. A Prototype has been developed for each of these techniques and it has been found that such systems are feasible and practical. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Investigation of Techniques to Increase The Scalability of Graph Based Data Mining Algorithms</strong><br /> Monday, April 03, 2006<br /> Srilatha Inavolu<br /><p><a href="#" id="341-show" class="showLink" onclick="showHide(341);return false;">Read More</a></p></p><div id="341" class="more"><p><a href="#" id="341-hide" class="hideLink" onclick="showHide(341);return false;">Hide</a></p><p><strong>Abstract: </strong>Frequent subgraph pattern recognition and graph-based relational learning have been an emerging area of data mining research with scientific and commercial applications. At the kernel of these algorithms are the computationally-expensive graph and subgraph isomorphism tests.  The graph isomorphism problem consists in deciding whether two graphs are isomorphic i.e., whether there is a one-one mapping between the vertices of the two graphs that respects the edge connections. Many graphs will be depicted quite differently but in actuality have the same inherent structure. This leads to the isomorphism problem. The graph isomorphism problem belongs to the class of NP problems and has been conjectured intractable though probably not NP-complete.  We hypothesize that approximation algorithms can be developed for the graph and subgraph isomorphism problems, and that these algorithms can improve the runtime of data mining systems that rely on these capabilities. We analyze the validity of our hypothesis by implementing and testing three approaches to the problem: a genetic algorithm for subgraph isomorphism</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Bridging Two Grids: The SAM-Grid/LCG Integration Project</strong><br /> Wednesday, March 29, 2006<br /> Tummalapalli Sudhamsh Reddy<br /><p><a href="#" id="338-show" class="showLink" onclick="showHide(338);return false;">Read More</a></p></p><div id="338" class="more"><p><a href="#" id="338-hide" class="hideLink" onclick="showHide(338);return false;">Hide</a></p><p><strong>Abstract: </strong>The SAM-Grid is an integrated data, job, and information management system. The SAM-Grid addresses the distributed computing needs of the Dzero experiment at Fermi Lab. The system typically relies on SAM-Grid specific services to be deployed at the remote facilities in order to manage the computing and storage resources. Such deployment requires special agreements with each resource provider and it is a labor intensive process. The Dzero VO also has access to computing resources through the LCG infrastructure. Therefore, we can enter into resource sharing agreements and deployment of standard middleware within the framework of the LCG project. The SAM-Grid/LCG interoperability project was started to enable the DZero users to retain the user-friendliness of the SAM-Grid interface, allowing, at the same time, access to the LCG pool of resources. This &quot;bridging&quot; between grids is beneficial for both SAM-Grid and LCG, since it minimizes the deployment efforts of the SAM-Grid team and exercises the LCG computing infrastructure with data intensive production applications of a running experiment.  The interoperability system is centered on job &quot;forwarding&quot; nodes, which receive jobs prepared by the SAM-Grid interface and submit them to LCG. We discuss the architecture of the system and how it addresses inherent issues of service accessibility and scalability. We also present the operational and support challenges that arise to operating the system in production. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Parallel Processing a Rubik&#039s Cube</strong><br /> Thursday, December 08, 2005<br /> Aslesha Nargolkar<br /><p><a href="#" id="337-show" class="showLink" onclick="showHide(337);return false;">Read More</a></p></p><div id="337" class="more"><p><a href="#" id="337-hide" class="hideLink" onclick="showHide(337);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis investigates parallel processing techniques for solving the 3 x 3 x 3 Rubik&#039s Cube. We explore various state-space search based algorithmic approaches to optimally solve the Cube. The parallel processing approach is based on the IDA using pattern database as the underlying heuristic because of its well established effectiveness. The parallel algorithm is an extension of the Michael Reid algorithm which is sequential. The parallel algorithm exhibits good speedup and scalability. Nearly 150 random as well as symmetrical cube configurations were tested for the experiments on sequential and parallel implementations. The proposed parallel algorithm using master-slave type of load balancing proves efficient in terms of time as well as memory resources while yielding optimal solution to find the state of a Rubik&#039s cube. Parallel processing helps in solving a Cube with initial cube configurations having solutions at a higher depth level in the search tree. Various comparative results are provided to support the efficiency of the parallel implementation.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Customication Of A Generic Search Engine By Adding User Categories</strong><br /> Tuesday, December 06, 2005<br /> Ajay Madkaiker<br /><p><a href="#" id="336-show" class="showLink" onclick="showHide(336);return false;">Read More</a></p></p><div id="336" class="more"><p><a href="#" id="336-hide" class="hideLink" onclick="showHide(336);return false;">Hide</a></p><p><strong>Abstract: </strong>The current search engines available on the Net are generic in nature. They do not consider user preferences and treat all users information needs in the same way. As a result they frequently return a large number of links, that do not meet the user&#039s information need. This requires more searching to find what the user is looking for. For example if a user is interested in a particular game, e.g. cricket, and enters the query world cup, a generic search engine would return links of all the sports that hold a world cup. The user has to browse through a considerable number of non-relevant pages before he is able to get to links he is looking for. This is also because search engines don&#039t have the ability to ask a few questions and they also can not rely on judgment and past experience to rank web pages, in the way humans can. This raises the issue of customizing a generic search engine to consider user preferences. There have been a number of attempts in the past to personalize the search for information on the Net. These systems are based on relevance feedback methods, similarity measures, of storing a user profile explicitly or implicitly. Some of them have shown impressive results in query expansion and providing pages similar to the user&#039s interest.   Here we propose a novel system for personalizing web search. Our method is based on creating a user profile as he performs his routine searches in a given user category. In this customization, the user is allowed to create personal user categories with which he could search for information on the Net without getting too many irrelevant links in his search results. The application enhances the query by adding words that are generated from the user profile stored for a particular user category. It uses information about the probabilistic co-occurrence of words in the user profile with other words in the query as a measure for adding words. The snippets returned from the generic search engine are then classified on the basis of the user profile and are re-ordered according to a measure representing the interest of the user.   With our method the user not only gets a personalized query expansion but also receives re-ordered search results from the search engine. In this system the query expansion is made to optimize the estimated returns of the search engine, taking into account the classification accuracy and re-ranking of results. The system would add words that would give the user the best possible returns according to his user profile.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>InfoSearch : A System for Searching and Retrieving Documents using Complex Queries</strong><br /> Friday, November 18, 2005<br /> Nikhil Deshpande<br /><p><a href="#" id="332-show" class="showLink" onclick="showHide(332);return false;">Read More</a></p></p><div id="332" class="more"><p><a href="#" id="332-hide" class="hideLink" onclick="showHide(332);return false;">Hide</a></p><p><strong>Abstract: </strong>The colossal amount of information available online has resulted in overloading users who need to navigate this information for their routine requirements. Although search engines have been effective in reducing this information overload, they support only keyword searches and queries that use Boolean operators. There are certain application domains where a finer granularity of searching is necessary. Consider searching a full-text patent database for documents containing more than n occurrences of a particular pattern, or for documents that have a particular pattern followed by another pattern within a specified interval. Such complex patterns involving proximity, word frequency, sequential and structural patterns are not supported by current search engines. Users searching documents based on narrow, precise semantics may need to specify such patterns. This calls for a document retrieval system that allows such expressive patterns. Existing alternatives for complex pattern searching stream in the entire data source for each query. This is suitable and required for searching patterns over frequently updated data sources like news feeds, in order to ensure freshness of the search results. However, this approach is inefficient for searching patterns over data sources that are relatively static. For such data sources, using an index over the stored documents should result in more efficient retrieval. In this thesis, we present InfoSearch, a system for specifying complex queries and retrieving the relevant documents by using information provided by an index over the document collection. InfoSearch allows specification of queries that include word frequency, proximity and sequential or structural patterns using a Pattern Specification Language (PSL). For searching these patterns and retrieving the matching documents, InfoSearch uses Pattern Detection Graphs (PDGs) to filter the results returned by the index.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Efficient XQuery Processing of Streamed XML Fragments</strong><br /> Thursday, November 17, 2005<br /> Seo Young Ahn<br /><p><a href="#" id="335-show" class="showLink" onclick="showHide(335);return false;">Read More</a></p></p><div id="335" class="more"><p><a href="#" id="335-hide" class="hideLink" onclick="showHide(335);return false;">Hide</a></p><p><strong>Abstract: </strong>XStreamCast is a push-based streamed XML query processing system that supports multiple servers and clients. The servers broadcast streamed XML data while the clients register to these servers for a specific service and process streamed XML fragments. This thesis presents methods for efficient XQuery processing of streamed XML fragments for the client. The XQuery parser parses the XQuery given by the user first. The client processes the fragments and stores only the needed data for the query. The query is then applied to stored XML fragments. This system can be valuable for managing the memory of the client because it does not have to deal with the entire XML document. This thesis shows the way to handle XQuery and XPath queries. Finally, this thesis shows the experimental results verifying our method for handling XQuery and XPath by comparing it with JAXP, which is the Java API for XML Processing.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Sabotaging Trusted Relationship: A Proposed Solution</strong><br /> Wednesday, November 16, 2005<br /> Rishi Jethwa<br /><p><a href="#" id="333-show" class="showLink" onclick="showHide(333);return false;">Read More</a></p></p><div id="333" class="more"><p><a href="#" id="333-hide" class="hideLink" onclick="showHide(333);return false;">Hide</a></p><p><strong>Abstract: </strong>This research is about one of the top network security problems, the Denial of Service attacks. The DoS attack blocks the services of a server to its legitimate users. The DoS service attacks carried out in the past have already cost billions of dollars to numerous organizations. The Research Work revolves around a novel DoS technique. A reliable solution for preventing such attacks has been proposed. This novel DoS approach which we call &quot;Sabotaging a Trusted Relationship&quot; uses TCP packet flooding attack. This attack aims at preventing two servers that trust each other from communicating. The history of DoS attacks says that the normal TCP packets are not commonly used for packet flooding attack. But this would be no longer true. The DoS attack in consideration uses the TCP flooding strategy. In reality, a workaround has to be created for the TCP protocol suit to send the raw TCP packets to the desired destination, although many present DoS tools available supports TCP flooding attack. The defense approached we proposed to combat such attacks requires some enhancements to the current routers as present routers are very specific hardware device. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Adaptive Agent Communities for Providing Services in Dynamic Networks</strong><br /> Wednesday, November 16, 2005<br /> Nayantara Mallesh<br /><p><a href="#" id="334-show" class="showLink" onclick="showHide(334);return false;">Read More</a></p></p><div id="334" class="more"><p><a href="#" id="334-hide" class="hideLink" onclick="showHide(334);return false;">Hide</a></p><p><strong>Abstract: </strong>New network applications are being created everyday to accommodate diverse user needs. Delivering services to the user in a timely manner taking into account network conditions, resources allocated and network load is a challenge. Multiprotocol Label Switching attempts to overcome best-effort service by providing a method for routing traffic around network congestion, resource reservation and quality of service (QoS) capabilities. IntServ and DiffServ are two other QoS models in  use today. IntServ provides per-flow guarantee of quality while DiffServ is based on aggregate service classes. Adaptive Network Service (ANS) is a community of adaptive, collaborating agents residing in the network that aims to provide enhanced performance, better quality of service and improved efficiency of network resources. ANS agents are distributed across the network and are strategically located to provide services and monitor network conditions. ANS agents gather network information in these locations and exchange this information with other agents in the community. Awareness of current network status enables the agent community to efficiently allocate resources and provide services in response to incoming user requests. Sharing information allows agents in one part of the network to be aware of conditions in other parts of the network. ANS uses this information to route user data flows away from congested network areas. The agents also share resource utilization information in different ANS nodes. Routing user connections to different service points based on current resource utilization leads to efficient use of resources. In our implementation we provide a TCP based service for transferring bulk data from a source to a destination. When a user contacts ANS, ANS first determines the best available node to service the user request. The ANS node to service the request is determined before the start of data flow and is selected on the basis of i) least congestion around the ANS node and ii) maximum availability of resources in the ANS node. This thesis describes an architecture for ANS and derives a mathematical model for ANS&#039s bulk data transfer service while presenting simulation results for a various experiments demonstrating the benefits of using ANS. Simulation results and model estimates show that ANS is able to achieve superior data transfer throughput compared to connections that do not use ANS scheme. Simulation results show that use of ANS improves data transfer performance by a factor of 2 in low traffic conditions and by a factor of up to 3.8 times in high traffic conditions. Future work in this direction includes introducing new services into the ANS framework and improving ANS agent intelligence to deliver these services in a user friendly way. We intend to enhance ANS for applications in pervasive computing environments. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Framework for Supporting Quality of Service Requirements in a Data Stream Management System</strong><br /> Wednesday, July 13, 2005<br /> Qingchun Jiang<br /><p><a href="#" id="325-show" class="showLink" onclick="showHide(325);return false;">Read More</a></p></p><div id="325" class="more"><p><a href="#" id="325-hide" class="hideLink" onclick="showHide(325);return false;">Hide</a></p><p><strong>Abstract: </strong>Currently, a large class of data-intensive applications, in which data are presented in the form of continuous data streams rather than static relations, have been widely recognized in the database community. Not only is the size of the data for these applications unbounded, but the data arrives in a highly bursty mode. Furthermore, these applications have to respond to events defined over data streams in a timely manner. In other words, these applications have to conform to Quality of Service (QoS) requirements for processing continuous queries over data streams. These characteristics make it infeasible to simply load the arriving data streams into a traditional database management system and use currently available techniques for their processing.<br /> Therefore, a data stream management system (DSMS) is needed to process continuous streaming data effectively and efficiently.<br /> In this thesis, we discuss and provide solutions to many aspects of a general-purpose DSMS with the emphasis on supporting QoS requirements and event and rule processing in the context of the MavHome project at the UT Arlington. Specifically, we address the following problems of a QoS-driven DSMS with Event-Condition-Action (ECA) support:<br /> 1. System Capacity Planning and QoS Metrics Estimation: We propose a queueing theory based model for analyzing a multiple continuous query processing system. Using our queueing model, we can estimate whether a given system can process the given CQs over the given data streams and satisfy their QoS requirements, given the input characteristics of data streams and resources such as CPU cycles and memory size. We also provide a solution to its reverse problem: estimate the main QoS metrics such as tuple latency and memory requirements through our close-form solution given CQs and input data streams. The estimated QoS metrics can be used as an indicator of system capacity and to assist QoS delivery.<br /> 2. Run-Time Resource Allocation (Scheduling Strategies): We propose a family of scheduling strategies for run-time resource allocation in DSMSs. Specifically, we propose an operator-path capacity (PC) strategy to minimize the overall tuple latency of final query results in a DSMS with computation sharing through common sub-expressions of CQs and provide detailed analysis and proofs of properties of the PC strategy. In order to decrease the memory requirement of the PC strategy, we also propose an operator-segment strategy and its variances: memory-optimal segment strategy (MOS) and simplified segment strategy (SS). The MOS strategy minimizes the total memory requirement and the SS strategy further decreases the tuple latency with a slightly higher memory requirement than the MOS strategy. We provide a comprehensive analysis of these strategies and prove the memory optimal property of the MOS strategy under a general DSMS. Finally, we propose a more applicable strategy to DSMSs – the threshold strategy, which is a hybrid of the PC strategy and the MOS strategy and inherits the properties of both strategies. <br />3. QoS Delivery Mechanism (Load Shedding): We develop a set of comprehensive techniques to handle the bursty nature of input data streams by activating/deactivating a set of shedders to gracefully discard tuples during overload periods in a general DSMS. We first formalize the problem and discuss the physical implementation of shedders. We then develop a set of algorithms to estimate system capacity, which implicitly determines when to shed load and how much to shed, to compute the optimal physical location of shedders in CQs, which determines where to shed load, and to allocate the total shedding load among non-active shedders, which determines how to shed load.<br /> 4. Event and Rule Processing: We develop an integrated model, termed Estream, to process complicated event expressions and rules under the context of data stream processing through a group of enhancements to a DSMS. These enhancements range from naming CQ and introducing semantic window and stream modifiers for improving the expressiveness power and computation ability of DSMSs to improved event operators, expressions, and various event consumption modes for detecting composite events. The Estream model greatly improves the expressiveness and computation ability of DSMSs in terms of processing complex real-life events and makes DSMSs actively respond to defined events over data streams and carry out defined sequences of actions automatically. <br /><br />Finally, the theoretical analysis is validated using a prototype implementation. We have prototyped the proposed solutions, algorithms, and techniques developed in this thesis in a general DSMS, termed MavStream, in C++. The system can process various Select-Project-Join continuous queries with run-time resource allocation and QoS delivery mechanisms. All solutions, algorithms, and techniques developed in this thesis are experimentally studied under the MavStream sys</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>An Approach for Behavior Discovery Using Clustering of Dynamics</strong><br /> Monday, April 25, 2005<br /> Ashokkumaar Loganathan<br /><p><a href="#" id="319-show" class="showLink" onclick="showHide(319);return false;">Read More</a></p></p><div id="319" class="more"><p><a href="#" id="319-hide" class="hideLink" onclick="showHide(319);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis presents a technique to extract and learn behaviors of objects from a stream of sensor observations. This technique aims to represent the actual data from the sensor into a sequence of behaviors, but should be more compact than actual sensory data. The robots identify segemtation points, i.e. where one behavior ends and another begins, from the data stream. Given the basic models the robots comes up with more appropriate models by themselves. An appropriate method for the segmenting the data and learning the models are presented. A new approach is designed for calculating the metric which would define the similarity of the model with the sensor readings. Unsupervised learning is used to find the ?natural groupings? of data. Then an EM approach is used for the robot learning. <br><br> The problem of extracting events becomes apparent when we intend to build robots which imitate humans. These robots first need to understand the behavior of objects in their environment. For this it must be able to perceive different behaviors of the same object. To detect any discrete behavior, we need to know the event points which will describe the beginning and end points of that particular behavior. Our goal is to design models of the discrete behaviors from a sequence of sensor observations. <br><br> Since the entire world contains too much of data to process, a simplified world is designed. The data what we consider would be those we get from different kinds of sensors such as cameras, sonars, etc., which observe the dynamics of events. We discuss the theory and the research for the proposed approach and we discuss the results and performance of this approach. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Development of Real-Time Transverse Profile Measurement System Using Scanning Laser</strong><br /> Monday, April 25, 2005<br /> Samar Asbe<br /><p><a href="#" id="321-show" class="showLink" onclick="showHide(321);return false;">Read More</a></p></p><div id="321" class="more"><p><a href="#" id="321-hide" class="hideLink" onclick="showHide(321);return false;">Hide</a></p><p><strong>Abstract: </strong>The thesis is about developing an embedded computer system to provide a solution to one of the common problems in Transportation Engineering: Measuring the rut of the road. This thesis is a result of the project ?development of Real Time Rut Measurement system using the scanning laser? for Texas Department of Transportation (TxDOT). Rut measurement has always been a non real time multi step manual process. The research involves remodeling the embedded computer system to function as a real time system that performs the scanning of the road as well as analysis of the scanned road data in real time. The solution implements an efficient mechanism to identify the rut on the road. This splits the whole rigorous procedure and delegates the responsibility of each part to different pieces of hardware and thus achieving parallelism. In addition to providing a real time solution to the problem, the research involves the introduction of time signal and hence providing the scans to be referenced with respect to time instead of distance.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Pseudogene Detection System Based on a High-Performance Computing Platform</strong><br /> Monday, April 25, 2005<br /> Akshay Vaidya<br /><p><a href="#" id="324-show" class="showLink" onclick="showHide(324);return false;">Read More</a></p></p><div id="324" class="more"><p><a href="#" id="324-hide" class="hideLink" onclick="showHide(324);return false;">Hide</a></p><p><strong>Abstract: </strong>Clusters and grids offer a low cost solution for speeding up processing of jobs that have high computational requirements. Clusters and grids have been successfully used for High Energy Physics experiments for many years. Recently there have been several initiatives towards use of grids by the Bioinformatics community. Pseudogene detection is a computationally intensive problem in bioinformatics. The first two steps of the pseudogene detection process require lots of processing and may benefit by use of clusters and grids. There are no current systems that allow any user to control the constraints used for detecting pseudogenes for his choice of genes. This thesis presents a system that allows any user to detect pseudogenes by only providing the parameters used in the process. The system is both cluster and grid enabled and provides benefits of using clusters and grids. The thesis also presents the goals of the system and evaluates the system against the stated goals. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Object Tracking in a Stereo System Using Particle Filter</strong><br /> Friday, April 22, 2005<br /> Anup Sabbi<br /><p><a href="#" id="313-show" class="showLink" onclick="showHide(313);return false;">Read More</a></p></p><div id="313" class="more"><p><a href="#" id="313-hide" class="hideLink" onclick="showHide(313);return false;">Hide</a></p><p><strong>Abstract: </strong>This thesis deals with tracking objects in a stereo camera system using particle filters based on the visual cues of objects. Tracking with stereo cameras is a challenging task because of the stereo correspondence problem. The approach taken alleviates this problem by representing and estimating the state of the system in the form of a probability density function (pdf). For a non-Linear and non-Gaussian model the difficulties are to represent the pdf using finite computer storage and to perform the integration when a new observation becomes available. To overcome these difficulties Monte Carlo based sampling (particle filtering) techniques are used. One more challenge is to incorporate the stereo constraints into the particle filter, since a stereo camera rig is used to get back the lost dimension(depth) in a single camera setup. The two possibilities that were investigated are, (1) maintaining two sets of particles one for each of the stereo image frames and establishing a mapping between the two sets, (2) the second possibility is to throw the particles into a three dimensional space and map it back into the stereo image frames. The ultimate goal is to be able to track in real time in 3D using a particle filter.<br><br> First observation or measurement models for the features of the objects are developed. Color histograms in the HSI color-space, edge density histograms for texture and shape similarity measures based on measurement lines are used to model the observations and their likelihoods. Bhattacharyya distance is used as the distance metric for comparing the target and candidate model histograms. These observations are then integrated with the dynamics model of the system to obtain a posterior probability distribution for the location of the object in the stereo images, using particle filters. Location estimates can be calculated from the obtained distribution. <br><br> The proposed approach can be extended to be used on a mobile robot for tracking stationary and moving objects.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Integration of the D’Artagnan Cognitive Architecture with Real-Time Simulated Environments</strong><br /> Friday, April 22, 2005<br /> Bharat Kondeti<br /><p><a href="#" id="322-show" class="showLink" onclick="showHide(322);return false;">Read More</a></p></p><div id="322" class="more"><p><a href="#" id="322-hide" class="hideLink" onclick="showHide(322);return false;">Hide</a></p><p><strong>Abstract: </strong>The D’Artagnan Cognitive Architecture (DCA) is a multi-agent system, in which each agent mimics an aspect of human intelligence for controlling a Computer Generated Force (CFG). The Cognitive-based Agent Management System (CAMS), a CORBA-based multi-agent framework that emphasizes dynamic control of communication bandwidth among agents, is used as a framework for DCA. Urban Terror (UrT), an urban warfare modification of the Quake 3 Arena first-person shooter game, is integrated as a test-bed for DCA, providing realistic warfare scenarios.<br><br> We provide design and implementation of various agents in CAMS – DCA, its integration with UrT and various enhancements to UrT as well as to CAMS – DCA in order to make a compatible integration. We developed five different scenarios (maps) in UrT with each map having five levels of difficulty resulting in twenty-five test scenarios. A performance comparison is made among different reflex agents of DCA. Out of twenty-five test scenarios, tasks in fifteen test scenarios were successfully accomplished by all the reflex agents with varying performances, whereas only one reflex agent could accomplish all the tasks. The results also indicate that the performance of two or more co-operating reflex agents is always better than the performance of individual reflex agents.     </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Spatial Reasoning for Real-Time Simulated Environments</strong><br /> Friday, April 22, 2005<br /> Maheswar Nallacharu<br /><p><a href="#" id="323-show" class="showLink" onclick="showHide(323);return false;">Read More</a></p></p><div id="323" class="more"><p><a href="#" id="323-hide" class="hideLink" onclick="showHide(323);return false;">Hide</a></p><p><strong>Abstract: </strong>The ability to spatially reason about the surroundings is necessary to navigate in simulated worlds. CAMS-DCA is a multiagent cognitive architecture that uses the Quake 3 modification named Urban Terror as its testbed. One specific feature that the DCA agents lack is the ability to efficiently navigate in the game world. This thesis presents a generic method for extracting spatial information from the most commonly used format of the 3D games genre. The static world is modeled into a hybrid model consisting of both a metric map and a topological map. This has enhanced the navigation ability of agents in CAMS-DCA. We show that by using this method we can provide the human perspective of the game world in the form of spatial information to the agents. We show the need for such a system and how it can be designed and implemented. The navigational system developed is based on the Metric Diagram / Place Vocabulary (MD/PV) model. Experimental results verify the performance enhancements due to the spatial reasoning agent. The role of agents working together in order to achieve tasks that cannot be accomplished by any single agent is also demonstrated. This work demonstrates the possibility of a generic spatial reasoning model based on the MD/PV model that can perform navigational reasoning tasks in simulation worlds.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Pervasive Resource Exploitation</strong><br /> Friday, April 15, 2005<br /> Derek Maxey<br /><p><a href="#" id="320-show" class="showLink" onclick="showHide(320);return false;">Read More</a></p></p><div id="320" class="more"><p><a href="#" id="320-hide" class="hideLink" onclick="showHide(320);return false;">Hide</a></p><p><strong>Abstract: </strong>Many computing system users are inconvenienced by the variations and limitations of computing system capabilities. This inconvenience results in enduring the pains of system inadequacies or selecting a more capable computing system to accomplish a task.  When a mobile device sounds a low battery alarm the user is given an ultimatum to quickly find a power source or suspend work until a more capable system is located.  What if computing devices could tap into the potential of more resourceful systems?  Not only tap into that potential, but automatically make decisions on when and how to access the potential of other computing systems. Using pervasive computing techniques user distraction and involvement can be minimized while user satisfaction is increased.  A middleware service architecture is developed and deployed throughout a network of machines to facilitate resource exploitation.  For a range of inputs a file zipping application shows an average 73% decrease in response time with an average 62% decrease in energy consumption.</p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>Coalesced QoS: A Pragmatic Approach to a Unified Model to Support Quality Of Service (QoS) In High Performance Kernel-Less Operating System (KLOS)</strong><br /> Monday, April 11, 2005<br /> Ashish Chawla<br /><p><a href="#" id="314-show" class="showLink" onclick="showHide(314);return false;">Read More</a></p></p><div id="314" class="more"><p><a href="#" id="314-hide" class="hideLink" onclick="showHide(314);return false;">Hide</a></p><p><strong>Abstract: </strong>Advances in software and hardware technologies have given operating systems the ability to process data and handle various concurrent processes. The increased ability has been one of the driving forces which have led to the proliferation of mechanisms in operating systems to satisfy the performance requirements of applications with predictable resource allocation. As different classes of applications require different resource management policies one needs to look into ways to satisfy all classes of applications. Conventional general purpose operating systems have been developed for a single class of best-effort applications, hence, are inadequate to support multiple classes of applications. We present an abstract architecture for the support of Quality of Service (QoS) in Kernel-Less Operating System (KLOS). We propose new semantics for the QoS resource management paradigm, based on the notion of Quality of Service. By virtue of this new semantics, it is possible to provide the support required by KLOS to various components of the operating system such as memory manager, processor time, IO management etc. These mechanisms which are required within an operating system to support this paradigm are described, and the design and implementation of a prototypical kernel which implements them is presented. Various notions of negotiation rules between the application and the operating systems are discussed along-with a feature which allows the user to express its requirements and the fact is that this model assures the user in providing the selected parameters and returns feedback about the way it meets those requirements. This QoS model presents a design paradigm that allows the internal components to be rearranged dynamically, adapting the architecture to the high performance of KLOS.<br>The benefits of our framework are demonstrated by building an informational model to represent how the various modules of an operating system and the interface between the processes and the operating system can be tailored to provide Quality of Service guarantees. </p></div></div><div style="border-bottom: 1px dotted #CCC;"><p><strong>A Rate Adaptive Hybrid MAC for Centralized Wireless Packet Network</strong><br /> Monday, April 11, 2005<br /> Umapathi Masila Mani<br /><p><a href="#" id="317-show" class="showLink" onclick="showHide(317);return false;">Read More</a></p></p><div id="317" class="more"><p><a href="#" id="317-hide" class="hideLink" onclick="showHide(317);return false;">Hide</a></p><p><strong>Abstract: </strong>Efficient wireless MAC layer protocol is important to provide seamless transition with the wireless ATM network which carries various types of traffic. Over the years there is lot of research on cellular networks accommodating the mobile stations with different services. We propose a scheme on scheduling the mobile stations and using the network bandwidth efficiently based on the channel condition. In recent years there have been researches on rate adaptation and scheduling of services based on the channel condition Rate adaptive hybrid MAC (RAH MAC) proposes an improvement.<br><br> RAH MAC protocol is a centralized wireless packet networks. In this protocol, the contention time is dynamically allocated to match the system conditions. Different classes of MSs have different priority to use the contention time. The channel condition is modeled as a finite state Markov chain with several SINR level. Polling method is used for giving the BS better control to dynamically schedule events based on the system state. </p></div></div></div></article>
</section>
<section>
<article>
<header>
<h2><!--section 2 heading--></h2>
</header>
<!--section 2 article-->
</article>
</section>
 <script>
togglemenu('submenu-seminars','seminars-link')
</script></div>
<footer><div id="footer-extra"><a href="https://twitter.com/intent/follow?original_referer=http%3A%2F%2Fcse.uta.edu%2Fresearch%2Fcenters-labs.php&amp;screen_name=cseuta&amp;tw_p=followbutton"><img alt="Follow " src="../_images/content/twitter.png" style="float: none; margin: 0 0 7px  5px;"/></a> <a href="https://www.facebook.com/groups/119117011465771/"><img alt="Facebook" height="30" src="../_images/content/icon-facebook.png" style="float: none; margin: 0 0 7px  5px;" width="30"/></a> <a class="align-right" href="https://www.linkedin.com/groups?trk=myg_ugrp_ovr&amp;gid=4120776"> <img alt="Linked In" height="30" src="../_images/content/icon-linkedin.png" style="float: none; margin: 0 0 7px 5px;" width="33"/></a><br/> <a href="../faculty-resources.php">Faculty Resources</a></div>
<div><img alt="The University of Texas at Arlington" height="73" src="../_images/elements/uta_sm_logo.png" width="82"/>
<p><strong>Department of Computer Science and Engineering</strong> <span class="print-only">[<a href="http://www.uta.edu/bioengineering">cse.uta.edu</a>]</span><br/> Engineering Research Building, Room 640, Box 19015, Arlington, TX 76010<br/> Phone: 817-272-3785 | Fax: 817-272-3784 <br/> <a href="http://www.uta.edu/"> &#169;  2015  The University of Texas at Arlington</a>.</p>
</div>
<div class="mobile-only">
<div style="clear: both; margin: 10px auto 10px auto; width: 90%;"><a href="https://twitter.com/intent/follow?original_referer=http%3A%2F%2Fcse.uta.edu%2Fresearch%2Fcenters-labs.php&amp;screen_name=cseuta&amp;tw_p=followbutton"><img alt="Follow " src="../_images/content/twitter.png" style="float: none; margin: 0 0 7px  5px;"/></a> <a href="https://www.facebook.com/groups/119117011465771/"><img alt="Facebook" height="30" src="../_images/content/icon-facebook.png" style="float: none; margin: 0 0 7px  5px;" width="30"/></a> <a class="align-right" href="https://www.linkedin.com/groups?trk=myg_ugrp_ovr&amp;gid=4120776"> <img alt="Linked In" height="30" src="../_images/content/icon-linkedin.png" style="float: none; margin: 0 0 7px 5px;" width="33"/></a><br/> <a href="../faculty-resources.php">Faculty Resources</a></div>
</div></footer>
</div>
</div>
</body>
</html>